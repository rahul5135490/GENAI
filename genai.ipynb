{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21732742-8f9b-4127-a1f7-7f718961743c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.1.13-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.29-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.3-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.29 (from langchain)\n",
      "  Using cached langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n",
      "  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.38-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (1.26.0)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Using cached pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (8.2.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.0-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.7 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.7 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.7 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.7 kB ? eta -:--:--\n",
      "     ----------------------- -------------- 30.7/50.7 kB 100.9 kB/s eta 0:00:01\n",
      "     ----------------------- -------------- 30.7/50.7 kB 100.9 kB/s eta 0:00:01\n",
      "     ------------------------------ ------- 41.0/50.7 kB 109.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 50.7/50.7 kB 123.4 kB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.16.3-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached langchain-0.1.13-py3-none-any.whl (810 kB)\n",
      "Downloading aiohttp-3.9.3-cp311-cp311-win_amd64.whl (365 kB)\n",
      "   ---------------------------------------- 0.0/365.3 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 30.7/365.3 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 143.4/365.3 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 256.0/365.3 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  358.4/365.3 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 365.3/365.3 kB 2.1 MB/s eta 0:00:00\n",
      "Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
      "Downloading langchain_core-0.1.37-py3-none-any.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.6 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 81.9/274.6 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/274.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/274.6 kB 173.8 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 163.8/274.6 kB 192.7 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 163.8/274.6 kB 192.7 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 163.8/274.6 kB 192.7 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 163.8/274.6 kB 192.7 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 163.8/274.6 kB 192.7 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 163.8/274.6 kB 192.7 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 174.1/274.6 kB 149.8 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/274.6 kB 166.1 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 204.8/274.6 kB 124.5 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 204.8/274.6 kB 124.5 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/274.6 kB 132.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 245.8/274.6 kB 103.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 245.8/274.6 kB 103.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.6 kB 103.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.6 kB 103.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.6 kB 103.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.6 kB 103.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.6 kB 99.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 274.6/274.6 kB 101.9 kB/s eta 0:00:00\n",
      "Using cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.38-py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/86.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/86.9 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/86.9 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 30.7/86.9 kB 325.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 81.9/86.9 kB 573.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 86.9/86.9 kB 543.5 kB/s eta 0:00:00\n",
      "Using cached pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "Downloading pydantic_core-2.16.3-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/1.9 MB 35.8 kB/s eta 0:00:52\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.2 kB/s eta 0:01:13\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.2 kB/s eta 0:01:13\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.2 kB/s eta 0:01:13\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.2 kB/s eta 0:01:13\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.2 kB/s eta 0:01:13\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.2 kB/s eta 0:01:13\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.2 kB/s eta 0:01:13\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.2 kB/s eta 0:01:13\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.4 kB/s eta 0:01:12\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.4 kB/s eta 0:01:12\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.4 kB/s eta 0:01:12\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.4 kB/s eta 0:01:12\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.4 kB/s eta 0:01:12\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.4 kB/s eta 0:01:12\n",
      "   - -------------------------------------- 0.1/1.9 MB 25.4 kB/s eta 0:01:12\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   - -------------------------------------- 0.1/1.9 MB 29.6 kB/s eta 0:01:01\n",
      "   -- ------------------------------------- 0.1/1.9 MB 30.9 kB/s eta 0:00:58\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 0.1/1.9 MB 33.5 kB/s eta 0:00:53\n",
      "   --- ------------------------------------ 0.1/1.9 MB 28.0 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.1/1.9 MB 28.0 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.1/1.9 MB 28.0 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.1/1.9 MB 28.0 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.1/1.9 MB 28.0 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.1/1.9 MB 28.0 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.1/1.9 MB 28.0 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.1/1.9 MB 28.0 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 27.8 kB/s eta 0:01:03\n",
      "   --- ------------------------------------ 0.2/1.9 MB 29.0 kB/s eta 0:00:59\n",
      "   --- ------------------------------------ 0.2/1.9 MB 29.0 kB/s eta 0:00:59\n",
      "   --- ------------------------------------ 0.2/1.9 MB 29.0 kB/s eta 0:00:59\n",
      "   --- ------------------------------------ 0.2/1.9 MB 29.0 kB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.4 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.4 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.4 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.4 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.4 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.4 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.4 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.4 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 31.3 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 32.0 kB/s eta 0:00:52\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 32.0 kB/s eta 0:00:52\n",
      "   ----- ---------------------------------- 0.2/1.9 MB 32.8 kB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 0.2/1.9 MB 32.8 kB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 0.2/1.9 MB 32.8 kB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 0.2/1.9 MB 32.8 kB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 0.2/1.9 MB 32.8 kB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 0.2/1.9 MB 32.8 kB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 0.2/1.9 MB 32.8 kB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 34.3 kB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 34.3 kB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 34.3 kB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 34.3 kB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 36.3 kB/s eta 0:00:45\n",
      "   ------ --------------------------------- 0.3/1.9 MB 37.3 kB/s eta 0:00:43\n",
      "   ------ --------------------------------- 0.3/1.9 MB 39.8 kB/s eta 0:00:40\n",
      "   ------- -------------------------------- 0.3/1.9 MB 43.6 kB/s eta 0:00:36\n",
      "   ------- -------------------------------- 0.4/1.9 MB 46.1 kB/s eta 0:00:34\n",
      "   ------- -------------------------------- 0.4/1.9 MB 47.3 kB/s eta 0:00:32\n",
      "   -------- ------------------------------- 0.4/1.9 MB 49.5 kB/s eta 0:00:31\n",
      "   -------- ------------------------------- 0.4/1.9 MB 50.7 kB/s eta 0:00:30\n",
      "   -------- ------------------------------- 0.4/1.9 MB 51.7 kB/s eta 0:00:29\n",
      "   --------- ------------------------------ 0.5/1.9 MB 56.6 kB/s eta 0:00:26\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.5/1.9 MB 57.7 kB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 49.2 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 45.0 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 45.0 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 45.0 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 45.0 kB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 46.1 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 46.1 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 46.1 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 46.1 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 46.1 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 46.1 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 46.1 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 45.6 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 45.6 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 45.6 kB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 46.8 kB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 46.8 kB/s eta 0:00:29\n",
      "   ------------ --------------------------- 0.6/1.9 MB 47.2 kB/s eta 0:00:28\n",
      "   ------------ --------------------------- 0.6/1.9 MB 48.8 kB/s eta 0:00:27\n",
      "   ------------- -------------------------- 0.7/1.9 MB 53.7 kB/s eta 0:00:23\n",
      "   --------------- ------------------------ 0.7/1.9 MB 60.3 kB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 66.8 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 64.9 kB/s eta 0:00:17\n",
      "   ------------------ --------------------- 0.9/1.9 MB 66.4 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 0.9/1.9 MB 66.4 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 0.9/1.9 MB 66.4 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 0.9/1.9 MB 66.4 kB/s eta 0:00:16\n",
      "   ------------------ --------------------- 0.9/1.9 MB 67.5 kB/s eta 0:00:15\n",
      "   ------------------- -------------------- 0.9/1.9 MB 68.0 kB/s eta 0:00:15\n",
      "   -------------------- ------------------- 0.9/1.9 MB 70.8 kB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 1.0/1.9 MB 78.3 kB/s eta 0:00:11\n",
      "   ------------------------ --------------- 1.2/1.9 MB 86.4 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 1.2/1.9 MB 91.5 kB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 101.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 1.5/1.9 MB 109.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 1.6/1.9 MB 117.1 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 122.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.8/1.9 MB 129.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 137.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 137.3 kB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.29-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.1 MB 2.0 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/2.1 MB 2.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.1 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.1 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.6 MB/s eta 0:00:00\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.8 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 92.2/292.8 kB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 245.8/292.8 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  286.7/292.8 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 292.8/292.8 kB 2.0 MB/s eta 0:00:00\n",
      "Using cached marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.0-cp311-none-win_amd64.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.2 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 81.9/139.2 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 139.2/139.2 kB 1.7 MB/s eta 0:00:00\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.7/76.7 kB 1.4 MB/s eta 0:00:00\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: pydantic-core, packaging, orjson, mypy-extensions, multidict, jsonpatch, greenlet, frozenlist, annotated-types, yarl, typing-inspect, SQLAlchemy, pydantic, marshmallow, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "Successfully installed SQLAlchemy-2.0.29 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 dataclasses-json-0.6.4 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.37 langchain-text-splitters-0.0.1 langsmith-0.1.38 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.0 packaging-23.2 pydantic-2.6.4 pydantic-core-2.16.3 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script langsmith.exe is installed in 'C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script langchain-server.exe is installed in 'C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4ba876-1c4e-4d30-8f2e-be8afd293f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-3.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pinecone-client) (2023.7.22)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pinecone-client) (4.8.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pinecone-client) (2.0.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n",
      "Downloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n",
      "   ---------------------------------------- 0.0/215.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/215.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.9 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 41.0/215.9 kB 19.9 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 41.0/215.9 kB 19.9 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 41.0/215.9 kB 19.9 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 41.0/215.9 kB 19.9 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 41.0/215.9 kB 19.9 kB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 28.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 28.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 28.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 28.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 28.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 28.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 28.7 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 81.9/215.9 kB 34.5 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 81.9/215.9 kB 34.5 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 92.2/215.9 kB 37.2 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 92.2/215.9 kB 37.2 kB/s eta 0:00:04\n",
      "   -------------------- ------------------ 112.6/215.9 kB 44.9 kB/s eta 0:00:03\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   ----------------------------- --------- 163.8/215.9 kB 65.5 kB/s eta 0:00:01\n",
      "   --------------------------------------  215.0/215.9 kB 66.5 kB/s eta 0:00:01\n",
      "   --------------------------------------- 215.9/215.9 kB 65.8 kB/s eta 0:00:00\n",
      "Installing collected packages: pinecone-client\n",
      "Successfully installed pinecone-client-3.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f662bf-c7db-479f-a4a7-ea3ac04b7f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-4.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/286.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/286.1 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/286.1 kB 330.3 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 30.7/286.1 kB 330.3 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 30.7/286.1 kB 330.3 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 30.7/286.1 kB 330.3 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 30.7/286.1 kB 330.3 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 30.7/286.1 kB 330.3 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 81.9/286.1 kB 218.5 kB/s eta 0:00:01\n",
      "   ------------ -------------------------- 92.2/286.1 kB 209.5 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 112.6/286.1 kB 234.3 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 225.3/286.1 kB 430.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 286.1/286.1 kB 519.4 kB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c81f305-363e-4005-ae46-0c16567adbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-1.14.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.2.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
      "   ---------------------------------------- 0.0/262.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/262.9 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/262.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 245.8/262.9 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 262.9/262.9 kB 1.8 MB/s eta 0:00:00\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, openai\n",
      "Successfully installed distro-1.9.0 openai-1.14.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script distro.exe is installed in 'C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e4981e4-737c-4f64-b654-5c9e6c20cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-win_amd64.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/798.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/798.7 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/798.7 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/798.7 kB 435.7 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 163.8/798.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 337.9/798.7 kB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 522.2/798.7 kB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 645.1/798.7 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  788.5/798.7 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 798.7/798.7 kB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9847a5-f37d-422b-9fd7-3c2b9adbf903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5947ed8-5c74-4ebe-995f-ca9ef167f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a7f1d3-f1b9-41bf-aab5-fbfa1fc8e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4386d3-4692-48ee-b715-a6840deb565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4c27c2-37af-45a3-8e97-1b42fd4ef44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0fbac78-86fe-4737-8baa-dce6f1ca6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ee1b75-d8cc-43d7-bd39-c7f0ce19a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f6fc65e-f76b-46ef-9ba5-ad001b22520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8afd25f4-31b5-40a5-a541-aadd7a1c287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfeb6782-f0af-49df-9738-12d4916bad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFDirectoryLoader(\"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5291b2d-8c75-4632-a035-e5877c8bebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd5f3694-f122-4bf7-8a9b-48b99af81c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n2\\n \\n \\n \\n \\n \\n \\n \\n \\nNeural Networks\\n \\n \\n \\nfrom Scratch in\\n \\n \\n \\nPython\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHarrison Kinsley & Daniel Kukieła\\n \\n ', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6571f8fa-b1f1-4fa5-8995-c17ef6bae934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n3\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAcknowledgements\\n \\nHarrison Kinsley:\\n \\nMy wife, Stephanie, for her unfailing support and faith in me throughout the years. You’ve never\\n \\ndoubted me.\\n \\nEach and every viewer and person who supported this book and project. Without my audience,\\n \\nnone of this would have been possible.\\n \\nThe Python programming community in general for being awesome!\\n \\nDaniel Kukie\\n\\u200b\\nł\\n\\u200b\\na for your u nwavering effort with this massive project that Neural Networks from\\n \\nScratch became. From learning C++ to make mods in GTA V, to Python for various projects, to\\n \\nthe calculus behind neural networks, there doesn’t seem to be any problem you cannot solve and\\n \\nit is a pleasure to do this for a living with you. I look forward to seeing what’s next!\\n \\n \\n \\n ', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b75fa692-8ada-4817-8769-57e6f36fa516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preface - Neural Networks from Scratch in Python\\n \\n2\\n \\n \\n \\n \\n \\n \\n \\n \\nNeural Networks\\n \\n \\n \\nfrom Scratch in\\n \\n \\n \\nPython\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHarrison Kinsley & Daniel Kukieła\\n \\n '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98743226-ac44-408f-9a2e-dada455e610e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preface - Neural Networks from Scratch in Python\\n \\n4\\n \\n \\n \\n \\nDaniel Kukieła:\\n \\nMy son, Oskar, for his patience and understanding during the busy days. My wife, Katarzyna,\\n \\nfor the boundless love, faith and support in all the things I do, have ever done, and plan to do,\\n \\nthe sunlight during most stormy days and the morning coffee every single day.\\n \\nHarrison for challenging me to learn Python then pushing me towards learning neural networks.\\n \\nFor showing me that things do not have to be perfectly done, all the support, and making me a\\n \\npart of so many interesting projects including “let’s make a tutorial on neural networks from\\n \\nscratch,” which turned into one the biggest challenges of my life — this book. I wouldn’t be at\\n \\nwhere I am now if all of that didn’t happen.\\n \\nThe Python community for making me a better programmer and for helping me to improve my\\n \\nlanguage skills.\\n \\n \\n '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aa48af6-931d-4af7-b855-08335277de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9444172-4a12-4bb6-b9b6-326817ef21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks=text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4404827-6799-4f5d-848b-69df1933141c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n2\\n \\n \\n \\n \\n \\n \\n \\n \\nNeural Networks\\n \\n \\n \\nfrom Scratch in\\n \\n \\n \\nPython\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHarrison Kinsley & Daniel Kukieła', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 0}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n3\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAcknowledgements\\n \\nHarrison Kinsley:\\n \\nMy wife, Stephanie, for her unfailing support and faith in me throughout the years. You’ve never\\n \\ndoubted me.\\n \\nEach and every viewer and person who supported this book and project. Without my audience,\\n \\nnone of this would have been possible.\\n \\nThe Python programming community in general for being awesome!\\n \\nDaniel Kukie\\n\\u200b\\nł\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 1}),\n",
       " Document(page_content='Daniel Kukie\\n\\u200b\\nł\\n\\u200b\\na for your u nwavering effort with this massive project that Neural Networks from\\n \\nScratch became. From learning C++ to make mods in GTA V, to Python for various projects, to\\n \\nthe calculus behind neural networks, there doesn’t seem to be any problem you cannot solve and\\n \\nit is a pleasure to do this for a living with you. I look forward to seeing what’s next!', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 1}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n4\\n \\n \\n \\n \\nDaniel Kukieła:\\n \\nMy son, Oskar, for his patience and understanding during the busy days. My wife, Katarzyna,\\n \\nfor the boundless love, faith and support in all the things I do, have ever done, and plan to do,\\n \\nthe sunlight during most stormy days and the morning coffee every single day.\\n \\nHarrison for challenging me to learn Python then pushing me towards learning neural networks.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 2}),\n",
       " Document(page_content='For showing me that things do not have to be perfectly done, all the support, and making me a\\n \\npart of so many interesting projects including “let’s make a tutorial on neural networks from\\n \\nscratch,” which turned into one the biggest challenges of my life — this book. I wouldn’t be at\\n \\nwhere I am now if all of that didn’t happen.\\n \\nThe Python community for making me a better programmer and for helping me to improve my\\n \\nlanguage skills.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 2}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n5\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCopyright\\n \\nCopyright © 2020 Harrison Kinsley\\n \\nCover Design copyright © 2020 Harrison Kinsley\\n \\nNo part of this book may be reproduced in any form or by any electronic or mechanical means,\\n \\nwith the following exceptions:\\n \\n \\n1.\\nBrief quotations from the book.\\n \\n \\n2.\\nPython Code/software (strings interpreted as logic with Python), which is housed under the\\n \\nMIT license, described on the next page.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 3}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n6\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLicense for Code\\n \\nThe Python code/software in this book is contained under the following MIT License:\\n \\nCopyright © 2020 Sentdex, Kinsley Enterprises Inc., https://nnfs.io\\n \\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\\n \\nassociated documentation files (the “Software”), to deal in the Software without restriction,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 4}),\n",
       " Document(page_content='including without limitation the rights to use, copy, modify, merge, publish, distribute,\\n \\nsublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\\n \\nfurnished to do so, subject to the following conditions:\\n \\nThe above copyright notice and this permission notice shall be included in all copies or substantial\\n \\nportions of the Software.\\n \\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 4}),\n",
       " Document(page_content='EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\n \\n \\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\\n \\nNONINFRINGEMENT.\\n \\n \\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\\n \\n \\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n \\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\\n \\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 4}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n7\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nReadme\\n \\nThe objective of this book is to break down an extremely complex topic, neural networks, into\\n \\nsmall pieces, consumable by anyone wishing to embark on this journey. Beyond breaking down\\n \\nthis topic, the hope is to dramatically demystify neural networks. As you will soon see, this\\n \\nsubject, when explored from scratch, can be an educational and engaging experience. This book is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 5}),\n",
       " Document(page_content='for anyone willing to put in the time to sit down and work through it. In return, you will gain a far\\n \\ndeeper understanding than most when it comes to neural networks and deep learning.\\n \\nThis book will be easier to understand if you already have an understanding of Python or\\n \\nanother programming language. Python is one of the most clear and understandable\\n \\nprogramming languages; we have no real interest in padding page counts and exhausting an', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 5}),\n",
       " Document(page_content='entire first chapter with a basics of Python tutorial. If you need one, we suggest you start here:\\n \\nhttps://pythonprogramming.net/python-fundamental-tutorials/ \\n\\u200b\\nTo cite this material:\\n \\nHarrison Kinsley & Daniel Kukie\\n\\u200b\\nł\\n\\u200b\\na Neural Networks from Scratch (NNFS) https://nnfs.io', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 5}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n8 \\nChapter 1\\nIntroducing Neural Networks  \\nWe begin with a general idea of what \\u200bneural networks \\u200b are and why you might be interested in  \\nthem. Neural networks, also called \\u200bArtificial Neural Networks \\u200b (though it seems, in recent  \\nyears, we’ve dropped the “artificial” part), are a type of machine learning often conflated with  \\ndeep learning. The defining characteristic of a \\u200bdeep \\u200b neural network is having two or more', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 6}),\n",
       " Document(page_content='hidden layers \\u200b — a concept that will be explained shortly, but these hidden layers are ones that  \\nthe neural network controls. It’s reasonably safe to say that most neural networks in use are a  \\nform of deep learning.  \\nFig 1.01: \\u200bDepicting the various fields of artificial intelligence and where they fit in overall.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 6}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n9\\n \\n \\nA Brief History\\n \\nSince the advent of computers, scientists have been formulating ways to enable machines to take\\n \\ninput and produce desired output for tasks like \\n\\u200b\\nclassification\\n\\u200b\\n and \\n\\u200b\\nregression\\n\\u200b\\n. Additionally, in\\n \\ngeneral, there’s \\n\\u200b\\nsupervised\\n\\u200b\\n and \\n\\u200b\\nunsupervised\\n\\u200b\\n machine learning. Supervised machine learning\\n \\nis used when you have pre-established and labeled data that can be used for training. Let’s say', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 7}),\n",
       " Document(page_content='you have sensor data for a server with metrics such as upload/download rates, temperature, and\\n \\nhumidity, all organized by time for every 10 minutes. Normally, this server operates as intended\\n \\nand has no outages, but sometimes parts fail and cause an outage. We might collect data and\\n \\nthen divide it into two classes: one class for times/observations when the server is operating\\n \\nnormally, and another class for times/observations when the server is experiencing an outage.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 7}),\n",
       " Document(page_content='When the server is failing, we want to label that sensor data leading up to failure as data that\\n \\npreceded a failure. When the server is operating normally, we simply label that data as\\n \\n“normal.”\\n \\nWhat each sensor measures in this example is called a feature. A group of features makes up a\\n \\nfeature set (represented as vectors/arrays), and the values of a feature set can be referred to as a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 7}),\n",
       " Document(page_content='sample. Samples are fed into neural network models to train them to fit desired outputs from these\\n \\ninputs or to predict based on them during the inference phase.\\n \\n \\nThe “normal” and “failure” labels are \\n\\u200b\\nclassifications\\n\\u200b\\n or \\n\\u200b\\nlabels.\\n\\u200b\\n \\n\\u200b\\nYou may also see these referred\\n \\nto as \\n\\u200b\\ntargets\\n\\u200b\\n or \\n\\u200b\\nground-truths\\n\\u200b\\n while we fit a machine learning algorithm. These targets are the\\n \\nclassifications that are the \\n\\u200b\\ngoal\\n\\u200b\\n or \\n\\u200b\\ntarget\\n\\u200b\\n, known to be \\n\\u200b\\ntrue and\\n\\u200b\\n \\n\\u200b\\ncorrect\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 7}),\n",
       " Document(page_content='\\u200b\\n \\n\\u200b\\ncorrect\\n\\u200b\\n, for the algorithm to\\n \\nlearn. For this example, the aim is to eventually train an algorithm to read sensor data and\\n \\naccurately predict when a failure is imminent. This is just one example of supervised learning\\n \\nin the form of classification. In addition to classification, there’s also regression, which is used\\n \\nto predict numerical values, like stock prices. There’s also unsupervised machine learning,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 7}),\n",
       " Document(page_content='where the machine finds structure in data without knowing the labels/classes ahead of time.\\n \\nThere are additional concepts (e.g., reinforcement learning and semi-supervised machine\\n \\nlearning) that fall under the umbrella of neural networks. For this book, we will focus on\\n \\nclassification and regression with neural networks, but what we cover here leads to other\\n \\nuse-cases.\\n \\nNeural networks were conceived in the 1940s, but figuring out how to train them remained a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 7}),\n",
       " Document(page_content='mystery for 20 years. The concept of \\n\\u200b\\nbackpropagation \\n\\u200b\\n(explained later) came in the 1960s, but\\n \\nneural networks still did not receive much attention until they started winning competitions in\\n \\n2010. Since then, neural networks have been on a meteoric rise due to their sometimes seemingly', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 7}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n10\\n \\nmagical ability to solve problems previously deemed unsolvable, such as image captioning,\\n \\nlanguage translation, audio and video synthesis, and more.\\n \\nCurrently, neural networks are the primary solution to most competitions and challenging\\n \\ntechnological problems like self-driving cars, calculating risk, detecting fraud, and early cancer\\n \\ndetection, to name a few.\\n \\nWhat is a Neural Network?', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 8}),\n",
       " Document(page_content='“Artificial” neural networks are inspired by the organic brain, translated to the computer. It’s not\\n \\na perfect comparison, but there are neurons, activations, and lots of interconnectivity, even if the\\n \\nunderlying processes are quite different.\\n \\n \\nFig 1.02: \\n\\u200b\\nComparing a biological neuron to an artificial neuron.\\n \\nA single neuron by itself is relatively useless, but, when combined with hundreds or thousands', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 8}),\n",
       " Document(page_content='(or many more) of other neurons, the interconnectivity produces relationships and results that\\n \\nfrequently outperform any other machine learning methods.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 8}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n11\\n \\n \\nFig 1.03:\\n\\u200b\\n Example of a neural network with 3 hidden layers of 16 neurons each.\\n \\n \\n \\nAnim 1.03:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/ntr\\n \\nThe above animation shows the examples of the model structures and the numbers of parameters\\n \\nthe model has to learn to adjust in order to produce the desired outputs. The details of what is seen\\n \\nhere are the subjects of future chapters.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 9}),\n",
       " Document(page_content='It might seem rather complicated when you look at it this way. Neural networks are considered\\n \\nto be “black boxes” in that we often have no idea \\n\\u200b\\nwhy\\n\\u200b\\n they reach the conclusions they do. We do\\n \\nunderstand \\n\\u200b\\nhow \\n\\u200b\\nthey do this, though.\\n \\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each\\n \\nneuron of a given layer is connected to every neuron of the next layer, which means that its output', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 9}),\n",
       " Document(page_content='value becomes an input for the next neurons. Each connection between neurons has a weight\\n \\nassociated with it, which is a trainable factor of how much of this input to use, and this weight\\n \\ngets multiplied by the input value. Once all of the \\n\\u200b\\ninputs·weights\\n\\u200b\\n flow into our neuron, they are', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 9}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n12 \\nsummed, and a bias, another trainable parameter, is added. The purpose of the bias is to offset the  \\noutput positively or negatively, which can further help us map more real-world types of dynamic  \\ndata. In chapter 4, we will show some examples of how this works.  \\nThe concept of weights and biases can be thought of as “knobs” that we can tune to fit our model', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 10}),\n",
       " Document(page_content='to data. In a neural network, we often have thousands or even millions of these parameters tuned  \\nby the optimizer during training. Some may ask, “why not just have biases or just weights?”  \\nBiases and weights are both tunable parameters, and both will impact the neurons’ outputs, but  \\nthey do so in different ways. Since weights are multiplied, they will only change the magnitude or  \\neven completely flip the sign from positive to negative, or vice versa. \\u200bOutput = weight·input+bias', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 10}),\n",
       " Document(page_content='is not unlike the equation for a line \\u200by = mx+b \\u200b. We can visualize this with:  \\nFig 1.04: \\u200b Graph of a single-input neuron’s output with a weight of 1, bias of 0 and input \\u200bx \\u200b. \\nAdjusting the weight will impact the slope of the function:  \\nFig 1.05: \\u200b Graph of a single-input neuron’s output with a weight of 2, bias of 0 and input \\u200bx \\u200b.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 10}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n13\\n \\nAs we increase the value of the weight, the slope will get steeper. If we decrease the weight, the\\n \\nslope will decrease. If we negate the weight, the slope turns to a negative:\\n \\n \\nFig 1.06:\\n\\u200b\\n Graph of a single-input neuron’s output with a weight of -0.70, bias of 0 and input \\n\\u200b\\nx\\n\\u200b\\n.\\n \\nThis should give you an idea of how the weight impacts the neuron’s output value that we get\\n \\nfrom \\n\\u200b\\ninputs·weights+bias.\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 11}),\n",
       " Document(page_content='\\u200b\\n Now, how about the bias parameter? The bias offsets the overall\\n \\nfunction. For example, with a weight of 1.0 and a bias of 2.0:\\n \\n \\nFig 1.07:\\n\\u200b\\n Graph of a single-input neuron’s output with a weight of 1, bias of 2 and input \\n\\u200b\\nx\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 11}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n14 \\nAs we increase the bias, the function output overall shifts upward. If we decrease the bias, then  \\nthe overall function output will move downward. For example, with a negative bias:  \\nFig 1.08: \\u200b Graph of a single-input neuron’s output with a weight of 1.0, bias of -0.70 and input \\u200bx \\u200b. \\nAnim 1.04-1.08: \\u200b \\u200bhttps://nnfs.io/bru  \\nAs you can see, weights and biases help to impact the outputs of neurons, but they do so in', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 12}),\n",
       " Document(page_content='slightly different ways. This will make even more sense when we cover \\u200bactivation functions \\u200b in \\nchapter 4. Still, you can hopefully already see the differences between weights and biases and  \\nhow they might individually help to influence output. Why this matters will be conveyed shortly.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 12}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n15 \\nAs a very general overview, the step function meant to mimic a neuron in the brain, either “firing”  \\nor not — like an on-off switch. In programming, an on-off switch as a function would be called a  \\nstep function \\u200b because it looks like a step if we graph it.  \\nFig 1.09: \\u200b Graph of a step function.  \\nFor a step function, if the neuron’s output value, which is calculated by \\u200bsum(inputs · weights)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 13}),\n",
       " Document(page_content='+ bias \\u200b, is greater than 0, the neuron fires (so it would output a 1). Otherwise, it does not fire                     \\nand would pass along a 0. The formula for a single neuron might look something like:  \\noutput \\u200b= \\u200bsum\\u200b(inputs \\u200b* \\u200bweights) \\u200b+ \\u200bbias\\nWe then usually apply an activation function to this output, noted by \\u200bactivation() \\u200b: \\noutput \\u200b= \\u200bactivation(output)\\nWhile you can use a step function for your activation function, we tend to use something slightly', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 13}),\n",
       " Document(page_content='more advanced. Neural networks of today tend to use more informative activation functions  \\n(rather than a step function), such as the \\u200bRectified Linear \\u200b (ReLU) activation function, which we  \\nwill cover in-depth in Chapter 4. Each neuron’s output could be a part of the ending output layer,  \\nas well as the input to another layer of neurons. While the full function of a neural network can  \\nget very large, let’s start with a simple example with 2 hidden layers of 4 neurons each.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 13}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n16\\n \\n \\nFig 1.10:\\n\\u200b\\n Example basic neural network.\\n \\nAlong with these 2 hidden layers, there are also two more layers here — the input and output\\n \\nlayers. The input layer represents your actual input data, for example, pixel values from an image\\n \\nor data from a temperature sensor. While this data can be “raw” in the exact form it was collected,\\n \\nyou will typically \\n\\u200b\\npreprocess\\n\\u200b\\n your data through functions like \\n\\u200b\\nnormalization\\n\\u200b\\n and \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 14}),\n",
       " Document(page_content='\\u200b\\n and \\n\\u200b\\nscaling\\n\\u200b\\n, and\\n \\nyour input needs to be in numeric form. Concepts like scaling and normalization will be covered\\n \\nlater in this book. However, it is common to preprocess data while retaining its features and\\n \\nhaving the values in similar ranges between 0 and 1 or -1 and 1. To achieve this, you will use\\n \\neither or both scaling and normalization functions. The output layer is whatever the neural', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 14}),\n",
       " Document(page_content='network returns. With classification, where we aim to predict the class of the input, the output\\n \\nlayer often has as many neurons as the training dataset has classes, but can also have a single\\n \\noutput neuron for binary (two classes) classification. We’ll discuss this type of model later and,\\n \\nfor now, focus on a classifier that uses a separate output neuron per each class. For example, if\\n \\nour goal is to classify a collection of pictures as a “dog” or “cat,” then there are two classes in', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 14}),\n",
       " Document(page_content='total. This means our output layer will consist of two neurons; one neuron associated with “dog”\\n \\nand the other with “cat.” You could also have just a single output neuron that is “dog” or “not\\n \\ndog.”', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 14}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n17\\n \\n \\nFig 1.11:\\n\\u200b\\n Visual depiction of passing image data through a neural network, getting a classification\\n \\nFor each image passed through this neural network, the final output will have a calculated value\\n \\nin the “cat” output neuron, and a calculated value in the “dog” output neuron. The output neuron\\n \\nthat received the highest score becomes the class prediction for the image used as input.\\n \\n \\nFig 1.12:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 15}),\n",
       " Document(page_content='Fig 1.12:\\n\\u200b\\n Visual depiction of passing image data through a neural network, getting a classification\\n \\n \\nAnim 1.11-1.12:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/qtb', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 15}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n18\\n \\nThe thing that makes neural networks appear challenging is the math involved and how scary it\\n \\ncan sometimes look. For example, let’s imagine a neural network, and take a journey through\\n \\nwhat’s going on during a simple forward pass of data, and the math behind it. Neural networks\\n \\nare really only a bunch of math equations that we, programmers, can turn into code. For this, do', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 16}),\n",
       " Document(page_content='not worry about understanding everything. The idea here is to give you a high-level impression\\n \\nof what’s going on overall. Then, this book’s purpose is to break down each of these elements\\n \\ninto painfully simple explanations, which will cover both forward and backward passes involved\\n \\nin training neural networks.\\n \\nWhen represented as one giant function, an example of a neural network’s forward pass would be\\n \\ncomputed with:\\n \\n \\nFig 1.13:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 16}),\n",
       " Document(page_content='Fig 1.13:\\n\\u200b\\n Full formula for the forward pass of an example neural network model.\\n \\n \\nAnim 1.13:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/vkt\\n \\nNaturally, that looks extremely confusing, and the above is actually the easy part of neural\\n \\nnetworks. This turns people away, understandably. In this book, however, we’re going to be\\n \\ncoding everything from scratch, and, when doing this, you should find that there’s no step along', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 16}),\n",
       " Document(page_content='the way to producing the above function that is very challenging to understand. For example, the\\n \\nabove function can also be represented in nested python functions like:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 16}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n19\\n \\n \\nFig 1.14:\\n\\u200b\\n Python code for the forward pass of an example neural network model.\\n \\nThere may be some functions there that you don’t understand yet. For example, maybe you do not\\n \\nknow what a log function is, but this is something simple that we’ll cover. Then we have a sum\\n \\noperation, an exponentiating operation (again, you may not exactly know what this does, but it’s', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 17}),\n",
       " Document(page_content='nothing hard). Then we have a dot product, which is still just about understanding how it works,\\n \\nthere’s nothing there that is over your head if you know how multiplication works! Finally, we\\n \\nhave some transposes, noted as .T, which, again, once you learn what that operation does, is not a\\n \\nchallenging concept. Once we’ve separated each of these elements, learning what they do and\\n \\nhow they work, suddenly, things will not appear to be as daunting or foreign. Nothing in this', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 17}),\n",
       " Document(page_content='forward pass requires education beyond basic high school algebra! For an animation that depicts\\n \\nhow all of this works in Python, you can check out the following animation, but it’s certainly not\\n \\nexpected that you’d immediately understand what’s going on. The point is that this seemingly\\n \\ncomplex topic can be broken down into small, easy to understand parts, which is the purpose of\\n \\nthe coming chapters!', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 17}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n \\n20\\n \\n \\nAnim 1.14:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/vkr\\n \\nA typical neural network has thousands or even up to millions of adjustable \\n\\u200b\\nparameters\\n\\u200b\\n (weights\\n \\nand biases). In this way, neural networks act as enormous functions with vast numbers of\\n \\nparameters\\n\\u200b\\n. The concept of a long function with millions of variables that could be used to solve\\n \\na problem isn’t all too difficult. With that many variables related to neurons, arranged as', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 18}),\n",
       " Document(page_content='interconnected layers, we can imagine there exist some combinations of values for these variables\\n \\nthat will yield desired outputs. Finding that combination of parameter (weight and bias) values is\\n \\nthe challenging part.\\n \\nThe end goal for neural networks is to adjust their weights and biases (the parameters), so when\\n \\napplied to a yet-unseen example in the input, they produce the desired output. When supervised', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 18}),\n",
       " Document(page_content='machine learning algorithms are trained, we show the algorithm examples of inputs and their\\n \\nassociated desired outputs. One major issue with this concept is \\n\\u200b\\noverfitting\\n\\u200b\\n — when the\\n \\nalgorithm only learns to fit the training data but doesn’t actually “understand” anything about\\n \\nunderlying input-output dependencies. The network basically just “memorizes” the training data.\\n \\nThus, we tend to use “in-sample” data to train a model and then use “out-of-sample” data to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 18}),\n",
       " Document(page_content='validate an algorithm (or a neural network model in our case). Certain percentages are set aside\\n \\nfor both datasets to partition the data. For example, if there is a dataset of 100,000 samples of data\\n \\nand labels, you will immediately take 10,000 and set them aside to be your “out-of-sample” or\\n \\n“validation” data. You will then train your model with the other 90,000 in-sample or “training”\\n \\ndata and finally validate your model with the 10,000 out-of-sample data that the model hasn’t yet', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 18}),\n",
       " Document(page_content='seen. The goal is for the model to not only accurately predict on the training data, but also to be\\n \\nsimilarly accurate while predicting on the withheld out-of-sample validation data.\\n \\nThis is called \\n\\u200b\\ngeneralization\\n\\u200b\\n, which means learning to fit the data instead of memorizing it. The\\n \\nidea is that we “train” (slowly adjusting weights and biases) a neural network on many examples\\n \\nof data. We then take out-of-sample data that the neural network has never been presented with', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 18}),\n",
       " Document(page_content='and hope it can accurately predict on these data too.\\n \\nYou should now have a general understanding of what neural networks are, or at least what the\\n \\nobjective is, and how we plan to meet this objective. To train these neural networks, we calculate', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 18}),\n",
       " Document(page_content='Preface - Neural Networks from Scratch in Python\\n21 \\nhow “wrong” they are using algorithms to calculate the error (called \\u200bloss \\u200b), and attempt to slowly  \\nadjust their parameters (weights and biases) so that, over many iterations, the network gradually  \\nbecomes less wrong. The goal of all neural networks is to generalize, meaning the network can  \\nsee many examples of never-before-seen data, and accurately output the values we hope to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 19}),\n",
       " Document(page_content='achieve. Neural networks can be used for more than just classification. They can perform  \\nregression (predict a scalar, singular, value), clustering (assign unstructured data into groups), and  \\nmany other tasks. Classification is just a common task for neural networks.  \\nSupplementary Material: \\u200bhttps://nnfs.io/ch1  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 19}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n6 \\nChapter 2\\nCoding Our First Neurons  \\nWhile we assume that we’re all beyond beginner programmers here, we will still try to start  \\nslowly and explain things the first time we see them. To begin, we will be using \\u200bPython 3.7  \\n(although any version of Python 3+ will likely work). We will also be using \\u200bNumPy \\u200b after  \\nshowing the pure-Python methods and Matplotlib for some visualizations. It should be the case', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 20}),\n",
       " Document(page_content='that a huge variety of versions should work, but you may wish to match ours exactly to rule out  \\nany version issues. Specifically, we are using:  \\nPython 3.7.5  \\nNumPy 1.15.0  \\nMatplotlib 3.1.1  \\nSince this is a \\u200bNeural Networks from Scratch in Python \\u200b book, we will demonstrate how to do  \\nthings without NumPy as well, but NumPy is Python’s all-things-numbers package. Building  \\nfrom scratch is the point of this book though ignoring NumPy would be a disservice since it is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 20}),\n",
       " Document(page_content='among the most, if not the most, important and useful packages for data science in Python.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 20}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n7\\n \\n \\nA Single Neuron\\n \\nLet’s say we have a single neuron, and there are three inputs to this neuron. As in most cases,\\n \\nwhen you initialize parameters in neural networks, our network will have weights initialized\\n \\nrandomly, and biases set as zero to start. Why we do this will become apparent later on. The input\\n \\nwill be either actual training data or the outputs of neurons from the previous layer in the neural', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 21}),\n",
       " Document(page_content='network. We’re just going to make up values to start with as input for now:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]\\n \\n \\nEach input also needs a weight associated with it. Inputs are the data that we pass into the model\\n \\nto get desired outputs, while the weights are the parameters that we’ll tune later on to get these\\n \\nresults. Weights are one of the types of values that change inside the model during the training', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 21}),\n",
       " Document(page_content='phase, along with biases that also change during training. The values for weights and biases are\\n \\nwhat get “trained,” and they are what make a model actually work (or not work). We’ll start by\\n \\nmaking up weights for now. Let’s say the first input, at index 0, which is a 1, has a weight of\\n \\n0.2, the second input has a weight of 0.8, and the third input has a weight of -0.5. Our input and\\n \\nweights lists should now be:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.2\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 21}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n]\\n \\n \\nNext, we need the bias. At the moment, we’re modeling a single neuron with three inputs. Since\\n \\nwe’re modeling a single neuron, we only have one bias, as there’s just one bias value per neuron.\\n \\nThe bias is an additional tunable value but is not associated with any input in contrast to the\\n \\nweights. We’ll randomly select a value of 2 as the bias for this example:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 21}),\n",
       " Document(page_content='[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n]\\n \\nbias \\n\\u200b\\n= \\n\\u200b\\n2', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 21}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n8\\n \\nThis neuron sums each input multiplied by that input’s weight, then adds the bias. All the neuron\\n \\ndoes is take the fractions of inputs, where these fractions (weights) are the adjustable parameters,\\n \\nand adds another adjustable parameter — the bias — then outputs the result. Our output would be\\n \\ncalculated up to this point like:\\n \\noutput \\n\\u200b\\n= \\n\\u200b\\n(inputs[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n          \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 22}),\n",
       " Document(page_content='\\u200b\\n+\\n \\n          \\n\\u200b\\ninputs[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n          \\n\\u200b\\ninputs[\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n+ \\n\\u200b\\nbias)\\n \\n \\nprint\\n\\u200b\\n(output)\\n \\n \\n>>>\\n \\n2.3\\n \\n \\nThe output here should be \\n\\u200b\\n2.3\\n\\u200b\\n. We will use \\n\\u200b\\n>>>\\n\\u200b\\n \\n\\u200b\\nto denote output in this book.\\n \\n \\nFig 2.01:\\n\\u200b\\n Visualizing the code that makes up the math of a basic neuron.\\n \\n \\nAnim 2.01:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/bkr', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 22}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n9\\n \\nWhat might we need to change if we have 4 inputs, rather than the 3 we’ve just shown? Next to\\n \\nthe additional input, we need to add an associated weight, which this new input will be multiplied\\n \\nwith. We’ll make up a value for this new weight as well. Code for this data could be:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1.0\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 23}),\n",
       " Document(page_content='\\u200b\\n, \\n\\u200b\\n1.0\\n\\u200b\\n]\\n \\nbias \\n\\u200b\\n= \\n\\u200b\\n2.0\\n \\n \\nWhich could be depicted visually as:\\n \\n \\nFig 2.02:\\n\\u200b\\n Visualizing how the inputs, weights, and biases from the code interact with the neuron.\\n \\n \\nAnim 2.02:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/djp\\n \\nAll together in code, including the new input and weight, to produce output:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1.0\\n\\u200b\\n]\\n \\nbias \\n\\u200b\\n= \\n\\u200b\\n2.0\\n \\n \\noutput \\n\\u200b\\n= \\n\\u200b\\n(inputs[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights[', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 23}),\n",
       " Document(page_content='\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n          \\n\\u200b\\ninputs[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n          \\n\\u200b\\ninputs[\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n          \\n\\u200b\\ninputs[\\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights[\\n\\u200b\\n3\\n\\u200b\\n] \\n\\u200b\\n+ \\n\\u200b\\nbias)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 23}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n10\\n \\n \\nprint\\n\\u200b\\n(output)\\n \\n \\n>>>\\n \\n4.8\\n \\n \\nVisually:\\n \\n \\nFig 2.03:\\n\\u200b\\n Visualizing the code that makes up a basic neuron, with 4 inputs this time.\\n \\n \\nAnim 2.03:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/djp', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 24}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n11\\n \\n \\nA Layer of Neurons\\n \\nNeural networks typically have layers that consist of more than one neuron. Layers are nothing\\n \\nmore than groups of neurons. Each neuron in a layer takes exactly the same input — the input\\n \\ngiven to the layer (which can be either the training data or the output from the previous layer),\\n \\nbut contains its own set of weights and its own bias, producing its own unique output. The layer’s', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 25}),\n",
       " Document(page_content='output is a set of each of these outputs — one per each neuron. Let’s say we have a scenario with\\n \\n3 neurons in a layer and 4 inputs:\\n \\n \\nFig 2.04:\\n\\u200b\\n Visualizing a layer of neurons with common input.\\n \\n \\nAnim 2.04:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/mxo', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 25}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n12\\n \\nWe’ll keep the initial 4 inputs and set of weights for the first neuron the same as we’ve been using\\n \\nso far. We’ll add 2 additional, made up, sets of weights and 2 additional biases to form 2 new\\n \\nneurons for a total of 3 in the layer. The layer’s output is going to be a list of 3 values, not just a\\n \\nsingle value like for a single neuron.\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n \\n \\nweights1', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 26}),\n",
       " Document(page_content='\\u200b\\n]\\n \\n \\nweights1 \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n]\\n \\nweights2 \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n]\\n \\nweights3 \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]\\n \\n \\nbias1 \\n\\u200b\\n= \\n\\u200b\\n2\\n \\nbias2 \\n\\u200b\\n= \\n\\u200b\\n3\\n \\nbias3 \\n\\u200b\\n= \\n\\u200b\\n0.5\\n \\n \\noutputs \\n\\u200b\\n= \\n\\u200b\\n[\\n \\n        \\n\\u200b\\n# Neuron 1:\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights1[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights1[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights1[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 26}),\n",
       " Document(page_content='\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights1[\\n\\u200b\\n3\\n\\u200b\\n] \\n\\u200b\\n+ \\n\\u200b\\nbias1,\\n \\n \\n        \\n\\u200b\\n# Neuron 2:\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights2[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights2[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights2[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights2[\\n\\u200b\\n3\\n\\u200b\\n] \\n\\u200b\\n+ \\n\\u200b\\nbias2,\\n \\n \\n        \\n\\u200b\\n# Neuron 3:\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights3[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights3[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 26}),\n",
       " Document(page_content='\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights3[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n        \\n\\u200b\\ninputs[\\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nweights3[\\n\\u200b\\n3\\n\\u200b\\n] \\n\\u200b\\n+ \\n\\u200b\\nbias3]\\n \\n \\nprint\\n\\u200b\\n(outputs)\\n \\n \\n>>>\\n \\n[\\n\\u200b\\n4.8\\n\\u200b\\n, \\n\\u200b\\n1.21\\n\\u200b\\n, \\n\\u200b\\n2.385\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 26}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n13\\n \\n \\nFig 2.04.2:\\n\\u200b\\n Code, math and visuals behind a layer of neurons.\\n \\n \\nAnim 2.04:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/mxo\\n \\nIn this code, we have three sets of weights and three biases, which define three neurons. Each\\n \\nneuron is “connected” to the same inputs. The difference is in the separate weights and bias\\n \\nthat each neuron applies to the input. This is called a \\n\\u200b\\nfully connected\\n\\u200b\\n neural network — every', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 27}),\n",
       " Document(page_content='neuron in the current layer has connections to every neuron from the previous layer. This is a\\n \\nvery common type of neural network, but it should be noted that there is no requirement to\\n \\nfully connect everything like this. At this point, we have only shown code for a single layer\\n \\nwith very few neurons. Imagine coding many more layers and more neurons. This would get\\n \\nvery challenging to code using our current methods. Instead, we could use a loop to scale and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 27}),\n",
       " Document(page_content='handle dynamically-sized inputs and layers. We’ve turned the separate weight variables into a\\n \\nlist of weights so we can iterate over them, and we changed the code to use loops instead of\\n \\nthe hardcoded operations.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 27}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n14\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]\\n \\nbiases \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n]\\n \\n \\n# Output of current layer\\n \\nlayer_outputs \\n\\u200b\\n= \\n\\u200b\\n[]\\n \\n# For each neuron\\n \\nfor \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 28}),\n",
       " Document(page_content='for \\n\\u200b\\nneuron_weights, neuron_bias \\n\\u200b\\nin \\n\\u200b\\nzip\\n\\u200b\\n(weights, biases):\\n \\n    \\n\\u200b\\n# Zeroed output of given neuron\\n \\n    \\n\\u200b\\nneuron_output \\n\\u200b\\n= \\n\\u200b\\n0\\n \\n    \\n\\u200b\\n# For each input and weight to the neuron\\n \\n    \\n\\u200b\\nfor \\n\\u200b\\nn_input, weight \\n\\u200b\\nin \\n\\u200b\\nzip\\n\\u200b\\n(inputs, neuron_weights):\\n \\n        \\n\\u200b\\n# Multiply this input by associated weight\\n \\n        # and add to the neuron’s output variable\\n \\n        \\n\\u200b\\nneuron_output \\n\\u200b\\n+= \\n\\u200b\\nn_input\\n\\u200b\\n*\\n\\u200b\\nweight\\n \\n    \\n\\u200b\\n# Add bias\\n \\n    \\n\\u200b\\nneuron_output \\n\\u200b\\n+= \\n\\u200b\\nneuron_bias', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 28}),\n",
       " Document(page_content='+= \\n\\u200b\\nneuron_bias\\n \\n    \\n\\u200b\\n# Put neuron’s result to the layer’s output list\\n \\n    \\n\\u200b\\nlayer_outputs.append(neuron_output)\\n \\n \\nprint\\n\\u200b\\n(layer_outputs)\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n4.8\\n\\u200b\\n, \\n\\u200b\\n1.21\\n\\u200b\\n, \\n\\u200b\\n2.385\\n\\u200b\\n]\\n \\n \\nThis does the same thing as before, just in a more dynamic and scalable way. If you find yourself\\n \\nconfused at one of the steps, \\n\\u200b\\nprint\\n\\u200b\\n()\\n\\u200b\\n out the objects to see what they are and what’s happening.\\n \\nThe \\n\\u200b\\nzip\\n\\u200b\\n()\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 28}),\n",
       " Document(page_content='The \\n\\u200b\\nzip\\n\\u200b\\n()\\n\\u200b\\n function lets us iterate over multiple iterables (lists in t his case) simultaneously.\\n \\nAgain, all we’re doing is, for each neuron (the outer loop in the code above, over neuron weights\\n \\nand biases), taking each input value multiplied by the associated weight for that input (the inner\\n \\nloop in the code above, over inputs and weights), adding all of these together, then adding a bias\\n \\nat the end. Finally, sending the neuron’s output to the layer’s output list.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 28}),\n",
       " Document(page_content='That’s it! How do we know we have three neurons? Why do we have three? We can tell we have\\n \\nthree neurons because there are 3 sets of weights and 3 biases. When you make a neural network\\n \\nof your own, you also get to decide how many neurons you want for each of the layers. You can\\n \\ncombine however many inputs you are given with however many neurons that you desire. As you\\n \\nprogress through this book, you will gain some intuition of how many neurons to try using. We', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 28}),\n",
       " Document(page_content='will start by using trivial numbers of neurons to aid in understanding how neural networks work\\n \\nat their core.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 28}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n15\\n \\nWith our above code that uses loops, we could modify our number of inputs or neurons in our\\n \\nlayer to be whatever we wanted, and our loop would handle it. As we said earlier, it would be\\n \\na disservice not to show NumPy here since Python alone doesn’t do matrix/tensor/array math\\n \\nvery efficiently.  But first, the reason the most popular deep learning library in Python is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 29}),\n",
       " Document(page_content='called “TensorFlow” is that it’s all about doing operations on \\n\\u200b\\ntensors\\n\\u200b\\n.\\n \\nTensors, Arrays and Vectors\\n \\nWhat are “tensors?”\\n \\nTensors are \\n\\u200b\\nclosely-related to\\n\\u200b\\n arrays. If you interchange tensor/array/matrix when it comes to\\n \\nmachine learning, people probably won’t give you too hard of a time. But there are subtle\\n \\ndifferences, and they are primarily either the context or attributes of the tensor object. To', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 29}),\n",
       " Document(page_content='understand a tensor, let’s compare and describe some of the other data containers in Python\\n \\n(things that hold data). Let’s start with a list. A Python list is defined by comma-separated\\n \\nobjects contained in brackets. So far, we’ve been using lists.\\n \\nThis is an example of a simple list:\\n \\nl \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n]\\n \\n \\nA list of lists:\\n \\nlol \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n3\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n]]\\n \\n \\nA list of lists of lists!\\n \\nlolol \\n\\u200b\\n=', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 29}),\n",
       " Document(page_content='lolol \\n\\u200b\\n= \\n\\u200b\\n[[[\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n],\\n \\n          [\\n\\u200b\\n3\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n]],\\n \\n         [[\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n],\\n \\n          [\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n,\\n\\u200b\\n8\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n]],\\n \\n         [[\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n8\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n],\\n \\n          [\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n9\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n]]]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 29}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n16\\n \\nEverything shown so far could also be an array or an array representation of a tensor. A list is just\\n \\na list, and it can do pretty much whatever it wants, including:\\n \\nanother_list_of_lists \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n4\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n],\\n \\n                         [\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n]]\\n \\n \\nThe above list of lists cannot be an array because it is not \\n\\u200b\\nhomologous\\n\\u200b\\n. A list of lists is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 30}),\n",
       " Document(page_content='homologous if each list along a dimension is identically long, and this must be true for each\\n \\ndimension. In the case of the list shown above, it’s a 2-dimensional list. The first dimension’s\\n \\nlength is the number of sublists in the total list (2). The second dimension is the length of each of\\n \\nthose sublists (3, then 2). In the above example, when reading across the “row” dimension (also\\n \\ncalled the second dimension), the first list is 3 elements long, and the second list is 2 elements', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 30}),\n",
       " Document(page_content='long — this is not homologous and, therefore, cannot be an array. While failing to be consistent in\\n \\none dimension is enough to show that this example is not homologous, we could also read down\\n \\nthe “column” dimension (the first dimension); the first two columns are 2 elements long while the\\n \\nthird column only contains 1 element. Note that every dimension does not necessarily need to be\\n \\nthe same length; it is perfectly acceptable to have an array with 4 rows and 3 columns (i.e., 4x3).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 30}),\n",
       " Document(page_content='A matrix is pretty simple. It’s a rectangular array. It has columns and rows. It is two dimensional.\\n \\nSo a matrix can be an array (a 2D array). Can all arrays be matrices? No. An array can be far\\n \\nmore than just columns and rows, as it could have four dimensions, twenty dimensions, and so on.\\n \\nlist_matrix_array \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n4\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n],\\n \\n                     [\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n],\\n \\n                     [\\n\\u200b\\n8\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n]]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 30}),\n",
       " Document(page_content='8\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n]]\\n \\n \\nThe above list could also be a valid matrix (because of its columns and rows), which\\n \\nautomatically means it could also be an array. The “shape” of this array would be 3x2, or more\\n \\nformally described as a shape of \\n\\u200b\\n(3, 2) \\n\\u200b\\nas it has 3 rows and 2 columns.\\n \\nTo denote a shape, we need to check every dimension. As we’ve already learned, a matrix is a\\n \\n2-dimensional array. The first dimension is what’s inside the most outer brackets, and if we look', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 30}),\n",
       " Document(page_content='at the above matrix, we can see 3 lists there: \\n\\u200b\\n[\\n\\u200b\\n4\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n, \\n\\u200b\\n[\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n, and \\n\\u200b\\n[\\n\\u200b\\n8\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n; thus, the size in this\\n \\ndimension is 3 and each of those lists has to be the same shape to form an array (and matrix in this\\n \\ncase). The next dimension’s size is the number of elements inside this more inner pair of brackets,\\n \\nand we see that it’s 2 as all of them contain 2 elements.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 30}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n17\\n \\nWith 3-dimensional arrays, like in \\n\\u200b\\nlolol\\n\\u200b\\n below, we’ll have a 3rd level of brackets:\\n \\nlolol \\n\\u200b\\n= \\n\\u200b\\n[[[\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n],\\n \\n          [\\n\\u200b\\n3\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n]],\\n \\n         [[\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n],\\n \\n          [\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n,\\n\\u200b\\n8\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n]],\\n \\n         [[\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n8\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n],\\n \\n          [\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n9\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n]]]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 31}),\n",
       " Document(page_content='9\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n]]]\\n \\n \\nThe first level of this array contains 3 matrices:\\n \\n        [[\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n],\\n \\n         [\\n\\u200b\\n3\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n]]\\n \\n \\n        [[\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n],\\n \\n         [\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n,\\n\\u200b\\n8\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n]]\\n \\n \\nAnd\\n \\n        [[\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n8\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n],\\n \\n         [\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n9\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n]]\\n \\n \\nThat’s what’s inside the most outer brackets and the size of this dimension is then 3. If we look at', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 31}),\n",
       " Document(page_content='the first matrix, we can see that it contains 2 lists \\n\\u200b\\n—\\n\\u200b\\n \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n6\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n and \\n\\u200b\\n[\\n\\u200b\\n3\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n so the size of\\n \\nthis dimension is 2 \\n\\u200b\\n—\\n\\u200b\\n while each list of this inner matrix includes 4 elements. These 4 elements\\n \\nmake up the 3rd and last dimension of this matrix since there are no more inner brackets.\\n \\nTherefore, the shape of this array is \\n\\u200b\\n(3, 2, 4)\\n\\u200b\\n and it’s a 3-dimensional array, since the shape\\n \\ncontains 3 dimensions.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 31}),\n",
       " Document(page_content='Fig 2.05:\\n\\u200b\\n Example of a 3-dimensional array.\\n \\n \\nAnim 2.05:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/jps', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 31}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n18\\n \\nFinally, what’s a tensor? When it comes to the discussion of tensors versus arrays in the context\\n \\nof computer science, pages and pages of debate have ensued. This intense debate appears to be\\n \\ncaused by the fact that people are arguing from entirely different places. There’s no question that\\n \\na tensor is not just an array, but the real question is: “What is a tensor, to a computer scientist, in', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 32}),\n",
       " Document(page_content='the context of deep learning?” We believe that we can solve the debate in one line:\\n \\nA tensor object is an object that can be represented as an array.\\n \\nWhat this means is, as programmers, we can (and will) treat tensors as arrays in the context of\\n \\ndeep learning, and that’s really all the thought we have to put into it. Are all tensors \\n\\u200b\\njust\\n\\u200b\\n arrays?\\n \\nNo, but they are represented as arrays in our code, so, to us, they’re only arrays, and this is why', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 32}),\n",
       " Document(page_content='there’s so much argument and confusion.\\n \\nNow, what is an array? In this book, we define an array as an ordered homologous container for\\n \\nnumbers, and mostly use this term when working with the NumPy package since that’s what the\\n \\nmain data structure is called within it. A linear array, also called a 1-dimensional array, is the\\n \\nsimplest example of an array, and in plain Python, this would be a list. Arrays can also consist', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 32}),\n",
       " Document(page_content='of multi-dimensional data, and one of the best-known examples is what we call a matrix in\\n \\nmathematics, which we’ll represent as a 2-dimensional array. Each element of the array can be\\n \\naccessed using a tuple of indices as a key, which means that we can retrieve any array element.\\n \\nWe need to learn one more notion \\n\\u200b\\n—\\n\\u200b\\n a vector. Put simply, a vector in math is what we call a list\\n \\nin Python or a 1-dimensional array in NumPy. Of course, lists and NumPy arrays do not have', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 32}),\n",
       " Document(page_content='the same properties as a vector, but, just as we can write a matrix as a list of lists in Python, we\\n \\ncan also write a vector as a list or an array! Additionally, we’ll look at the vector algebraically\\n \\n(mathematically) as a set of numbers in brackets. This is in contrast to the physics perspective,\\n \\nwhere the vector’s representation is usually seen as an arrow, characterized by a magnitude and\\n \\na direction.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 32}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n19\\n \\n \\nDot Product and Vector Addition\\n \\nLet’s now address vector multiplication, as that’s one of the most important operations we’ll\\n \\nperform on vectors. We can achieve the same result as in our pure Python implementation of\\n \\nmultiplying each element in our inputs and weights vectors element-wise by using a \\n\\u200b\\ndot product\\n\\u200b\\n,\\n \\nwhich we’ll explain shortly. Traditionally, we use dot products for \\n\\u200b\\nvectors\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 33}),\n",
       " Document(page_content='\\u200b\\nvectors\\n\\u200b\\n (yet another name for\\n \\na container), and we can certainly refer to what we’re doing here as working with vectors just as\\n \\nwe can call them “tensors.” Nevertheless, this seems to add to the mysticism of neural networks\\n \\n— like they’re these objects out in a complex multi-dimensional vector space that we’ll never\\n \\nunderstand. Keep thinking of vectors as arrays \\n\\u200b\\n—\\n\\u200b\\n a 1-dimensional array is just a vector (or a list\\n \\nin Python).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 33}),\n",
       " Document(page_content='in Python).\\n \\nBecause of the sheer number of variables and interconnections made, we can model very complex\\n \\nand non-linear relationships with non-linear activation functions, and truly feel like wizards, but\\n \\nthis might do more harm than good. Yes, we will be using the “dot product,” but we’re doing this\\n \\nbecause it results in a clean way to perform the necessary calculations. It’s nothing more in-depth', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 33}),\n",
       " Document(page_content='than that — as you’ve already seen, we can do this math with far more rudimentary-sounding\\n \\nwords. When multiplying vectors, you either perform a dot product or a cross product. A cross\\n \\nproduct results in a vector while a dot product results in a scalar (a single value/number).\\n \\nFirst, let’s explain what a dot product of two vectors is. Mathematicians would say:\\n \\n \\n \\nA dot product of two vectors is a sum of products of consecutive vector elements. Both vectors', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 33}),\n",
       " Document(page_content='must be of the same size (have an equal number of elements).\\n \\nLet’s write out how a dot product is calculated in Python. For it, you have two vectors, which we\\n \\ncan represent as lists in Python. We then multiply their elements from the same index values and\\n \\nthen add all of the resulting products. Say we have two lists acting as our vectors:\\n \\na \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n]\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n3\\n\\u200b\\n,\\n\\u200b\\n4\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 33}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n20\\n \\nTo obtain the dot product:\\n \\ndot_product \\n\\u200b\\n= \\n\\u200b\\na[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nb[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n+ \\n\\u200b\\na[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nb[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+ \\n\\u200b\\na[\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\nb[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\nprint\\n\\u200b\\n(dot_product)\\n \\n \\n>>>\\n \\n20\\n \\n \\nFig 2.06:\\n\\u200b\\n Math behind the dot product example.\\n \\n \\nAnim 2.06:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/xpo\\n \\nNow, what if we called \\n\\u200b\\na\\n\\u200b\\n “inputs” and \\n\\u200b\\nb \\n\\u200b\\n“weights?” Suddenly, this dot product looks like a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 34}),\n",
       " Document(page_content='succinct way to perform the operations we need and have already performed in plain Python. We\\n \\nneed to multiply our weights and inputs of the same index values and add the resulting values\\n \\ntogether. The dot product performs this exact type of operation; thus, it makes lots of sense to use\\n \\nhere. Returning to the neural network code, let’s make use of this dot product. Plain Python does\\n \\nnot contain methods or functions to perform such an operation, so we’ll use the NumPy package,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 34}),\n",
       " Document(page_content='which is capable of this, and many more operations that we’ll use in the future.\\n \\nWe’ll also need to perform a vector addition operation in the not-too-distant future. Fortunately,\\n \\nNumPy lets us perform this in a natural way — using the plus sign with the variables containing\\n \\nvectors of the data. The addition of the two vectors is an operation performed element-wise,\\n \\nwhich means that both vectors have to be of the same size, and the result will become a vector of\\n \\nthis', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 34}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n21\\n \\nsize as well. The result is a vector calculated as a sum of the consecutive vector elements:\\n \\n \\nA Single Neuron with NumPy\\n \\nLet’s code the solution, for a single neuron to start, using the dot product and the addition of the\\n \\nvectors with NumPy. This makes the code much simpler to read and write (and faster to run):\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 35}),\n",
       " Document(page_content='3.0\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1.0\\n\\u200b\\n]\\n \\nbias \\n\\u200b\\n= \\n\\u200b\\n2.0\\n \\n \\n \\noutputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(weights, inputs) \\n\\u200b\\n+ \\n\\u200b\\nbias\\n \\n \\nprint\\n\\u200b\\n(outputs)\\n \\n \\n>>>\\n \\n4.8\\n \\n \\nFig 2.07:\\n\\u200b\\n Visualizing the math of the dot product of inputs and weights for a single neuron.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 35}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n22\\n \\n \\nFig 2.08:\\n\\u200b\\n Visualizing the math summing the dot product and bias.\\n \\n \\nAnim 2.07-2.08:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/blq\\n \\nA Layer of Neurons with NumPy\\n \\nNow we’re back to the point where we’d like to calculate the output of a layer of 3 neurons,\\n \\nwhich means the weights will be a matrix or list of weight vectors. In plain Python, we wrote this', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 36}),\n",
       " Document(page_content='as a list of lists. With NumPy, this will be a 2-dimensional array, which we’ll call a matrix.\\n \\nPreviously with the 3-neuron example, we performed a multiplication of those weights with a list\\n \\ncontaining inputs, which resulted in a list of output values \\n\\u200b\\n—\\n\\u200b\\n one per neuron.\\n \\nWe also described the dot product of two vectors, but the weights are now a matrix, and we need\\n \\nto perform a dot product of them and the input vector. NumPy makes this very easy for us \\n\\u200b\\n—', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 36}),\n",
       " Document(page_content='\\u200b\\n—\\n \\ntreating this matrix as a list of vectors and performing the dot product one by one with the vector\\n \\nof inputs, returning a list of dot products.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 36}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n23\\n \\nThe dot product’s result, in our case, is a vector (or a list) of sums of the weight and input\\n \\nproducts for each of the neurons. From here, we still need to add corresponding biases to them.\\n \\nThe biases can be easily added to the result of the dot product operation as they are a vector of the\\n \\nsame size. We can also use the plain Python list directly here, as NumPy will convert it to an\\n \\narray internally.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 37}),\n",
       " Document(page_content='array internally.\\n \\nPreviously, we had calculated outputs of each neuron by performing a dot product and adding a\\n \\nbias, one by one. Now we have changed the order of those operations \\n\\u200b\\n—\\n\\u200b\\n we’re performing dot\\n \\nproduct first as one operation on all neurons and inputs, and then we are adding a bias in the next\\n \\noperation. When we add two vectors using NumPy, each i-th element is added together, resulting', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 37}),\n",
       " Document(page_content='in a new vector of the same size. This is both a simplification and an optimization, giving us\\n \\nsimpler and faster code.\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]\\n \\nbiases \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n]\\n \\n \\n \\nlayer_outputs \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 37}),\n",
       " Document(page_content='layer_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(weights, inputs) \\n\\u200b\\n+ \\n\\u200b\\nbiases\\n \\n \\nprint\\n\\u200b\\n(layer_outputs)\\n \\n \\n>>>\\n \\narray([\\n\\u200b\\n4.8   1.21  2.385\\n\\u200b\\n])\\n \\n \\n \\nFig 2.09:\\n\\u200b\\n Code and visuals for the dot product applied to the layer of neurons.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 37}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n24\\n \\n \\nFig 2.10:\\n\\u200b\\n Code and visuals for the sum of the dot product and bias with a layer of neurons.\\n \\n \\nAnim 2.09-2.10:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/cyx\\n \\nThis syntax involving the dot product of weights and inputs followed by the vector addition of\\n \\nbias is the most commonly used way to represent this calculation of \\n\\u200b\\ninputs·weights+bias\\n\\u200b\\n. To\\n \\nexplain the order of parameters we are passing into \\n\\u200b\\nnp.dot()\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 38}),\n",
       " Document(page_content='\\u200b\\nnp.dot()\\n\\u200b\\n, we should think of it as whatever\\n \\ncomes first will decide the output shape. In our case, we are passing a list of neuron weights first\\n \\nand then the inputs, as our goal is to get a list of neuron outputs. As we mentioned, a dot product\\n \\nof a matrix and a vector results in a list of dot products. The \\n\\u200b\\nnp.dot()\\n\\u200b\\n method treats the matrix as\\n \\na list of vectors and performs a dot product of each of those vectors with the other vector. In this', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 38}),\n",
       " Document(page_content='example, we used that property to pass a matrix, which was a list of neuron weight vectors and a\\n \\nvector of inputs and get a list of dot products \\n\\u200b\\n—\\n\\u200b\\n neuron outputs.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 38}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n25\\n \\n \\nA Batch of Data\\n \\nTo train, neural networks tend to receive data in \\n\\u200b\\nbatches.\\n\\u200b\\n So far, the example input data have\\n \\nbeen only one sample (or \\n\\u200b\\nobservation\\n\\u200b\\n) of various features called a feature set:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n \\n \\nHere, the \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n\\u200b\\n \\n\\u200b\\ndata are somehow meaningful and descriptive to the output we', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 39}),\n",
       " Document(page_content='desire. Imagine each number as a value from a different sensor, from the example in chapter 1,\\n \\nall simultaneously. Each of these values is a feature observation datum, and together they form a\\n \\nfeature set instance\\n\\u200b\\n, also called an \\n\\u200b\\nobservation\\n\\u200b\\n, or most commonly, a \\n\\u200b\\nsample\\n\\u200b\\n.\\n \\n \\n \\nFig 2.11:\\n\\u200b\\n Visualizing a 1D array.\\n \\n \\nAnim 2.11:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/lqw\\n \\nOften, neural networks expect to take in many \\n\\u200b\\nsamples\\n\\u200b\\n at a time for two reasons. One reason', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 39}),\n",
       " Document(page_content='is that it’s faster to train in batches in parallel processing, and the other reason is that batches', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 39}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n26\\n \\nhelp with generalization during training. If you fit (perform a step of a training process) on one\\n \\nsample at a time, you’re highly likely to keep fitting to that individual sample, rather than\\n \\nslowly producing general tweaks to weights and biases that fit the entire dataset. Fitting or\\n \\ntraining in batches gives you a higher chance of making more meaningful changes to weights', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 40}),\n",
       " Document(page_content='and biases. For the concept of fitment in batches rather than one sample at a time, the\\n \\nfollowing animation can help:\\n \\n \\nFig 2.12:\\n\\u200b\\n Example of a linear equation fitting batches of 32 chosen samples. See animation below\\n \\nfor other sizes of samples at a time to see how much of a difference batch size can make.\\n \\n \\nAnim 2.12:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/vyu\\n \\nAn example of a batch of data could look like:\\n \\n \\nFig 2.13:\\n\\u200b\\n Example of a batch, its shape, and type.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 40}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n27\\n \\n \\nAnim 2.13:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/lqw\\n \\nRecall that in Python, and in our case, lists are useful containers for holding a sample as well\\n \\nas multiple samples that make up a batch of observations. Such an example of a batch of\\n \\nobservations, each with its own sample, looks like:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n], [\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n], [\\n\\u200b\\n-\\n\\u200b\\n1.5\\n\\u200b\\n, \\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n3.3', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 41}),\n",
       " Document(page_content=', \\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.8\\n\\u200b\\n]]\\n \\nThis list of lists could be made into an array since it is homologous. Note that each “list” in this\\n \\nlarger list is a sample representing a feature set. \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n]\\n\\u200b\\n, \\n\\u200b\\n[\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n, and\\n \\n[\\n\\u200b\\n-\\n\\u200b\\n1.5\\n\\u200b\\n, \\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.8\\n\\u200b\\n]\\n\\u200b\\n are all \\n\\u200b\\nsamples\\n\\u200b\\n, and are also referred to as \\n\\u200b\\nfeature set instances \\n\\u200b\\nor\\n \\nobservations\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 41}),\n",
       " Document(page_content='observations\\n\\u200b\\n.\\n \\nWe have a matrix of inputs and a matrix of weights now, and we need to perform the dot product\\n \\non them somehow, but how and what will the result be? Similarly, as we performed a dot product\\n \\non a matrix and a vector, we treated the matrix as a list of vectors, resulting in a list of dot\\n \\nproducts. In this example, we need to manage both matrices as lists of vectors and perform dot', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 41}),\n",
       " Document(page_content='products on all of them in all combinations, resulting in a list of lists of outputs, or a matrix; this\\n \\noperation is called the \\n\\u200b\\nmatrix product\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 41}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n28\\n \\n \\nMatrix Product\\n \\nThe \\n\\u200b\\nmatrix product\\n\\u200b\\n is an operation in which we have 2 matrices, and we are performing dot\\n \\nproducts of all combinations of rows from the first matrix and the columns of the 2nd matrix,\\n \\nresulting in a matrix of those atomic \\n\\u200b\\ndot products\\n\\u200b\\n:\\n \\n \\nFig 2.14:\\n\\u200b\\n Visualizing how a single element in the resulting matrix from matrix product is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 42}),\n",
       " Document(page_content='calculated. See animation for the full calculation of each element.\\n \\n \\nAnim 2.14:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/jei', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 42}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n29\\n \\nTo perform a matrix product, the size of the second dimension of the left matrix must match the\\n \\nsize of the first dimension of the right matrix. For example, if the left matrix has a shape of \\n\\u200b\\n(5, 4)\\n \\nthen the right matrix must match this 4 within the first shape value \\n\\u200b\\n(4, 7)\\n\\u200b\\n. The shape of the\\n \\nresulting array is always the first dimension of the left array and the second dimension of the right', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 43}),\n",
       " Document(page_content='array, \\n\\u200b\\n(5, 7)\\n\\u200b\\n. In the above example, the left matrix has a shape of \\n\\u200b\\n(5, 4)\\n\\u200b\\n, and the upper-right matrix\\n \\nhas a shape of \\n\\u200b\\n(4, 5)\\n\\u200b\\n. The second dimension of the left array and the first dimension of the second\\n \\narray are both \\n\\u200b\\n4\\n\\u200b\\n, they match, and the resulting array has a shape of \\n\\u200b\\n(5, 5)\\n\\u200b\\n.\\n \\nTo elaborate, we can also show that we can perform the matrix product on vectors. In\\n \\nmathematics, we can have something called a column vector and row vector, which we’ll explain', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 43}),\n",
       " Document(page_content='better shortly. They’re vectors, but represented as matrices with one of the dimensions having a\\n \\nsize of 1:\\n \\n \\n \\na\\n\\u200b\\n is a row vector. It looks very similar to a vector \\n\\u200b\\na\\n\\u200b\\n (with an arrow above it) described earlier\\n \\nalong with the vector product. The difference in notation between a row vector and vector are\\n \\ncommas between values and the arrow above symbol \\n\\u200b\\na\\n\\u200b\\n is missing on a row vector. It’s called a\\n \\nrow vector as it’s a vector of a row of a matrix. \\n\\u200b\\nb\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 43}),\n",
       " Document(page_content='\\u200b\\nb\\n\\u200b\\n, on the other hand, is called a column vector\\n \\nbecause it’s a column of a matrix. As row and column vectors are technically matrices, we do not\\n \\ndenote them with vector arrows anymore.\\n \\nWhen we perform the matrix product on them, the result becomes a matrix as well, like in the\\n \\nprevious example, but containing just a single value, the same value as in the dot product example\\n \\nwe have discussed previously:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 43}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n30\\n \\n \\nFig 2.15:\\n\\u200b\\n Product of row and column vectors.\\n \\n \\nAnim 2.15:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/bkw\\n \\nIn other words, row and column vectors are matrices with one of their dimensions being of a\\n \\nsize of 1; and, we perform the \\n\\u200b\\nmatrix product\\n\\u200b\\n on them instead of the \\n\\u200b\\ndot product\\n\\u200b\\n, which\\n \\nresults in a matrix containing a single value. In this case, we performed a matrix multiplication\\n \\nof matrices with shapes \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 44}),\n",
       " Document(page_content='\\u200b\\n(1, 3)\\n\\u200b\\n and \\n\\u200b\\n(3, 1)\\n\\u200b\\n, then the resulting array has the shape \\n\\u200b\\n(1, 1)\\n\\u200b\\n or a size of\\n \\n1x1\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 44}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n31\\n \\n \\nTransposition for the Matrix Product\\n \\nHow did we suddenly go from 2 vectors to row and column vectors? We used the relation of the\\n \\ndot product and matrix product saying that a dot product of two vectors equals a matrix product of\\n \\na row and column vector (the arrows above the letters signify that they are vectors):\\n \\n \\nWe also have temporarily used some simplification, not showing that column vector \\n\\u200b\\nb\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 45}),\n",
       " Document(page_content='\\u200b\\nb\\n\\u200b\\n is actually\\n \\na \\n\\u200b\\ntransposed\\n\\u200b\\n vector \\n\\u200b\\nb\\n\\u200b\\n.\\n\\u200b\\n \\n\\u200b\\nThe proper equation, matching the dot product of vectors \\n\\u200b\\na\\n\\u200b\\n and \\n\\u200b\\nb\\n\\u200b\\n written\\n \\nas matrix product should look like:\\n \\n \\nHere we introduced one more new operation \\n\\u200b\\n—\\n\\u200b\\n \\n\\u200b\\ntransposition\\n\\u200b\\n. Transposition simply modifies a\\n \\nmatrix in a way that its rows become columns and columns become rows:\\n \\n \\nFig 2.16:\\n\\u200b\\n Example of an array transposition.\\n \\n \\nAnim 2.16:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/qut', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 45}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n32\\n \\n \\nFig 2.17:\\n\\u200b\\n Another example of an array transposition.\\n \\n \\nAnim 2.17:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/pnq\\n \\nNow we need to get back to row and column vector definitions and update them with what we\\n \\nhave just learned.\\n \\nA row vector is a matrix whose first dimension’s size (the number of rows) equals 1 and the\\n \\nsecond dimension’s size (the number of columns) equals \\n\\u200b\\nn\\n\\u200b\\n — the vector size. In other words, it’s', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 46}),\n",
       " Document(page_content='a 1×n array or array of shape (1, n):\\n \\n \\nWith NumPy and with 3 values, we would define it as:\\n \\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]])\\n \\n \\nNote the use of double brackets here. To transform a list into a matrix containing a single row\\n \\n(perform an equivalent operation of turning a vector into row vector), we can put it into a list and\\n \\ncreate numpy array:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 46}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n33\\n \\na \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]\\n \\nnp.array([a])\\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]])\\n \\n \\nAgain, note that we encase \\n\\u200b\\na\\n\\u200b\\n in brackets before converting to an array in this case.\\n \\nOr we can turn it into a 1D array and expand dimensions using one of the NumPy abilities:\\n \\na \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]\\n \\nnp.expand_dims(np.array(a), \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n0\\n\\u200b\\n)\\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 47}),\n",
       " Document(page_content='\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]])\\n \\n \\nWhere \\n\\u200b\\nnp.expand_dims() \\n\\u200b\\nadds a new dimension at the index of the \\n\\u200b\\naxis\\n\\u200b\\n.\\n \\nA column vector is a matrix where the second dimension’s size equals 1, in other words, it’s an\\n \\narray of shape (n, 1):\\n \\n \\nWith NumPy it can be created the same way as a row vector, but needs to be additionally\\n \\ntransposed \\n\\u200b\\n—\\n\\u200b\\n transposition turns rows into columns and columns into rows:\\n \\n \\n \\nTo turn vector \\n\\u200b\\nb\\n\\u200b\\n into row vector \\n\\u200b\\nb\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 47}),\n",
       " Document(page_content='\\u200b\\nb\\n\\u200b\\n, we’ll use the same method that we used to turn vector \\n\\u200b\\na\\n\\u200b\\n into\\n \\nrow vector \\n\\u200b\\na\\n\\u200b\\n, then we can perform a transposition on it to make it a column vector \\n\\u200b\\nb\\n\\u200b\\n:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 47}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n34\\n \\n \\n \\nWith NumPy code:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\na \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n4\\n\\u200b\\n]\\n \\n \\na \\n\\u200b\\n= \\n\\u200b\\nnp.array([a])\\n \\nb \\n\\u200b\\n= \\n\\u200b\\nnp.array([b]).T\\n \\n \\n \\nnp.dot(a, b)\\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n20\\n\\u200b\\n]])\\n \\n \\nWe have achieved the same result as the dot product of two vectors, but performed on matrices\\n \\nand returning a matrix \\n\\u200b\\n—\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 48}),\n",
       " Document(page_content='\\u200b\\n—\\n\\u200b\\n exactly what we expected and wanted. It’s worth mentioning that\\n \\nNumPy does not have a dedicated method for performing matrix product \\n\\u200b\\n—\\n\\u200b\\n the dot product and\\n \\nmatrix product are both implemented in a single method: \\n\\u200b\\nnp.dot()\\n\\u200b\\n.\\n \\nAs we can see, to perform a matrix product on two vectors, we took one as is, transforming it into\\n \\na row vector, and the second one using transposition on it to turn it into a column vector. That', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 48}),\n",
       " Document(page_content='allowed us to perform a matrix product that returned a matrix containing a single value. We also\\n \\nperformed the matrix product on two example arrays to learn how a matrix product works \\n\\u200b\\n—\\n\\u200b\\n it\\n \\ncreates a matrix of dot products of all combinations of row and column vectors.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 48}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n35\\n \\n \\nA Layer of Neurons & Batch of Data w/ NumPy\\n \\nLet’s get back to our inputs and weights \\n\\u200b\\n—\\n\\u200b\\n when covering them, we mentioned that we need to\\n \\nperform dot products on all of the vectors that consist of both input and weight matrices. As we\\n \\nhave just learned, that’s the operation that the matrix product performs. We just need to perform', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 49}),\n",
       " Document(page_content='transposition on its second argument, which is the weights matrix in our case, to turn the row\\n \\nvectors it currently consists of into column vectors.\\n \\nInitially, we were able to perform the dot product on the inputs and the weights without a\\n \\ntransposition because the weights were a matrix, but the inputs were just a vector. In this case, the\\n \\ndot product results in a vector of atomic dot products performed on each row from the matrix and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 49}),\n",
       " Document(page_content='this single vector. When inputs become a batch of inputs (a matrix), we need to perform the\\n \\nmatrix product. It takes all of the combinations of rows from the left matrix and columns from the\\n \\nright matrix, performing the dot product on them and placing the results in an output array. Both\\n \\narrays have the same shape, but, to perform the matrix product, the shape’s value from the index 1\\n \\nof the first matrix and the index 0 of the second matrix must match — they don’t right now.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 49}),\n",
       " Document(page_content='Fig 2.18:\\n\\u200b\\n Depiction of why we need to transpose to perform the matrix product.\\n \\nIf we transpose the second array, values of its shape swap their positions.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 49}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n36\\n \\n \\nFig 2.19:\\n\\u200b\\n After transposition, we can perform the matrix product.\\n \\n \\nAnim 2.18-2.19:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/crq\\n \\nIf we look at this from the perspective of the input and weights, we need to perform the dot\\n \\nproduct of each input and each weight set in all of their combinations. The dot product takes the\\n \\nrow from the first array and the column from the second one, but currently the data in both arrays', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 50}),\n",
       " Document(page_content='are row-aligned. Transposing the second array shapes the data to be column-aligned. The matrix\\n \\nproduct of inputs and transposed weights will result in a matrix containing all atomic dot products\\n \\nthat we need to calculate. The resulting matrix consists of outputs of all neurons after operations\\n \\nperformed on each input sample:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 50}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n37\\n \\n \\nFig 2.20:\\n\\u200b\\n Code and visuals depicting the dot product of inputs and transposed weights.\\n \\n \\nAnim 2.20:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/gjw\\n \\nWe mentioned that the second argument for \\n\\u200b\\nnp.dot()\\n\\u200b\\n is going to be our transposed weights, so\\n \\nfirst will be inputs, but previously weights were the first parameter. We changed that here.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 51}),\n",
       " Document(page_content='Before, we were modeling neuron output using a single sample of data, a vector, but now we are\\n \\na step forward when we model layer behavior on a batch of data. We could retain the current\\n \\nparameter order, but, as we’ll soon learn, it’s more useful to have a result consisting of a list of\\n \\nlayer outputs per each sample than a list of neurons and their outputs sample-wise. We want the\\n \\nresulting array to be sample-related and not neuron-related as we’ll pass those samples further', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 51}),\n",
       " Document(page_content='through the network, and the next layer will expect a batch of inputs.\\n \\nWe can code this solution using NumPy now. We can perform \\n\\u200b\\nnp.dot()\\n\\u200b\\n on a plain Python list of\\n \\nlists as NumPy will convert them to matrices internally. We are converting weights ourselves\\n \\nthough to perform transposition operation first, \\n\\u200b\\nT\\n\\u200b\\n in the code, as plain Python list of lists does not\\n \\nsupport it. Speaking of biases, we do not need to make it a NumPy array for the same reason \\n\\u200b\\n—', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 51}),\n",
       " Document(page_content='\\u200b\\n—\\n \\nNumPy is going to do that internally.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 51}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n \\n38\\n \\nBiases are a list, though, so they are a 1D array as a NumPy array. The addition of this bias vector\\n \\nto a matrix (of the dot products in this case) works similarly to the dot product of a matrix and\\n \\nvector that we described earlier; The bias vector will be added to each row vector of the matrix.\\n \\nSince each column of the matrix product result is an output of one neuron, and the vector is going', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 52}),\n",
       " Document(page_content='to be added to each row vector, the first bias is going to be added to each first element of those\\n \\nvectors, second to second, etc. That’s what we need \\n\\u200b\\n—\\n\\u200b\\n the bias of each neuron needs to be added\\n \\nto all of the results of this neuron performed on all input vectors (samples).\\n \\n \\nFig 2.21:\\n\\u200b\\n Code and visuals for inputs multiplied by the weights, plus the bias.\\n \\n \\nAnim 2.21:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/qty', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 52}),\n",
       " Document(page_content='Chapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\\n39 \\nNow we can implement what we have learned into code:  \\nimport \\u200bnumpy \\u200bas \\u200bnp  \\ninputs \\u200b= \\u200b[[\\u200b1.0\\u200b, \\u200b2.0\\u200b, \\u200b3.0\\u200b, \\u200b2.5\\u200b],  \\n [\\u200b2.0\\u200b, \\u200b5.0\\u200b, \\u200b-\\u200b1.0\\u200b, \\u200b2.0\\u200b],  \\n   [\\u200b-\\u200b1.5\\u200b, \\u200b2.7\\u200b, \\u200b3.3\\u200b, \\u200b-\\u200b0.8\\u200b]]  \\nweights \\u200b= \\u200b[[\\u200b0.2\\u200b, \\u200b0.8\\u200b, \\u200b-\\u200b0.5\\u200b, \\u200b1.0\\u200b],  \\n [\\u200b0.5\\u200b, \\u200b-\\u200b0.91\\u200b, \\u200b0.26\\u200b, \\u200b-\\u200b0.5\\u200b],  \\n   [\\u200b-\\u200b0.26\\u200b, \\u200b-\\u200b0.27\\u200b, \\u200b0.17\\u200b, \\u200b0.87\\u200b]]  \\nbiases \\u200b= \\u200b[\\u200b2.0\\u200b, \\u200b3.0\\u200b, \\u200b0.5\\u200b]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 53}),\n",
       " Document(page_content='layer_outputs \\u200b= \\u200bnp.dot(inputs, np.array(weights).T) \\u200b+ \\u200bbiases  \\nprint\\u200b(layer_outputs)  \\n>>> \\narray([[ \\u200b4.8  1.21  2.385\\u200b],  \\n [ \\u200b8.9   \\u200b-\\u200b1.81  0.2  \\u200b],  \\n [ \\u200b1.41  1.051  0.026\\u200b]])  \\nAs you can see, our neural network takes in a group of samples (inputs) and outputs a group of  \\npredictions. If you’ve used any of the deep learning libraries, this is why you pass in a list of  \\ninputs (even if it’s just one feature set) and are returned a list of predictions, even if there’s only', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 53}),\n",
       " Document(page_content='one prediction.  \\nSupplementary Material: \\u200bhttps://nnfs.io/ch2  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 53}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n6\\n \\n \\n \\n \\n \\nChapter 3\\n \\nAdding Layers\\n \\nThe neural network we’ve built is becoming more respectable, but at the moment, we have only\\n \\none layer. Neural networks become “deep” when they have 2 or more \\n\\u200b\\nhidden layers\\n\\u200b\\n. At the\\n \\nmoment, we have just one layer, which is effectively an output layer. Why we want two or more\\n \\nhidden\\n\\u200b\\n layers will become apparent in a later chapter. Currently, we have no hidden layers. A', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 54}),\n",
       " Document(page_content='hidden layer isn’t an input or output layer; as the scientist, you see data as they are handed to the\\n \\ninput layer and the resulting data from the output layer. Layers between these endpoints have\\n \\nvalues that we don’t necessarily deal with, hence the name “hidden.” Don’t let this name convince\\n \\nyou that you can’t access these values, though. You will often use them to diagnose issues or\\n \\nimprove your neural network. To explore this concept, let’s add another layer to this neural', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 54}),\n",
       " Document(page_content='network, and, for now, let’s assume these two layers that we’re going to have will be the hidden\\n \\nlayers, and we just have not coded our output layer yet.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 54}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n7\\n \\nBefore we add another layer, let’s think about what will be coming. In the case of the first layer,\\n \\nwe can see that we have an input with 4 features.\\n \\n \\nFig 3.01:\\n\\u200b\\n Input layer with 4 features into a hidden layer with 3 neurons.\\n \\nSamples (feature set data) get fed through the input, which does not change it in any way, to our\\n \\nfirst hidden layer, which we can see has 3 sets of weights, with 4 values each.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 55}),\n",
       " Document(page_content='Each of those 3 unique weight sets is associated with its distinct neuron. Thus, since we have 3\\n \\nweight sets, we have 3 neurons in this first hidden layer. Each neuron has a unique set of weights,\\n \\nof which we have 4 (as there are 4 inputs to this layer), which is why our initial weights have a\\n \\nshape of \\n\\u200b\\n(3,4)\\n\\u200b\\n.\\n \\nNow, we wish to add another layer. To do that, we must make sure that the expected input to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 55}),\n",
       " Document(page_content='that layer matches the previous layer’s output. We have set the number of neurons in a layer by\\n \\nsetting how many weight sets and biases we have. The previous layer’s influence on weight sets\\n \\nfor the current layer is that each weight set needs to have a separate weight per input. This\\n \\nmeans a distinct weight per neuron from the previous layer (or feature if we’re talking the\\n \\ninput). The previous layer has 3 weight sets and 3 biases, so we know it has 3 neurons. This then', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 55}),\n",
       " Document(page_content='means, for the next layer, we can have as many weight sets as we want (because this is how\\n \\nmany neurons this new layer will have), but each of those weight sets must have 3 discrete\\n \\nweights.\\n \\nTo create this new layer, we are going to copy and paste our \\n\\u200b\\nweights\\n\\u200b\\n \\n\\u200b\\nand \\n\\u200b\\nbiases\\n\\u200b\\n to \\n\\u200b\\nweights2\\n \\nand \\n\\u200b\\nbiases2\\n\\u200b\\n, and change their values to new made up sets. Here’s an example:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n],\\n \\n          [\\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n5.\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.\\n\\u200b\\n,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 55}),\n",
       " Document(page_content='\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n],\\n \\n          [\\n\\u200b\\n-\\n\\u200b\\n1.5\\n\\u200b\\n, \\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.8\\n\\u200b\\n]]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]\\n \\nbiases \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 55}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n8\\n \\nweights2 \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.14\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.12\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.33\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n-\\n\\u200b\\n0.44\\n\\u200b\\n, \\n\\u200b\\n0.73\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.13\\n\\u200b\\n]]\\n \\nbiases2 \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n]\\n \\n \\nNext, we will now call \\n\\u200b\\noutputs \\n\\u200b\\n“\\n\\u200b\\nlayer1_ouputs\\n\\u200b\\n”\\n\\u200b\\n:\\n \\nlayer1_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, np.array(weights).T) \\n\\u200b\\n+ \\n\\u200b\\nbiases', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 56}),\n",
       " Document(page_content='\\u200b\\n+ \\n\\u200b\\nbiases\\n \\n \\nAs previously stated, inputs to layers are either inputs from the actual dataset you’re training with\\n \\nor outputs from a previous layer. That’s why we defined 2 versions of \\n\\u200b\\nweights\\n\\u200b\\n and \\n\\u200b\\nbiases\\n\\u200b\\n but only\\n \\n1 of \\n\\u200b\\ninputs\\n\\u200b\\n — because the inputs for layer 2 will be the outputs from the previous layer:\\n \\nlayer2_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(layer1_outputs, np.array(weights2).T) \\n\\u200b\\n+ \\n\\u200b\\n\\\\\\n \\n                 biases2\\n \\n \\nAll together now:\\n \\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 56}),\n",
       " Document(page_content='\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n], [\\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n5.\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n], [\\n\\u200b\\n-\\n\\u200b\\n1.5\\n\\u200b\\n, \\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.8\\n\\u200b\\n]]\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n           [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]\\n \\nbiases \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n]\\n \\nweights2 \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.14\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n            [', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 56}),\n",
       " Document(page_content='],\\n \\n            [\\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.12\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.33\\n\\u200b\\n],\\n \\n            [\\n\\u200b\\n-\\n\\u200b\\n0.44\\n\\u200b\\n, \\n\\u200b\\n0.73\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.13\\n\\u200b\\n]]\\n \\nbiases2 \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n]\\n \\n \\nlayer1_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, np.array(weights).T) \\n\\u200b\\n+ \\n\\u200b\\nbiases\\n \\nlayer2_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(layer1_outputs, np.array(weights2).T) \\n\\u200b\\n+ \\n\\u200b\\nbiases2\\n \\n \\nprint\\n\\u200b\\n(layer2_outputs)\\n \\n \\n \\n>>>\\n \\narray([[ \\n\\u200b\\n0.5031  \\n\\u200b\\n-\\n\\u200b\\n1.04185 \\n\\u200b\\n-\\n\\u200b\\n2.03875\\n\\u200b\\n],\\n \\n       [ \\n\\u200b\\n0.2434  \\n\\u200b\\n-\\n\\u200b\\n2.7332  \\n\\u200b\\n-\\n\\u200b\\n5.7633 \\n\\u200b\\n],', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 56}),\n",
       " Document(page_content='-\\n\\u200b\\n5.7633 \\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n-\\n\\u200b\\n0.99314  1.41254 \\n\\u200b\\n-\\n\\u200b\\n0.35655\\n\\u200b\\n]])', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 56}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n9\\n \\nAt this point, our neural network could be visually represented as:\\n \\n \\nFig 3.02:\\n\\u200b\\n 4 features input into 2 hidden layers of 3 neurons each.\\n \\n \\nTraining Data\\n \\nNext, rather than hand-typing in random data, we’ll use a function that can create non-linear data.\\n \\nWhat do we mean by non-linear? Linear data can be fit with or represented by a straight line.\\n \\n \\nFig 3.03:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 57}),\n",
       " Document(page_content='Fig 3.03:\\n\\u200b\\n Example of data (orange dots) that can be represented (fit) by a straight line (green\\n \\nline).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 57}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n10\\n \\nNon-linear data cannot be represented well by a straight line.\\n \\n \\nFig 3.04:\\n\\u200b\\n Example of data (orange dots) that is not well fit by a straight line.\\n \\nIf you were to graph data points of the form \\n\\u200b\\n(x, y)\\n\\u200b\\n where \\n\\u200b\\ny = f(x)\\n\\u200b\\n, and it looks to be a line with a\\n \\nclear trend or slope, then chances are, they’re linear data! Linear data are very easily', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 58}),\n",
       " Document(page_content='approximated by far simpler machine learning models than neural networks. What other machine\\n \\nlearning algorithms cannot approximate so easily are non-linear datasets. To simplify this, we’ve\\n \\ncreated a Python package that you can install with pip, called \\n\\u200b\\nnnfs\\n\\u200b\\n:\\n \\npip install nnfs\\n \\nThe nnfs package contains functions that we can use to create data. For example:\\n \\nfrom \\n\\u200b\\nnnfs.datasets \\n\\u200b\\nimport \\n\\u200b\\nspiral_data\\n \\n \\nThe \\n\\u200b\\nspiral_data\\n\\u200b\\n function was slightly modified from', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 58}),\n",
       " Document(page_content='https://cs231n.github.io/neural-networks-case-study/\\n\\u200b\\n, which is a great supplementary resource for\\n \\nthis topic.\\n \\nYou will typically not be generating training data from a function for your neural networks. You\\n \\nwill have an actual dataset. Generating a dataset this way is purely for convenience at this stage.\\n \\nWe will also use this package to ensure repeatability for everyone, using \\n\\u200b\\nnnfs.init()\\n\\u200b\\n, after\\n \\nimporting NumPy:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nimport \\n\\u200b\\nnnfs', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 58}),\n",
       " Document(page_content='import \\n\\u200b\\nnnfs\\n \\n \\nnnfs.init()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 58}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n11\\n \\nThe \\n\\u200b\\nnnfs.init()\\n\\u200b\\n does three things: it sets the random seed to 0 (by the default), creates a\\n \\nfloat32\\n\\u200b\\n dtype default, and overrides the original dot product from NumPy. All of these are meant\\n \\nto ensure repeatable results for following along.\\n \\nThe \\n\\u200b\\nspiral_data\\n\\u200b\\n function allows us to create a dataset with as many classes as we want. The', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 59}),\n",
       " Document(page_content='function has parameters to choose the number of classes and the number of points/observations\\n \\nper class in the resulting non-linear dataset. For example:\\n \\nimport \\n\\u200b\\nmatplotlib.pyplot \\n\\u200b\\nas \\n\\u200b\\nplt\\n \\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\nplt.scatter(X[:,\\n\\u200b\\n0\\n\\u200b\\n], X[:,\\n\\u200b\\n1\\n\\u200b\\n])\\n \\nplt.show()\\n \\n \\n \\nFig 3.05:\\n\\u200b\\n Uncolored spiral dataset.\\n \\nIf you trace from the center, you can determine all 3 classes separately, but this is a very', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 59}),\n",
       " Document(page_content='challenging problem for a machine learning classifier to solve. Adding color to the chart makes\\n \\nthis more clear:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 59}),\n",
       " Document(page_content=\"Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n12\\n \\nplt.scatter(X[:, \\n\\u200b\\n0\\n\\u200b\\n], X[:, \\n\\u200b\\n1\\n\\u200b\\n], \\n\\u200b\\nc\\n\\u200b\\n=\\n\\u200b\\ny, \\n\\u200b\\ncmap\\n\\u200b\\n=\\n\\u200b\\n'brg'\\n\\u200b\\n)\\n \\nplt.show()\\n \\n \\nFig 3.06:\\n\\u200b\\n Spiral dataset colored by class.\\n \\nKeep in mind that the neural network will not be aware of the color differences as the data have\\n \\nno class encodings. This is only made as an instruction for the reader. In the data above, each dot\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 60}),\n",
       " Document(page_content='is the feature, and its coordinates are the samples that form the dataset. The “classification” for\\n \\nthat dot has to do with which spiral it is a part of, depicted by blue, green, or red color in the\\n \\nprevious image. These colors would then be assigned a class number for the model to fit to, like 0,\\n \\n1, and 2.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 60}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n13\\n \\n \\nDense Layer Class\\n \\nNow that we no longer need to hand-type our data, we should create something similar for our\\n \\nvarious types of neural network layers. So far, we’ve only used what’s called a \\n\\u200b\\ndense\\n\\u200b\\n or\\n \\nfully-connected\\n\\u200b\\n layer. These layers are commonly referred to as “dense” layers in papers,\\n \\nliterature, and code, but you will occasionally see them called fully-connected or “fc” for short in', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 61}),\n",
       " Document(page_content='code. Our dense layer class will begin with two methods.\\n \\nclass \\n\\u200b\\nLayer_Dense\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nn_inputs\\n\\u200b\\n, \\n\\u200b\\nn_neurons\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Initialize weights and biases\\n \\n        \\n\\u200b\\npass  \\n\\u200b\\n# using pass statement as a placeholder\\n \\n \\n    # Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Calculate output values from inputs, weights and biases\\n \\n        \\n\\u200b\\npass  \\n\\u200b\\n# using pass statement as a placeholder', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 61}),\n",
       " Document(page_content='As previously stated, weights are often initialized randomly for a model, but not always. If you\\n \\nwish to load a pre-trained model, you will initialize the parameters to whatever that pretrained\\n \\nmodel finished with. It’s also possible that, even for a new model, you have some other\\n \\ninitialization rules besides random. For now, we’ll stick with random initialization. Next, we have\\n \\nthe \\n\\u200b\\nforward \\n\\u200b\\nmethod. When we pass data through a model from beginning to end, this is called a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 61}),\n",
       " Document(page_content='forward pass\\n\\u200b\\n. Just like everything else, however, this is not the only way to do things. You can\\n \\nhave the data loop back around and do other interesting things. We’ll keep it usual and perform a\\n \\nregular forward pass.\\n \\nTo continue the \\n\\u200b\\nLayer_Dense\\n\\u200b\\n class’ code let’s add the random initialization of weights and\\n \\nbiases:\\n \\n# Layer initialization\\n \\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nn_inputs\\n\\u200b\\n, \\n\\u200b\\nn_neurons\\n\\u200b\\n):\\n \\n    self.weights \\n\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 61}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(n_inputs, n_neurons)\\n \\n    self.biases \\n\\u200b\\n= \\n\\u200b\\nnp.zeros((\\n\\u200b\\n1\\n\\u200b\\n, n_neurons))', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 61}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n14\\n \\nHere, we’re setting weights to be random and biases to be 0. Note that we’re initializing weights\\n \\nto be \\n\\u200b\\n(inputs, neurons), \\n\\u200b\\nrather than (\\n\\u200b\\nneurons, inputs)\\n\\u200b\\n. We’re doing this ahead instead of\\n \\ntransposing every time we perform a forward pass, as explained in the previous chapter. Why zero\\n \\nbiases? In specific scenarios, like with many samples containing values of 0, a bias can ensure', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 62}),\n",
       " Document(page_content='that a neuron fires initially. It sometimes may be appropriate to initialize the biases to some\\n \\nnon-zero number, but the most common initialization for biases is 0. However, in these scenarios,\\n \\nyou may find success in doing things another way. This will vary depending on your use-case and\\n \\nis just one of many things you can tweak when trying to improve results. One situation where you\\n \\nmight want to try something else is with what’s called \\n\\u200b\\ndead neurons\\n\\u200b\\n. We haven’t yet covered', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 62}),\n",
       " Document(page_content='activation functions in practice, but imagine our step function again.\\n \\n \\nFig 3.07:\\n\\u200b\\n Graph of a step function.\\n \\nIt’s possible for \\n\\u200b\\nweights · inputs + biases \\n\\u200b\\nnot to meet the threshold of the step function, which\\n \\nmeans the neuron will output a 0. Alone, this is not a big issue, but it becomes a problem if this\\n \\nhappens to this neuron for every one of the input samples (it’ll become clear why once we cover', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 62}),\n",
       " Document(page_content='backpropagation). So then this neuron’s 0 output is the input to another neuron. Any weight\\n \\nmultiplied by zero will be zero. With an increasing number of neurons outputting 0, more inputs\\n \\nto the next neurons will receive these 0s rendering the network essentially non-trainable, or\\n \\n“dead.”\\n \\nNext, let’s explore \\n\\u200b\\nnp.random.randn\\n\\u200b\\n \\n\\u200b\\nand \\n\\u200b\\nnp.zeros\\n\\u200b\\n \\n\\u200b\\nin more detail. These methods are\\n \\nconvenient ways to initialize arrays. \\n\\u200b\\nnp.random.randn\\n\\u200b\\n \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 62}),\n",
       " Document(page_content='\\u200b\\n \\n\\u200b\\nproduces a Gaussian distribution with a\\n \\nmean of 0 and a variance of 1, which means that it’ll generate random numbers, positive and\\n \\nnegative, centered at 0 and with the mean value close to 0. In general, neural networks work best\\n \\nwith values between -1 and +1, which we’ll discuss in an upcoming chapter. So this\\n \\nnp.random.randn\\n\\u200b\\n \\n\\u200b\\ngenerates values around those numbers. We’re going to multiply this\\n \\nGaussian distribution for the weights by \\n\\u200b\\n0.01\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 62}),\n",
       " Document(page_content='\\u200b\\n0.01\\n\\u200b\\n to generate numbers that are a couple of\\n \\nmagnitudes smaller.  Otherwise, the model will take more time to fit the data during the training\\n \\nprocess as starting values will be disproportionately large compared to the updates being made', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 62}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n15\\n \\nduring training. The idea here is to start a model with non-zero values small enough that they\\n \\nwon’t affect training. This way, we have a bunch of values to begin working with, but hopefully\\n \\nnone too large or as zeros. You can experiment with values other than \\n\\u200b\\n0.01\\n\\u200b\\n if you like.\\n \\nFinally, the \\n\\u200b\\nnp.random.randn\\n\\u200b\\n function takes dimension sizes as parameters and creates the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 63}),\n",
       " Document(page_content='output array with this shape. The weights here will be the number of inputs for the first dimension\\n \\nand the number of neurons for the 2nd dimension. This is similar to our previous made up array of\\n \\nweights, just randomly generated. Whenever there’s a function or block of code that you’re not\\n \\nsure about, you can always print it out. For example:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nimport \\n\\u200b\\nnnfs\\n \\n \\nnnfs.init()\\n \\n \\nprint\\n\\u200b\\n(np.random.randn(\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\n[[ \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 63}),\n",
       " Document(page_content='>>>\\n \\n[[ \\n\\u200b\\n1.7640524   0.4001572   0.978738    2.2408931   1.867558  \\n\\u200b\\n]\\n \\n [\\n\\u200b\\n-\\n\\u200b\\n0.9772779   0.95008844 \\n\\u200b\\n-\\n\\u200b\\n0.1513572  \\n\\u200b\\n-\\n\\u200b\\n0.10321885  0.41059852\\n\\u200b\\n]]\\n \\n \\nThe example function call has returned a 2x5 array (which we can also say is “\\n\\u200b\\nwith a shape of\\n \\n(2,5)\\n\\u200b\\n”) with data randomly sampled from a Gaussian distribution with a mean of 0.\\n \\nNext, the \\n\\u200b\\nnp.zeros\\n\\u200b\\n function takes a desired array shape as an argument and returns an array of\\n \\nthat shape filled with zeros.\\n \\nprint', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 63}),\n",
       " Document(page_content='print\\n\\u200b\\n(np.zeros((\\n\\u200b\\n2\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n)))\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n0. 0. 0. 0. 0.\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0. 0. 0. 0. 0.\\n\\u200b\\n]]\\n \\n \\nWe’ll initialize the biases with the shape of \\n\\u200b\\n(1, n_neurons)\\n\\u200b\\n, as a row vector, which will let us\\n \\neasily add it to the result of the dot product later, without additional operations like transposition.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 63}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n \\n16\\n \\nTo see an example of how our method initializes weights and biases:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nimport \\n\\u200b\\nnnfs\\n \\n \\nnnfs.init()\\n \\n \\nn_inputs \\n\\u200b\\n= \\n\\u200b\\n2\\n \\nn_neurons \\n\\u200b\\n= \\n\\u200b\\n4\\n \\n \\nweights \\n\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(n_inputs, n_neurons)\\n \\nbiases \\n\\u200b\\n= \\n\\u200b\\nnp.zeros((\\n\\u200b\\n1\\n\\u200b\\n, n_neurons))\\n \\n \\nprint\\n\\u200b\\n(weights)\\n \\nprint\\n\\u200b\\n(biases)\\n \\n \\n \\n>>>\\n \\n[[ \\n\\u200b\\n0.01764052  0.00400157  0.00978738  0.02240893\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0.01867558 \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 64}),\n",
       " Document(page_content='[ \\n\\u200b\\n0.01867558 \\n\\u200b\\n-\\n\\u200b\\n0.00977278  0.00950088 \\n\\u200b\\n-\\n\\u200b\\n0.00151357\\n\\u200b\\n]]\\n \\n[[\\n\\u200b\\n0. 0. 0. 0.\\n\\u200b\\n]]\\n \\n \\nOn to our forward method — we need to update it with the dot product+biases calculation:\\n \\ndef\\n\\u200b\\n \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n    self.output \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, self.weights) \\n\\u200b\\n+ \\n\\u200b\\nself.biases\\n \\n \\nNothing new here, just turning the previous code into a method. Our full \\n\\u200b\\nLayer_Dense\\n\\u200b\\n class so\\n \\nfar:\\n \\nclass \\n\\u200b\\nLayer_Dense\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\ndef __init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 64}),\n",
       " Document(page_content='\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nn_inputs\\n\\u200b\\n, \\n\\u200b\\nn_neurons\\n\\u200b\\n):\\n \\n        self.weights \\n\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(n_inputs, n_neurons)\\n \\n        self.biases \\n\\u200b\\n= \\n\\u200b\\nnp.zeros((\\n\\u200b\\n1\\n\\u200b\\n, n_neurons))\\n \\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, self.weights) \\n\\u200b\\n+ \\n\\u200b\\nself.biases', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 64}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n17 \\nWe’re ready to make use of this new class instead of hardcoded calculations, so let’s generate  \\nsome data using the discussed dataset creation method and use our new layer to perform a  \\nforward pass:  \\n# Create dataset\\nX, y \\u200b= \\u200bspiral_data(\\u200bsamples \\u200b=\\u200b100\\u200b, \\u200bclasses \\u200b=\\u200b3\\u200b)\\n# Create Dense layer with 2 input features and 3 output values  \\ndense1 \\u200b= \\u200bLayer_Dense(\\u200b2\\u200b, \\u200b3\\u200b)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 65}),\n",
       " Document(page_content=\"# Perform a forward pass of our training data through this layer  \\ndense1.forward(X)\\n# Let's see output of the first few samples:         \\nprint\\u200b(dense1.output[:\\u200b5\\u200b]) \\u200bGo ahead and run     \\neverything.  \\nFull code up to this point:  \\nimport \\u200bnumpy \\u200bas \\u200bnp  \\nimport \\u200bnnfs  \\nfrom \\u200bnnfs.datasets \\u200bimport \\u200bspiral_data  \\nnnfs.init()  \\n# Dense layer  \\nclass \\u200bLayer_Dense\\u200b:  \\n \\u200b# Layer initialization  \\n \\u200bdef \\u200b__init__\\u200b(\\u200bself \\u200b, \\u200bn_inputs \\u200b, \\u200bn_neurons \\u200b): \\n \\u200b# Initialize weights and biases\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 65}),\n",
       " Document(page_content='\\u200bself.weights \\u200b= \\u200b0.01 \\u200b* \\u200bnp.random.randn(n_inputs, n_neurons)  \\n self.biases \\u200b= \\u200bnp.zeros((\\u200b1\\u200b, n_neurons))', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 65}),\n",
       " Document(page_content='Chapter 3 - Adding Layers - Neural Networks from Scratch in Python\\n18 \\n \\u200b# Forward pass  \\n \\u200bdef \\u200bforward\\u200b(\\u200bself \\u200b, \\u200binputs \\u200b): \\n \\u200b# Calculate output values from inputs, weights and biases  \\n \\u200bself.output \\u200b= \\u200bnp.dot(inputs, self.weights) \\u200b+ \\u200bself.biases  \\n# Create dataset  \\nX, y \\u200b= \\u200bspiral_data(\\u200bsamples \\u200b=\\u200b100\\u200b, \\u200bclasses \\u200b=\\u200b3\\u200b) \\n# Create Dense layer with 2 input features and 3 output values  \\ndense1 \\u200b= \\u200bLayer_Dense(\\u200b2\\u200b, \\u200b3\\u200b)  \\n# Perform a forward pass of our training data through this layer', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 66}),\n",
       " Document(page_content=\"dense1.forward(X)  \\n# Let's see output of the first few samples:  \\nprint\\u200b(dense1.output[:\\u200b5\\u200b])  \\n>>> \\n[[ \\u200b0.0000000e+00  0.0000000e+00  0.0000000e+00\\u200b]  \\n [\\u200b-\\u200b1.0475188e-04  1.1395361e-04 \\u200b-\\u200b4.7983500e-05\\u200b]  \\n [\\u200b-\\u200b2.7414842e-04  3.1729150e-04 \\u200b-\\u200b8.6921798e-05\\u200b]  \\n [\\u200b-\\u200b4.2188365e-04  5.2666257e-04 \\u200b-\\u200b5.5912682e-05\\u200b]  \\n [\\u200b-\\u200b5.7707680e-04  7.1401405e-04 \\u200b-\\u200b8.9430439e-05\\u200b]]  \\nIn the output, you can see we have 5 rows of data that have 3 values each. Each of those 3 values\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 66}),\n",
       " Document(page_content='is the value from the 3 neurons in the \\u200bdense1 \\u200b layer after passing in each of the samples. Great! We  \\nhave a network of neurons, so our neural network model is almost deserving of its name, but  \\nwe’re still missing the activation functions, so let’s do those next!  \\nSupplementary Material: \\u200bhttps://nnfs.io/ch3  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 66}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n6 \\nChapter 4\\nActivation Functions  \\nIn this chapter, we will tackle a few of the activation functions and discuss their roles. We use  \\ndifferent activation functions for different cases, and understanding how they work can help you  \\nproperly pick which of them is best for your task. The activation function is applied to the output', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 67}),\n",
       " Document(page_content='of a neuron (or layer of neurons), which modifies outputs. We use activation functions because if  \\nthe activation function itself is nonlinear, it allows for neural networks with usually two or more  \\nhidden layers to map nonlinear functions. We’ll be showing how this works in this chapter.   \\nIn general, your neural network will have two types of activation functions. The first will be the  \\nactivation function used in hidden layers, and the second will be used in the output layer. Usually,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 67}),\n",
       " Document(page_content='the activation function used for hidden neurons will be the same for all of them, but it doesn’t  \\nhave to.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 67}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n7\\n \\n \\nThe Step Activation Function\\n \\nRecall the purpose this activation function serves is to mimic a neuron “firing” or “not firing”\\n \\nbased on input information. The simplest version of this is a step function. In a single neuron, if\\n \\nthe \\n\\u200b\\nweights · inputs + bias\\n\\u200b\\n results in a value greater than 0, the neuron will fire and output a 1;\\n \\notherwise, it will output a 0.\\n \\n \\nFig 4.01:\\n\\u200b\\n Step function graph.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 68}),\n",
       " Document(page_content='This activation function has been used historically in hidden layers, but nowadays, it is rarely a\\n \\nchoice.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 68}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n8\\n \\n \\nThe Linear Activation Function\\n \\nA linear function is simply the equation of a line. It will appear as a straight line when graphed,\\n \\nwhere y=x and the output value equals the input.\\n \\n \\n \\nFig 4.02:\\n\\u200b\\n Linear function graph.\\n \\nThis activation function is usually applied to the last layer’s output in the case of a regression', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 69}),\n",
       " Document(page_content='model — a model that outputs a scalar value instead of a classification. We’ll cover regression in\\n \\nchapter 17 and soon in an example in this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 69}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n9\\n \\n \\nThe Sigmoid Activation Function\\n \\nThe problem with the step function is it’s not very informative. When we get to training and\\n \\nnetwork optimizers, you will see that the way an optimizer works is by assessing individual\\n \\nimpacts that weights and biases have on a network’s output. The problem with a step function is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 70}),\n",
       " Document(page_content='that it’s less clear to the optimizer what these impacts are because there’s very little information\\n \\ngathered from this function. It’s either on (1) or off (0). It’s hard to tell how “close” this step\\n \\nfunction was to activating or deactivating. Maybe it was very close, or maybe it was very far. In\\n \\nterms of the final output value from the network, it doesn’t matter if it was \\n\\u200b\\nclose\\n\\u200b\\n to outputting', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 70}),\n",
       " Document(page_content='\\u200b\\n to outputting\\n \\nsomething else. Thus, when it comes time to optimize weights and biases, it’s easier for the\\n \\noptimizer if we have activation functions that are more granular and informative.\\n \\nThe original, more granular, activation function used for neural networks was the \\n\\u200b\\nSigmoid\\n \\nactivation function, which looks like:\\n \\n \\nFig 4.03:\\n\\u200b\\n Sigmoid function graph.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 70}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n10\\n \\nThis function returns a value in the range of 0 for negative infinity, through 0.5 for the input of 0,\\n \\nand to 1 for positive infinity. We’ll talk about this function more in chapter 16.\\n \\nAs mentioned earlier, with “dead neurons,” it’s usually better to have a more granular approach\\n \\nfor the hidden neuron activation functions. In this case, we’re getting a value that can be', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 71}),\n",
       " Document(page_content='reversed to its original value; the returned value contains all the information from the input,\\n \\ncontrary to a function like the step function, where an input of 3 will output the same value as an\\n \\ninput of 300,000. The output from the Sigmoid function, being in the range of 0 to 1, also works\\n \\nbetter with neural networks — especially compared to the range of the negative to the positive\\n \\ninfinity — and adds nonlinearity. The importance of nonlinearity will become more clear shortly', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 71}),\n",
       " Document(page_content='in this chapter. The Sigmoid function, historically used in hidden layers, was eventually replaced\\n \\nby the \\n\\u200b\\nRectified Linear Units\\n\\u200b\\n activation function (or \\n\\u200b\\nReLU\\n\\u200b\\n). That said, we will be using the\\n \\nSigmoid function as the output layer’s activation function in chapter 16.\\n \\nThe Rectified Linear Activation Function\\n \\n \\nFig 4.04:\\n\\u200b\\n Graph of the ReLU activation function.\\n \\nThe rectified linear activation function is simpler than the sigmoid. It’s quite literally \\n\\u200b\\ny=x\\n\\u200b\\n, clipped', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 71}),\n",
       " Document(page_content='\\u200b\\ny=x\\n\\u200b\\n, clipped\\n \\nat 0 from the negative side. If \\n\\u200b\\nx\\n\\u200b\\n is less than or equal to \\n\\u200b\\n0\\n\\u200b\\n, then \\n\\u200b\\ny\\n\\u200b\\n is \\n\\u200b\\n0\\n\\u200b\\n — otherwise, \\n\\u200b\\ny\\n\\u200b\\n is equal to \\n\\u200b\\nx\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 71}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n11\\n \\n \\nThis simple yet powerful activation function is the most widely used activation function at the\\n \\ntime of writing for various reasons — mainly speed and efficiency. While the sigmoid activation\\n \\nfunction isn’t the most complicated, it’s still much more challenging to compute than the ReLU\\n \\nactivation function. The ReLU activation function is extremely close to being a linear activation', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 72}),\n",
       " Document(page_content='function while remaining nonlinear, due to that bend after 0. This simple property is, however,\\n \\nvery effective.\\n \\nWhy Use Activation Functions?\\n \\nNow that we understand what activation functions represent, how some of them look, and what\\n \\nthey return, let’s discuss \\n\\u200b\\nwhy\\n\\u200b\\n we use activation functions in the first place. In most cases, for a\\n \\nneural network to fit a nonlinear function, we need it to contain two or more hidden layers, and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 72}),\n",
       " Document(page_content='we need those hidden layers to use a nonlinear activation function.\\n \\nFirst off, what’s a nonlinear function? A nonlinear function cannot be represented well by a\\n \\nstraight line, such as a sine function:\\n \\n \\nFig 4.05:\\n\\u200b\\n Graph of y=sin(x)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 72}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n12\\n \\nWhile there are certainly problems in life that are linear in nature, for example, trying to figure\\n \\nout the cost of some number of shirts, and we know the cost of an individual shirt, and that there\\n \\nare no bulk discounts, then the equation to calculate the price of any number of those products is a\\n \\nlinear equation. Other problems in life are not so simple, like the price of a home. The number of', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 73}),\n",
       " Document(page_content='factors that come into play, such as size, location, time of year attempting to sell, number of\\n \\nrooms, yard, neighborhood, and so on, makes the pricing of a home a nonlinear equation. Many of\\n \\nthe more interesting and hard problems of our time are nonlinear. The main attraction for neural\\n \\nnetworks has to do with their ability to solve nonlinear problems. First, let’s consider a situation\\n \\nwhere neurons have no activation function, which would be the same as having an activation', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 73}),\n",
       " Document(page_content='function of \\n\\u200b\\ny=x\\n\\u200b\\n. With this linear activation function in a neural network with 2 hidden layers of 8\\n \\nneurons each, the result of training this model will look like:\\n \\n \\nFig 4.06:\\n\\u200b\\n Neural network with linear activation functions in hidden layers attempting to fit\\n \\n \\ny=sin(x)\\n \\nWhen using the same 2 hidden layers of 8 neurons each with the rectified linear activation\\n \\nfunction, we see the following result after training:\\n \\n \\nFig 4.07:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 73}),\n",
       " Document(page_content='Fig 4.07:\\n\\u200b\\n ReLU activation functions in hidden layers attempting to fit y=sin(x)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 73}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n13\\n \\n \\nLinear Activation in the Hidden Layers\\n \\nNow that you can see that this is the case, we still should consider \\n\\u200b\\nwhy\\n\\u200b\\n this is the case. To begin,\\n \\nlet’s revisit the linear activation function of \\n\\u200b\\ny=x\\n\\u200b\\n, and let’s consider this on a singular neuron level.\\n \\nGiven values for weights and biases, what will the output be for a neuron with a \\n\\u200b\\ny=x \\n\\u200b\\nactivation', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 74}),\n",
       " Document(page_content='y=x \\n\\u200b\\nactivation\\n \\nfunction? Let’s look at some examples — first, let’s try to update the first weight with a positive\\n \\nvalue:\\n \\n \\nFig 4.08:\\n\\u200b\\n Example of output with a neuron using a linear activation function.\\n \\nAs we continue to tweak with weights, updating with a negative number this time:\\n \\n \\nFig 4.09:\\n\\u200b\\n Example of output with a neuron using a linear activation function, updated weight.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 74}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n14\\n \\nAnd updating weights and additionally a bias:\\n \\n \\nFig 4.10:\\n\\u200b\\n Example of output with a neuron using a linear activation function, updated another\\n \\n \\nweight.\\n \\nNo matter what we do with this neuron’s weights and biases, the output of this neuron will be\\n \\nperfectly linear to \\n\\u200b\\ny=x\\n\\u200b\\n of the activation function. This linear nature will continue throughout the\\n \\nentire network:\\n \\n \\nFig 4.11:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 75}),\n",
       " Document(page_content='Fig 4.11:\\n\\u200b\\n A neural network with all linear activation functions.\\n \\nNo matter what we do, however many layers we have, this network can only depict linear\\n \\nrelationships if we use linear activation functions. It should be fairly obvious that this will be the\\n \\ncase as each neuron in each layer acts linearly, so the entire network is a linear function as well.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 75}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n15\\n \\n \\nReLU Activation in a Pair of Neurons\\n \\nWe believe it is less obvious how, with a barely nonlinear activation function, like the rectified\\n \\nlinear activation function, we can suddenly map nonlinear relationships and functions, so now\\n \\nlet’s cover that. Let’s start again with a single neuron. We’ll begin with both a weight of 0 and a\\n \\nbias of 0:\\n \\n \\nFig 4.12:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 76}),\n",
       " Document(page_content='Fig 4.12:\\n\\u200b\\n Single neuron with single input (zeroed weight) and ReLU activation function.\\n \\nIn this case, no matter what input we pass, the output of this neuron will always be a 0, because\\n \\nthe weight is 0, and there’s no bias. Let’s set the weight to be 1:\\n \\n \\nFig 4.13:\\n\\u200b\\n Single neuron with single input and ReLU activation function, weight set to 1.0.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 76}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n16\\n \\nNow it looks just like the basic rectified linear function, no surprises yet! Now let’s set the bias to\\n \\n \\n0.50:\\n \\n \\nFig 4.14:\\n\\u200b\\n Single neuron with single input and ReLU activation function, bias applied.\\n \\nWe can see that, in this case, with a single neuron, the bias offsets the overall function’s\\n \\nactivation point \\n\\u200b\\nhorizontally\\n\\u200b\\n. By increasing bias, we’re making this neuron activate earlier. What', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 77}),\n",
       " Document(page_content='happens when we negate the weight to -1.0?\\n \\n \\nFig 4.15:\\n\\u200b\\n Single neuron with single input and ReLU activation function, negative weight.\\n \\nWith a negative weight and this single neuron, the function has become a question of when this\\n \\nneuron \\n\\u200b\\ndeactivates\\n\\u200b\\n. Up to this point, you’ve seen how we can use the bias to offset the function\\n \\nhorizontally, and the weight to influence the slope of the activation. Moreover, we’re also able to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 77}),\n",
       " Document(page_content='control whether the function is one for determining where the neuron activates or deactivates.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 77}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n17\\n \\nWhat happens when we have, rather than just the one neuron, a pair of neurons? For example,\\n \\nlet’s pretend that we have 2 hidden layers of 1 neuron each. Thinking back to the \\n\\u200b\\ny=x\\n\\u200b\\n activation\\n \\nfunction, we unsurprisingly discovered that a linear activation function produced linear results no\\n \\nmatter what chain of neurons we made. Let’s see what happens with the rectified linear function', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 78}),\n",
       " Document(page_content='for the activation. We’ll begin with the last values for the 1st neuron and a weight of 1, with a\\n \\nbias of 0, for the 2nd neuron:\\n \\n \\nFig 4.16:\\n\\u200b\\n Pair of neurons with single inputs and ReLU activation functions.\\n \\nAs we can see so far, there’s no change. This is because the 2nd neuron’s bias is doing no\\n \\noffsetting, and the 2nd neuron’s weight is just multiplying output by 1, so there’s no change. Let’s\\n \\ntry to adjust the 2nd neuron’s bias now:\\n \\n \\nFig 4.17:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 78}),\n",
       " Document(page_content='Fig 4.17:\\n\\u200b\\n Pair of neurons with single inputs and ReLU activation functions, other bias applied.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 78}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n18\\n \\nNow we see some fairly interesting behavior. The bias of the second neuron indeed shifted the\\n \\noverall function, but, rather than shifting it \\n\\u200b\\nhorizontally\\n\\u200b\\n, it shifted the function \\n\\u200b\\nvertically\\n\\u200b\\n. What\\n \\nthen might happen if we make that 2nd neuron’s weight -2 rather than 1?\\n \\n \\nFig 4.18:\\n\\u200b\\n Pair of neurons with single inputs and ReLU activation functions, other negative\\n \\nweight.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 79}),\n",
       " Document(page_content='weight.\\n \\nSomething exciting has occurred! What we have here is a neuron that has both an activation and a\\n \\ndeactivation point. When \\n\\u200b\\nboth\\n\\u200b\\n neurons are activated, when their “area of effect” comes into play,\\n \\nthey produce values in the range of the granular, variable, and output. If any neuron in the pair is\\n \\ninactive, the pair will produce non-variable output:\\n \\n \\nFig 4.19:\\n\\u200b\\n Pair of neurons with single inputs and ReLU activation functions, area of effect.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 79}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n19\\n \\n \\nReLU Activation in the Hidden Layers\\n \\nLet’s now take this concept and use it to fit to the sine wave function using 2 hidden layers of 8\\n \\nneurons each, and we can hand-tune the values to fit the curve. We’ll do this by working with 1\\n \\npair of neurons at a time, which means 1 neuron from each layer individually. For simplicity, we', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 80}),\n",
       " Document(page_content='are also going to assume that the layers are not densely connected, and each neuron from the first\\n \\nhidden layer connects to only one neuron from the second hidden layer. That’s usually not the\\n \\ncase with the real models, but we want this simplification for the purpose of this demo.\\n \\nAdditionally, this example model takes a single value as an input, the input to the sine function,\\n \\nand outputs a single value like the sine function. The output layer uses the Linear activation', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 80}),\n",
       " Document(page_content='function, and the hidden layers will use the rectified linear activation function.\\n \\nTo start, we’ll set all weights to 0 and work with the first pair of neurons:\\n \\n \\nFig 4.20:\\n\\u200b\\n Hand-tuning a neural network starting with the first pair of neurons.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 80}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n20\\n \\nNext, we can set the weight for the hidden layer neurons and the output neuron to 1, and we can\\n \\nsee how this impacts the output:\\n \\n \\nFig 4.21:\\n\\u200b\\n Adjusting weights for the first/top pair of neurons all to 1.\\n \\nIn this case, we can see that the slope of the overall function is impacted. We can further increase\\n \\nthis slope by adjusting the weight for the first neuron of the first layer to 6.0:\\n \\n \\nFig 4.22:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 81}),\n",
       " Document(page_content='Fig 4.22:\\n\\u200b\\n Setting weight for first hidden neuron to 6.\\n \\nWe can now see, for example, that the initial slope of this function is what we’d like, but we have\\n \\na problem. Currently, this function never ends because this neuron pair never \\n\\u200b\\ndeactivates\\n\\u200b\\n. We can\\n \\nvisually see where we’d like the deactivation to occur. It’s where the red fitment line (our current\\n \\nneural network’s output) diverges initially from the green sine wave. So now, while we have the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 81}),\n",
       " Document(page_content='correct slope, we need to set this spot as our deactivation point. To do that, we start by increasing', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 81}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n21\\n \\nthe bias for the 2nd neuron of the hidden layer pair to 0.70. Recall that this offsets the overall\\n \\nfunction \\n\\u200b\\nvertically\\n\\u200b\\n:\\n \\n \\nFig 4.23:\\n\\u200b\\n Using the bias for the 2nd hidden neuron in the top pair to offset function vertically.\\n \\nNow we can set the weight for the 2nd neuron to -1, causing a deactivation point to occur, at least\\n \\nhorizontally, where we want it:\\n \\n \\nFig 4.24:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 82}),\n",
       " Document(page_content='Fig 4.24:\\n\\u200b\\n Setting the weight for the 2nd neuron in the top pair to -1.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 82}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n22\\n \\nNow we’d like to flip this slope back. How might we flip the output of these two neurons? It\\n \\nseems like we can take the weight of the connection to the output neuron, which is currently a 1.0,\\n \\nand just flip it to a -1, and that flips the function:\\n \\n \\nFig 4.25:\\n\\u200b\\n Setting the weight to the output neuron to -1.\\n \\nWe’re certainly getting closer to making this first section fit how we want. Now, all we need to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 83}),\n",
       " Document(page_content='do is offset this up a bit. For this hand-optimized example, we’re going to use the first 7 pairs of\\n \\nneurons in the hidden layers to create the sine wave’s shape, then the bottom pair to offset\\n \\neverything vertically. If we set the bias of the 2nd neuron in the bottom pair to 1.0 and the\\n \\nweight to the output neuron as 0.7, we can vertically shift the line like so:\\n \\n \\nFig 4.26:\\n\\u200b\\n Using the bottom pair of neurons to offset the entire neural network function.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 83}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n23\\n \\nAt this point, we have completed the first section with an “area of effect” being the first upward\\n \\nsection of the sine wave. We can start on the next section that we wish to do. We can start by\\n \\nsetting all weights for this 2nd pair of neurons to 1, including the output neuron:\\n \\n \\nFig 4.27:\\n\\u200b\\n Starting to adjust the 2nd pair of neurons (from the top) for the next segment of the\\n \\n \\noverall function.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 84}),\n",
       " Document(page_content='overall function.\\n \\nAt this point, this 2nd pair of neurons’ activation is beginning too soon, which is impacting the\\n \\n“area of effect” of the top pair that we already aligned. To fix this, we want this second pair to\\n \\nstart influencing the output where the first pair deactivates, so we want to adjust the function\\n \\nhorizontally. As you can recall from earlier, we adjust the first neuron’s bias in this neuron pair to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 84}),\n",
       " Document(page_content='achieve this. Also, to modify the slope, we’ll set the weight coming into that first neuron for the\\n \\n2nd pair, setting it to 3.5. This is the same method we used to set the slope for the first section,\\n \\nwhich is controlled by the top pair of neurons in the hidden layer. After these adjustments:\\n \\n \\nFig 4.28:\\n\\u200b\\n Adjusting the weight and bias into the first neuron of the 2nd pair.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 84}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n24\\n \\nWe will now use the same methodology as we did with the first pair to set the deactivation point.\\n \\nWe set the weight for the 2nd neuron in the hidden layer pair to -1 and the bias to 0.27.\\n \\n \\nFig 4.29:\\n\\u200b\\n Adjusting the bias of the 2nd neuron in the 2nd pair.\\n \\nThen we can flip this section’s function, again the same way we did with the first one, by setting\\n \\nthe weight to the output neuron from 1.0 to -1.0:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 85}),\n",
       " Document(page_content='Fig 4.30:\\n\\u200b\\n Flipping the 2nd pair’s function segment, flipping the weight to the output neuron.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 85}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n25\\n \\nAnd again, just like the first pair, we will use the bottom pair to fix the vertical offset:\\n \\n \\nFig 4.31:\\n\\u200b\\n Using the bottom pair of neurons to adjust the network’s overall function.\\n \\nWe then just continue with this methodology. We’ll leave it flat for the top section, which means\\n \\nwe will only begin the activation for the 3rd pair of hidden layer neurons when we wish for the\\n \\nslope to start going down:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 86}),\n",
       " Document(page_content='Fig 4.32:\\n\\u200b\\n Adjusting the 3rd pair of neurons for the next segment.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 86}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n26\\n \\nThis process is simply repeated for each section, giving us a final result:\\n \\n \\nFig 4.33:\\n\\u200b\\n The completed process (see anim for all values).\\n \\nWe can then begin to pass data through to see how these neuron’s areas of effect come into play\\n \\n— only when both neurons are activated based on input:\\n \\n \\nFig 4.34:\\n\\u200b\\n Example of data passing through this hand-crafted model.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 87}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n27\\n \\nIn this case, given an input of 0.08, we can see the only pairs activated are the top ones, as this is\\n \\ntheir area of effect. Continuing with another example:\\n \\n \\nFig 4.35:\\n\\u200b\\n Example of data passing through this hand-crafted model.\\n \\nIn this case, only the fourth pair of neurons is activated. As you can see, even without any of the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 88}),\n",
       " Document(page_content='other weights, we’ve used some crude properties of a pair of neurons with rectified linear\\n \\nactivation functions to fit this sine wave pretty well. If we enable all of the weights now and allow\\n \\na mathematical optimizer to train, we can see even better fitment:\\n \\n \\nFig 4.36:\\n\\u200b\\n Example of fitment after fully-connecting the neurons and using an optimizer.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 88}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n28\\n \\nAnimation for the entirety of the concept of ReLU fitment:\\n \\n \\nAnim 4.12-4.36:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/mvp\\n \\nIt should begin to make more sense to you now how more neurons can enable more unique areas\\n \\nof effect, why we need two or more hidden layers, and why we need nonlinear activation\\n \\nfunctions to map nonlinear problems. For further example, we can take the above example with 2', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 89}),\n",
       " Document(page_content='hidden layers of 8 neurons each, and instead use 64 neurons per hidden layer, seeing the even\\n \\nfurther continued improvement:\\n \\n \\nFig 4.37:\\n\\u200b\\n Fitment with 2 hidden layers of 64 neurons each, fully connected, with optimizer.\\n \\n \\nAnim 4.37:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/moo', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 89}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n29\\n \\n \\nReLU Activation Function Code\\n \\nDespite the fancy sounding name, the rectified linear activation function is straightforward to\\n \\ncode. Most closely to its definition:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n1.1\\n\\u200b\\n, \\n\\u200b\\n2.2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n100\\n\\u200b\\n]\\n \\n \\noutput \\n\\u200b\\n= \\n\\u200b\\n[]\\n \\nfor \\n\\u200b\\ni \\n\\u200b\\nin \\n\\u200b\\ninputs:\\n \\n    \\n\\u200b\\nif \\n\\u200b\\ni \\n\\u200b\\n> \\n\\u200b\\n0\\n\\u200b\\n:\\n \\n        output.append(i)\\n \\n    \\n\\u200b\\nelse\\n\\u200b\\n:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 90}),\n",
       " Document(page_content='\\u200b\\nelse\\n\\u200b\\n:\\n \\n        output.append(\\n\\u200b\\n0\\n\\u200b\\n)\\n \\n \\nprint\\n\\u200b\\n(output)\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1.1\\n\\u200b\\n, \\n\\u200b\\n2.2\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n]\\n \\n \\nWe made up a list of values to start. The ReLU in this code is a loop where we’re checking if the\\n \\ncurrent value is greater than 0. If it is, we’re appending it to the output list, and if it’s not, we’re\\n \\nappending 0. This can be written more simply, as we just need to take the largest of two values: 0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 90}),\n",
       " Document(page_content='or neuron value. For example:\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n1.1\\n\\u200b\\n, \\n\\u200b\\n2.2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n100\\n\\u200b\\n]\\n \\n \\noutput \\n\\u200b\\n= \\n\\u200b\\n[]\\n \\nfor \\n\\u200b\\ni \\n\\u200b\\nin \\n\\u200b\\ninputs:\\n \\n    output.append(\\n\\u200b\\nmax\\n\\u200b\\n(\\n\\u200b\\n0\\n\\u200b\\n, i))\\n \\n \\nprint\\n\\u200b\\n(output)\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1.1\\n\\u200b\\n, \\n\\u200b\\n2.2\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 90}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n30\\n \\nNumPy contains an equivalent — \\n\\u200b\\nnp.maximum()\\n\\u200b\\n:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n1.1\\n\\u200b\\n, \\n\\u200b\\n2.2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n100\\n\\u200b\\n]\\n \\noutput \\n\\u200b\\n= \\n\\u200b\\nnp.maximum(\\n\\u200b\\n0\\n\\u200b\\n, inputs)\\n \\nprint\\n\\u200b\\n(output)\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0.  2.  0.  3.3 0.  1.1 2.2 0. \\n\\u200b\\n]\\n \\n \\nThis method compares each element of the input list (or an array) and returns an object of the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 91}),\n",
       " Document(page_content='same shape filled with new values. We will use it in our new rectified linear activation class:\\n \\n# ReLU activation\\n \\nclass \\n\\u200b\\nActivation_ReLU\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Calculate output values from input\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nnp.maximum(\\n\\u200b\\n0\\n\\u200b\\n, inputs)\\n \\n \\nLet’s apply this activation function to the dense layer’s outputs in our code:\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 91}),\n",
       " Document(page_content='\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 3 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Make a forward pass of our training data through this layer\\n \\ndense1.forward(X)\\n \\n \\n# Forward pass through activation func.\\n \\n# Takes in output from previous layer\\n \\nactivation1.forward(dense1.output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 91}),\n",
       " Document(page_content=\"Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n31\\n \\n# Let's see output of the first few samples:\\n \\nprint\\n\\u200b\\n(activation1.output[:\\n\\u200b\\n5\\n\\u200b\\n])\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n0.         0.         0.        \\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.         0.00011395 0.        \\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.         0.00031729 0.        \\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.         0.00052666 0.        \\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.         0.00071401 0.        \\n\\u200b\\n]]\\n \\n \\nAs you can see, negative values have been \\n\\u200b\\nclipped\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 92}),\n",
       " Document(page_content='\\u200b\\nclipped\\n\\u200b\\n (modified to be zero). That’s all there is to the\\n \\nrectified linear activation function used in the hidden layer. Let’s talk about the activation\\n \\nfunction that we are going to use on the output of the last layer.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 92}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n32\\n \\n \\nThe Softmax Activation Function\\n \\nIn our case, we’re looking to get this model to be a classifier, so we want an activation function\\n \\nmeant for classification. One of these is the Softmax activation function. First, why are we\\n \\nbothering with another activation function? It just depends on what our overall goals are. In this', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 93}),\n",
       " Document(page_content='case, the rectified linear unit is unbounded, not normalized with other units, and exclusive. “Not\\n \\nnormalized” implies the values can be anything, an output of \\n\\u200b\\n[12, 99, 318]\\n\\u200b\\n is without context, and\\n \\n“exclusive” means each output is independent of the others. To address this lack of context, the\\n \\nsoftmax activation on the output data can take in non-normalized, or uncalibrated, inputs and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 93}),\n",
       " Document(page_content='produce a normalized distribution of probabilities for our classes. In the case of classification,\\n \\nwhat we want to see is a prediction of which class the network “thinks” the input represents. This\\n \\ndistribution returned by the softmax activation function represents \\n\\u200b\\nconfidence scores\\n\\u200b\\n for each\\n \\nclass and will add up to 1. The predicted class is associated with the output neuron that returned', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 93}),\n",
       " Document(page_content='the largest confidence score. Still, we can also note the other confidence scores in our overarching\\n \\nalgorithm/program that uses this network. For example, if our network has a confidence\\n \\ndistribution for two classes: \\n\\u200b\\n[0.45, 0.55]\\n\\u200b\\n, the prediction is the 2nd class, but the confidence in this\\n \\nprediction isn’t very high. Maybe our program would not act in this case since it’s not very\\n \\nconfident.\\n \\nHere’s the function for the \\n\\u200b\\nSoftmax\\n\\u200b\\n:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 93}),\n",
       " Document(page_content='\\u200b\\nSoftmax\\n\\u200b\\n:\\n \\n \\nThat might look daunting, but we can break it down into simple pieces and express it in Python\\n \\ncode, which you may find is more approachable than the formula above. To start, here are\\n \\nexample outputs from a neural network layer:\\n \\nlayer_outputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n4.8\\n\\u200b\\n, \\n\\u200b\\n1.21\\n\\u200b\\n, \\n\\u200b\\n2.385\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 93}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n33\\n \\nThe first step for us is to “exponentiate” the outputs. We do this with Euler’s number, \\n\\u200b\\ne, \\n\\u200b\\nwhich is\\n \\nroughly \\n\\u200b\\n2.71828182846\\n\\u200b\\n and referred to as the “exponential growth” number. Exponentiating is\\n \\ntaking this constant to the power of the given parameter:\\n \\n \\nBoth the numerator and the denominator of the Softmax function contain \\n\\u200b\\ne\\n\\u200b\\n raised to the power of\\n \\nz\\n\\u200b\\n, where \\n\\u200b\\nz\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 94}),\n",
       " Document(page_content='z\\n\\u200b\\n, where \\n\\u200b\\nz\\n\\u200b\\n, given indices, means a singular output value — the index \\n\\u200b\\ni\\n\\u200b\\n means the current sample\\n \\nand the index \\n\\u200b\\nj\\n\\u200b\\n means the current output in this sample. The numerator exponentiates the current\\n \\noutput value and the denominator takes a sum of all of the exponentiated outputs for a given\\n \\nsample. We need then to calculate these exponentiates to continue:\\n \\n# Values from the previous output when we described\\n \\n# what a neural network is\\n \\nlayer_outputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n4.8\\n\\u200b\\n,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 94}),\n",
       " Document(page_content=\"\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n4.8\\n\\u200b\\n, \\n\\u200b\\n1.21\\n\\u200b\\n, \\n\\u200b\\n2.385\\n\\u200b\\n]\\n \\n \\n# e - mathematical constant, we use E here to match a common coding\\n \\n# style where constants are uppercased\\n \\nE \\n\\u200b\\n= \\n\\u200b\\n2.71828182846  \\n\\u200b\\n# you can also use math.e\\n \\n \\n# For each value in a vector, calculate the exponential value\\n \\nexp_values \\n\\u200b\\n= \\n\\u200b\\n[]\\n \\nfor \\n\\u200b\\noutput \\n\\u200b\\nin \\n\\u200b\\nlayer_outputs:\\n \\n    exp_values.append(E \\n\\u200b\\n** \\n\\u200b\\noutput)  \\n\\u200b\\n# ** - power operator in Python\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'exponentiated values:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(exp_values)\\n \\n \\n \\n>>>\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 94}),\n",
       " Document(page_content='>>>\\n \\nexponentiated values:\\n \\n[\\n\\u200b\\n121.51041751893969\\n\\u200b\\n, \\n\\u200b\\n3.3534846525504487\\n\\u200b\\n, \\n\\u200b\\n10.85906266492961\\n\\u200b\\n]\\n \\n \\nExponentiation serves multiple purposes. To calculate the probabilities, we need non-negative\\n \\nvalues. Imagine the output as \\n\\u200b\\n[\\n\\u200b\\n4.8\\n\\u200b\\n, \\n\\u200b\\n1.21\\n\\u200b\\n, -\\n\\u200b\\n2.385\\n\\u200b\\n]\\n\\u200b\\n — even after normalization, the last\\n \\nvalue will still be negative since we’ll just divide all of them by their sum. A negative probability', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 94}),\n",
       " Document(page_content='(or confidence) does not make much sense. An exponential value of any number is always\\n \\nnon-negative — it returns 0 for negative infinity, 1 for the input of 0, and increases for positive\\n \\nvalues:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 94}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n34\\n \\n \\nFig 4.38:\\n\\u200b\\n Graph of an exponential function.\\n \\nThe exponential function is a monotonic function. This means that, with higher input values,\\n \\noutputs are also higher, so we won’t change the predicted class after applying it while making\\n \\nsure that we get non-negative values. It also adds stability to the result as the normalized', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 95}),\n",
       " Document(page_content='exponentiation is more about the difference between numbers than their magnitudes. Once we’ve\\n \\nexponentiated, we want to convert these numbers to a probability distribution (converting the\\n \\nvalues into the vector of confidences, one for each class, which add up to 1 for everything in the\\n \\nvector). What that means is that we’re about to perform a normalization where we take a given\\n \\nvalue and divide it by the sum of all of the values. For our outputs, exponentiated at this stage,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 95}),\n",
       " Document(page_content='that’s what the equation of the Softmax function describes next — to take a given exponentiated\\n \\nvalue and divide it by the sum of all of the exponentiated values. Since each output value\\n \\nnormalizes to a fraction of the sum, all of the values are now in the range of 0 to 1 and add up to 1\\n \\n— they share the probability of 1 between themselves. Let’s add the sum and normalization to the\\n \\ncode:\\n \\n# Now normalize values\\n \\nnorm_base \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(exp_values)  \\n\\u200b\\n# We sum all values', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 95}),\n",
       " Document(page_content=\"norm_values \\n\\u200b\\n= \\n\\u200b\\n[]\\n \\nfor \\n\\u200b\\nvalue \\n\\u200b\\nin \\n\\u200b\\nexp_values:\\n \\n    norm_values.append(value \\n\\u200b\\n/ \\n\\u200b\\nnorm_base)\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'Normalized exponentiated values:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(norm_values)\\n \\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'Sum of normalized values:'\\n\\u200b\\n, \\n\\u200b\\nsum\\n\\u200b\\n(norm_values))\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 95}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n35\\n \\n \\n>>>\\n \\nNormalized exponentiated values:\\n \\n[\\n\\u200b\\n0.8952826639573506\\n\\u200b\\n, \\n\\u200b\\n0.024708306782070668\\n\\u200b\\n, \\n\\u200b\\n0.08000902926057876\\n\\u200b\\n]\\n \\nSum of normalized values: \\n\\u200b\\n1.0\\n \\n \\nWe can perform the same set of operations with the use of NumPy in the following way:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Values from the earlier previous when we described\\n \\n# what a neural network is\\n \\n \\nlayer_outputs \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n4.8\\n\\u200b\\n, \\n\\u200b\\n1.21\\n\\u200b\\n, \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 96}),\n",
       " Document(page_content=\"\\u200b\\n, \\n\\u200b\\n1.21\\n\\u200b\\n, \\n\\u200b\\n2.385\\n\\u200b\\n]\\n \\n \\n# For each value in a vector, calculate the exponential value\\n \\nexp_values \\n\\u200b\\n= \\n\\u200b\\nnp.exp(layer_outputs)\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'exponentiated values:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(exp_values)\\n \\n \\n# Now normalize values\\n \\nnorm_values \\n\\u200b\\n= \\n\\u200b\\nexp_values \\n\\u200b\\n/ \\n\\u200b\\nnp.sum(exp_values)\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'normalized exponentiated values:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(norm_values)\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'sum of normalized values:'\\n\\u200b\\n, np.sum(norm_values))\\n \\n \\n \\n>>>\\n \\nexponentiated values:\\n \\n[\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 96}),\n",
       " Document(page_content='[\\n\\u200b\\n121.51041752   3.35348465  10.85906266\\n\\u200b\\n]\\n \\nnormalized exponentiated values:\\n \\n[\\n\\u200b\\n0.89528266 0.02470831 0.08000903\\n\\u200b\\n]\\n \\nsum of normalized values: \\n\\u200b\\n0.9999999999999999\\n \\n \\nNotice the results are similar, but faster to calculate and the code is easier to read with NumPy.\\n \\nWe can exponentiate all of the values with a single call of the \\n\\u200b\\nnp.exp()\\n\\u200b\\n, then immediately\\n \\nnormalize them with the sum. To train in batches, we need to convert this functionality to accept', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 96}),\n",
       " Document(page_content='layer outputs in batches. Doing this is as easy as:\\n \\n# Get unnormalized probabilities\\n \\nexp_values \\n\\u200b\\n= \\n\\u200b\\nnp.exp(inputs)\\n \\n \\n# Normalize them for each sample\\n \\nprobabilities \\n\\u200b\\n= \\n\\u200b\\nexp_values \\n\\u200b\\n/ \\n\\u200b\\nnp.sum(exp_values, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 96}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n36\\n \\nWe have some new functions. Specifically, \\n\\u200b\\nnp.exp()\\n\\u200b\\n does the \\n\\u200b\\nE\\n\\u200b\\n**\\n\\u200b\\noutput\\n\\u200b\\n part. We should\\n \\nalso address what \\n\\u200b\\naxis \\n\\u200b\\nand \\n\\u200b\\nkeepdims \\n\\u200b\\nmean in the above. Let’s first discuss the \\n\\u200b\\naxis\\n\\u200b\\n. Axis is\\n \\neasier to show than tell, but, in a 2D array/matrix, axis 0 refers to the rows, and axis 1 refers to\\n \\nthe columns. Let’s see some examples of how \\n\\u200b\\naxis\\n\\u200b\\n affects the sum using NumPy. First, we', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 97}),\n",
       " Document(page_content=\"will just show the default, which is \\n\\u200b\\nNone\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n \\nlayer_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n4.8\\n\\u200b\\n, \\n\\u200b\\n1.21\\n\\u200b\\n, \\n\\u200b\\n2.385\\n\\u200b\\n],\\n \\n                          [\\n\\u200b\\n8.9\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.81\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],\\n \\n                          [\\n\\u200b\\n1.41\\n\\u200b\\n, \\n\\u200b\\n1.051\\n\\u200b\\n, \\n\\u200b\\n0.026\\n\\u200b\\n]])\\n \\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'Sum without axis'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(np.sum(layer_outputs))\\n \\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'This will be identical to the above since default is None:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(np.sum(layer_outputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\nNone\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 97}),\n",
       " Document(page_content='\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\nNone\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\nSum without axis\\n \\n18.172\\n \\nThis will be identical to the above since default is None:\\n \\n18.172\\n \\n \\nWith no axis specified, we are just summing all of the values, even if they’re in varying\\n \\ndimensions. Next, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n0\\n\\u200b\\n. This means to sum row-wise, along axis 0. In other words, the output\\n \\nhas the same size as this axis, as at each of the positions of this output, the values from all the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 97}),\n",
       " Document(page_content=\"other dimensions at this position are summed to form it. In the case of our 2D array, where we\\n \\nhave only a single other dimension, the columns, the output vector will sum these columns. This\\n \\nmeans we’ll perform \\n\\u200b\\n4.8+8.9+1.41\\n\\u200b\\n and so on.\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'Another way to think of it w/ a matrix == axis 0: columns:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(np.sum(layer_outputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n0\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\nAnother way to think of it w/\\n\\u200b\\n \\n\\u200b\\na matrix ==\\n\\u200b\\n \\n\\u200b\\naxis 0: columns:\\n \\n[\\n\\u200b\\n15.11   0.451  2.611\\n\\u200b\\n]\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 97}),\n",
       " Document(page_content='\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 97}),\n",
       " Document(page_content=\"Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n37\\n \\nThis isn’t what we want, though. We want sums of the rows. You can probably guess how to do\\n \\nthis with NumPy, but we’ll still show the “from scratch” version:\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'But we want to sum the rows instead, like this w/ raw py:'\\n\\u200b\\n)\\n \\n \\nfor \\n\\u200b\\ni \\n\\u200b\\nin \\n\\u200b\\nlayer_outputs:\\n \\n    \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\nsum\\n\\u200b\\n(i))\\n \\n \\n \\n>>>\\n \\nBut we want to sum the rows instead, like this w/ raw py:\\n \\n8.395\\n \\n7.29\\n \\n2.4869999999999997\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 98}),\n",
       " Document(page_content=\"With the above, we could append these to some list in any way we want. That said, we’re going to\\n \\nuse NumPy. As you probably guessed, we’re going to sum along axis 1:\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'So we can sum axis 1, but note the current shape:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(np.sum(layer_outputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\nSo we can sum axis 1, but note the current shape:\\n \\n[\\n\\u200b\\n8.395 7.29 2.487\\n\\u200b\\n]\\n \\n \\nAs pointed out by “\\n\\u200b\\nnote the current shape\\n\\u200b\\n,” we did get the sums that we expected, but\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 98}),\n",
       " Document(page_content=\"actually, we want to simplify the outputs to a single value per sample. We’re trying to sum all\\n \\nthe outputs from a layer for each sample in a batch; converting the layer’s output array with row\\n \\nlength equal to the number of neurons in the layer, to just one value. We need a column vector\\n \\nwith these values since it will let us normalize the whole batch of samples, sample-wise, with a\\n \\nsingle calculation.\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'Sum axis 1, but keep the same dimensions as input:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 98}),\n",
       " Document(page_content='\\u200b\\n)\\n \\nprint\\n\\u200b\\n(np.sum(layer_outputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\nSum axis 1, but keep the same dimensions as input:\\n \\n[[\\n\\u200b\\n8.395\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n7.29 \\n\\u200b\\n]\\n \\n [\\n\\u200b\\n2.487\\n\\u200b\\n]]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 98}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n38\\n \\nWith this, we keep the same dimensions as the input. Now, if we divide the array containing a\\n \\nbatch of the outputs with this array, NumPy will perform this sample-wise. That means that it’ll\\n \\ndivide all of the values from each output row by the corresponding row from the sum array. Since\\n \\nthis sum in each row is a single value, it’ll be used for the division with every value from the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 99}),\n",
       " Document(page_content='corresponding output row). We can combine all of this into a softmax class, like:\\n \\n# Softmax activation\\n \\nclass \\n\\u200b\\nActivation_Softmax\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Get unnormalized probabilities\\n \\n        \\n\\u200b\\nexp_values \\n\\u200b\\n= \\n\\u200b\\nnp.exp(inputs \\n\\u200b\\n- \\n\\u200b\\nnp.max(inputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n))\\n \\n        \\n\\u200b\\n# Normalize them for each sample\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 99}),\n",
       " Document(page_content='\\u200b\\nprobabilities \\n\\u200b\\n= \\n\\u200b\\nexp_values \\n\\u200b\\n/ \\n\\u200b\\nnp.sum(exp_values, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nprobabilities\\n \\n \\nFinally, we also included a subtraction of the largest of the inputs before we did the\\n \\nexponentiation. There are two main pervasive challenges with neural networks: “dead neurons”\\n \\nand very large numbers (referred to as “exploding” values). “Dead” neurons and enormous', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 99}),\n",
       " Document(page_content='numbers can wreak havoc down the line and render a network useless over time. The exponential\\n \\nfunction used in softmax activation is one of the sources of exploding values. Let’s see some\\n \\nexamples of how and why this can easily happen:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\nprint\\n\\u200b\\n(np.exp(\\n\\u200b\\n1\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\n2.718281828459045\\n \\n \\n \\nprint\\n\\u200b\\n(np.exp(\\n\\u200b\\n10\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\n22026.465794806718', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 99}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n39\\n \\nprint\\n\\u200b\\n(np.exp(\\n\\u200b\\n100\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\n2.6881171418161356e+43\\n \\n \\n \\nprint\\n\\u200b\\n(np.exp(\\n\\u200b\\n1000\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\n__main__:1: \\n\\u200b\\nRuntimeWarning\\n\\u200b\\n: overflow encountered in exp\\n \\ninf\\n \\n \\nIt doesn’t take a very large number, in this case, a mere \\n\\u200b\\n1,000\\n\\u200b\\n, to cause an overflow error. We\\n \\nknow the exponential function tends toward 0 as its input value approaches negative infinity, and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 100}),\n",
       " Document(page_content='the output is 1 when the input is 0 (as shown in the chart earlier):\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\nprint\\n\\u200b\\n(np.exp(\\n\\u200b\\n-\\n\\u200b\\nnp.inf), np.exp(\\n\\u200b\\n0\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\n0.0 1.0\\n \\n \\nWe can use this property to prevent the exponential function from overflowing. Suppose we\\n \\nsubtract the maximum value from a list of input values. We would then change the output values\\n \\nto always be in a range from some negative value up to 0, as the largest number subtracted by', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 100}),\n",
       " Document(page_content='itself returns 0, and any smaller number subtracted by it will result in a negative number —\\n \\nexactly the range discussed above. With Softmax, thanks to the normalization, we can subtract\\n \\nany value from all of the inputs, and it will not change the output:\\n \\nsoftmax \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n \\nsoftmax.forward([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]])\\n \\nprint\\n\\u200b\\n(softmax.output)\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n0.09003057 0.24472847 0.66524096\\n\\u200b\\n]]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 100}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n40\\n \\nsoftmax.forward([[\\n\\u200b\\n-\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n]])  \\n\\u200b\\n# subtracted 3 - max from the list\\n \\nprint\\n\\u200b\\n(softmax.output)\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n0.09003057 0.24472847 0.66524096\\n\\u200b\\n]]\\n \\n \\nThis is another useful property of the exponentiated and normalized function. There’s one more\\n \\nthing to mention in addition to these calculations. What happens if we divide the layer’s output\\n \\ndata, \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 101}),\n",
       " Document(page_content='\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n, for example, by 2?\\n \\nsoftmax.forward([[\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1.5\\n\\u200b\\n]])\\n \\nprint\\n\\u200b\\n(softmax.output)\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n0.18632372 0.30719589 0.50648039\\n\\u200b\\n]]\\n \\n \\nThe output confidences have changed due to the nonlinearity nature of the exponentiation. This\\n \\nis one example of why we need to scale all of the input data to a neural network in the same way,\\n \\nwhich we’ll explain in further detail in chapter 22.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 101}),\n",
       " Document(page_content='Now, we can add another dense layer as the output layer, setting it to contain as many inputs as\\n \\nthe previous layer has outputs and as many outputs as our data includes classes. Then we can\\n \\napply the softmax activation to the output of this new layer:\\n \\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 3 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 101}),\n",
       " Document(page_content='2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 3 input features (as we take output\\n \\n# of previous layer here) and 3 output values\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax activation (to be used with Dense layer):\\n \\nactivation2 \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n \\n# Make a forward pass of our training data through this layer\\n \\ndense1.forward(X)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 101}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n41\\n \\n# Make a forward pass through activation function\\n \\n# it takes the output of first dense layer here\\n \\nactivation1.forward(dense1.output)\\n \\n \\n# Make a forward pass through second Dense layer\\n \\n# it takes outputs of activation function of first layer as inputs\\n \\ndense2.forward(activation1.output)\\n \\n \\n# Make a forward pass through activation function\\n \\n# it takes the output of second dense layer here', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 102}),\n",
       " Document(page_content=\"activation2.forward(dense2.output)\\n \\n \\n# Let's see output of the first few samples:\\n \\nprint\\n\\u200b\\n(activation2.output[:\\n\\u200b\\n5\\n\\u200b\\n])\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n0.33333334 0.33333334 0.33333334\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333316 0.3333332  0.33333364\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333287 0.3333329  0.33333418\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.3333326  0.33333263 0.33333477\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333233 0.3333324  0.33333528\\n\\u200b\\n]]\\n \\n \\nAs you can see, the distribution of predictions is almost equal, as each of the samples has ~33%\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 102}),\n",
       " Document(page_content='(0.33) predictions for each class. This results from the random initialization of weights (a draw\\n \\nfrom the normal distribution, as not every random initialization will result in this) and zeroed\\n \\nbiases. These outputs are also our “confidence scores.” To determine which classification the\\n \\nmodel has chosen to be the prediction, we perform an \\n\\u200b\\nargmax\\n\\u200b\\n on these outputs, which checks\\n \\nwhich of the classes in the output distribution has the highest confidence and returns its index -', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 102}),\n",
       " Document(page_content='the predicted class index. That said, the confidence score can be as important as the class\\n \\nprediction itself. For example, the argmax of \\n\\u200b\\n[\\n\\u200b\\n0.22\\n\\u200b\\n, \\n\\u200b\\n0.6\\n\\u200b\\n, \\n\\u200b\\n0.18\\n\\u200b\\n]\\n\\u200b\\n \\n\\u200b\\nis the same as the argmax for\\n \\n \\n[\\n\\u200b\\n0.32\\n\\u200b\\n, \\n\\u200b\\n0.36\\n\\u200b\\n, \\n\\u200b\\n0.32\\n\\u200b\\n]\\n\\u200b\\n. In both of these, the argmax function would return an index value of 1\\n \\n(the 2nd element in Python’s zero-indexed paradigm), but obviously, a 60% confidence is much\\n \\nbetter than a 36% confidence.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 102}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n42\\n \\n \\nFull code up to this point:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nimport \\n\\u200b\\nnnfs\\n \\nfrom \\n\\u200b\\nnnfs.datasets \\n\\u200b\\nimport \\n\\u200b\\nspiral_data\\n \\n \\nnnfs.init()\\n \\n \\n \\n# Dense layer\\n \\nclass \\n\\u200b\\nLayer_Dense\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Layer initialization\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nn_inputs\\n\\u200b\\n, \\n\\u200b\\nn_neurons\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Initialize weights and biases\\n \\n        \\n\\u200b\\nself.weights \\n\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 103}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(n_inputs, n_neurons)\\n \\n        self.biases \\n\\u200b\\n= \\n\\u200b\\nnp.zeros((\\n\\u200b\\n1\\n\\u200b\\n, n_neurons))\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Calculate output values from inputs, weights and biases\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, self.weights) \\n\\u200b\\n+ \\n\\u200b\\nself.biases\\n \\n \\n \\n# ReLU activation\\n \\nclass \\n\\u200b\\nActivation_ReLU\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 103}),\n",
       " Document(page_content='\\u200b\\n):\\n \\n        \\n\\u200b\\n# Calculate output values from inputs\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nnp.maximum(\\n\\u200b\\n0\\n\\u200b\\n, inputs)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 103}),\n",
       " Document(page_content='Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n \\n43\\n \\n# Softmax activation\\n \\nclass \\n\\u200b\\nActivation_Softmax\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Get unnormalized probabilities\\n \\n        \\n\\u200b\\nexp_values \\n\\u200b\\n= \\n\\u200b\\nnp.exp(inputs \\n\\u200b\\n- \\n\\u200b\\nnp.max(inputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n))\\n \\n        \\n\\u200b\\n# Normalize them for each sample\\n \\n        \\n\\u200b\\nprobabilities', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 104}),\n",
       " Document(page_content='\\u200b\\nprobabilities \\n\\u200b\\n= \\n\\u200b\\nexp_values \\n\\u200b\\n/ \\n\\u200b\\nnp.sum(exp_values, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nprobabilities\\n \\n \\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 3 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 104}),\n",
       " Document(page_content='activation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 3 input features (as we take output\\n \\n# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax activation (to be used with Dense layer):\\n \\nactivation2 \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n \\n# Make a forward pass of our training data through this layer\\n \\ndense1.forward(X)\\n \\n \\n# Make a forward pass through activation function', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 104}),\n",
       " Document(page_content='# it takes the output of first dense layer here\\n \\nactivation1.forward(dense1.output)\\n \\n \\n# Make a forward pass through second Dense layer\\n \\n# it takes outputs of activation function of first layer as inputs\\n \\ndense2.forward(activation1.output)\\n \\n \\n# Make a forward pass through activation function\\n \\n# it takes the output of second dense layer here\\n \\nactivation2.forward(dense2.output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 104}),\n",
       " Document(page_content=\"Chapter 4 - Activation Functions - Neural Networks from Scratch in Python\\n44 \\n# Let's see output of the first few samples:  \\nprint\\u200b(activation2.output[:\\u200b5\\u200b])  \\n>>> \\n[[\\u200b0.33333334 0.33333334 0.33333334\\u200b]  \\n [\\u200b0.33333316 0.3333332  0.33333364\\u200b]  \\n [\\u200b0.33333287 0.3333329  0.33333418\\u200b]  \\n [\\u200b0.3333326  0.33333263 0.33333477\\u200b]  \\n [\\u200b0.33333233 0.3333324  0.33333528\\u200b]]  \\nWe’ve completed what we need for forward-passing data through our model. We used the\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 105}),\n",
       " Document(page_content='Rectified Linear (ReLU \\u200b) activation function on the hidden layer, which works on a per-neuron  \\nbasis. We additionally used the \\u200bSoftmax \\u200b activation function for the output layer since it accepts  \\nnon-normalized values as input and outputs a probability distribution, which we’re using as  \\nconfidence scores for each class. Recall that, although neurons are interconnected, they each have  \\ntheir respective weights and biases and are not “normalized” with each other.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 105}),\n",
       " Document(page_content='As you can see, our example model is currently random. To remedy this, we need a way to  \\ncalculate how wrong the neural network is at current predictions and begin adjusting weights  \\nand biases to decrease error over time. Thus, our next step is to quantify how wrong the model is  \\nthrough what’s defined as a \\u200bloss function \\u200b. \\nSupplementary Material: \\u200bhttps://nnfs.io/ch4  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 105}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n6\\n \\n \\n \\n \\n \\nChapter 5\\n \\nCalculating Network Error with\\n \\nLoss\\n \\nWith a randomly-initialized model, or even a model initialized with more sophisticated\\n \\napproaches, our goal is to train, or teach, a model over time. To train a model, we tweak the\\n \\nweights and biases to improve the model’s accuracy and confidence. To do this, we calculate how\\n \\nmuch error the model has. The \\n\\u200b\\nloss function\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 106}),\n",
       " Document(page_content='\\u200b\\nloss function\\n\\u200b\\n, also referred to as the \\n\\u200b\\ncost function\\n\\u200b\\n, is the\\n \\nalgorithm that quantifies how wrong a model is. \\n\\u200b\\nLoss\\n\\u200b\\n is the measure of this metric. Since loss is\\n \\nthe model’s error, we ideally want it to be 0.\\n \\nYou may wonder why we do not calculate the error of a model based on the argmax accuracy.\\n \\n \\nRecall our earlier example of confidence: \\n\\u200b\\n[\\n\\u200b\\n0.22\\n\\u200b\\n, \\n\\u200b\\n0.6\\n\\u200b\\n, \\n\\u200b\\n0.18\\n\\u200b\\n]\\n\\u200b\\n vs \\n\\u200b\\n[\\n\\u200b\\n0.32\\n\\u200b\\n, \\n\\u200b\\n0.36\\n\\u200b\\n, \\n\\u200b\\n0.32\\n\\u200b\\n]\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 106}),\n",
       " Document(page_content=', \\n\\u200b\\n0.32\\n\\u200b\\n]\\n\\u200b\\n.\\n \\nIf the correct class were indeed the middle one (index 1), the model accuracy would be identical\\n \\nbetween the two above. But are these two examples \\n\\u200b\\nreally\\n\\u200b\\n as accurate as each other? They are\\n \\nnot, because accuracy is simply applying an argmax to the output to find the index of the\\n \\nbiggest value. The output of a neural network is actually confidence, and more confidence in', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 106}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n7\\n \\nthe correct answer is better. Because of this, we strive to increase correct confidence and\\n \\ndecrease misplaced confidence.\\n \\nCategorical Cross-Entropy Loss\\n \\nIf you’re familiar with linear regression, then you already know one of the loss functions used\\n \\nwith neural networks that do regression: \\n\\u200b\\nsquared error\\n\\u200b\\n (or \\n\\u200b\\nmean squared error\\n\\u200b\\n \\n\\u200b\\nwith neural\\n \\nnetworks).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 107}),\n",
       " Document(page_content='networks).\\n \\nWe’re not performing regression in this example; we’re classifying, so we need a different loss\\n \\nfunction. The model has a softmax activation function for the output layer, which means it’s\\n \\noutputting a probability distribution. \\n\\u200b\\nCategorical cross-entropy\\n\\u200b\\n is explicitly used to compare\\n \\na “ground-truth” probability (\\n\\u200b\\ny \\n\\u200b\\nor\\n\\u200b\\n \\n\\u200b\\n“\\n\\u200b\\ntargets\\n\\u200b\\n”) and some predicted distribution (\\n\\u200b\\ny-hat \\n\\u200b\\nor\\n \\n“\\n\\u200b\\npredictions\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 107}),\n",
       " Document(page_content='“\\n\\u200b\\npredictions\\n\\u200b\\n”), so it makes sense to use cross-entropy here. It is also one of the most\\n \\ncommonly used loss functions with a softmax activation on the output layer.\\n \\nThe formula for calculating the categorical cross-entropy of \\n\\u200b\\ny\\n\\u200b\\n (actual/desired distribution) and\\n \\ny-hat\\n\\u200b\\n (predicted distribution) is:\\n \\n \\nWhere \\n\\u200b\\nL\\n\\u200b\\ni\\n\\u200b\\n denotes sample loss value, \\n\\u200b\\ni\\n\\u200b\\n is the i-th sample in the set,  \\n\\u200b\\nj \\n\\u200b\\nis the label/output index, \\n\\u200b\\ny\\n \\ndenotes the target values, and \\n\\u200b\\ny-hat\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 107}),\n",
       " Document(page_content='\\u200b\\ny-hat\\n\\u200b\\n denotes the predicted values.\\n \\nOnce we start coding the solution, we’ll simplify it further to -\\n\\u200b\\nlog(correct_class_confidence)\\n\\u200b\\n, the\\n \\nformula for which is:\\n \\n \\nWhere \\n\\u200b\\nL\\n\\u200b\\ni\\n\\u200b\\n denotes sample loss value, \\n\\u200b\\ni\\n\\u200b\\n is the i-th sample in a set, \\n\\u200b\\nk\\n\\u200b\\n is the index of the target label\\n \\n(ground-true label), \\n\\u200b\\ny\\n\\u200b\\n denotes the target values and \\n\\u200b\\ny-hat\\n\\u200b\\n denotes the predicted values.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 107}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n8\\n \\nYou may ask why we call this cross-entropy and not \\n\\u200b\\nlog loss\\n\\u200b\\n, which is also a type of loss. If you\\n \\ndo not know what log loss is, you may wonder why there is such a fancy looking formula for\\n \\nwhat looks to be a fairly basic description.\\n \\nIn general, the log loss error function is what we apply to the output of a binary logistic regression', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 108}),\n",
       " Document(page_content='model (which we’ll describe in chapter 16)  — there are only two classes in the distribution, each\\n \\nof them applying to a single output (neuron) which is targeted as a 0 or 1. In our case, we have a\\n \\nclassification model that returns a probability distribution over all of the outputs. Cross-entropy\\n \\ncompares two probability distributions. In our case, we have a softmax output, let’s say it’s:\\n \\nsoftmax_output \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 108}),\n",
       " Document(page_content='\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n]\\n \\nWhich probability distribution do we intend to compare this to? We have 3 class confidences in\\n \\nthe above output, and let’s assume that the desired prediction is the first class (index 0, which is\\n \\ncurrently 0.7). If that’s the intended prediction, then the desired probability distribution is  \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n0\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n. Cross-entropy can also work on probability distributions like \\n\\u200b\\n[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.3\\n\\u200b\\n]\\n\\u200b\\n; they', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 108}),\n",
       " Document(page_content='0.3\\n\\u200b\\n]\\n\\u200b\\n; they\\n \\nwouldn’t have to look like the one above. That said, the desired probabilities will consist of a 1\\n \\nin the desired class, and a 0 in the remaining undesired classes. Arrays or vectors like this are\\n \\ncalled \\n\\u200b\\none-hot\\n\\u200b\\n,\\n\\u200b\\n meaning one of the values is “hot” (on), with a value of 1, and the rest are\\n \\n“cold” (off), with values of 0. When comparing the model’s results to a one-hot vector using', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 108}),\n",
       " Document(page_content='cross-entropy, the other parts of the equation zero out, and the target probability’s log loss is\\n \\nmultiplied by 1, making the cross-entropy calculation relatively simple. This is also a special\\n \\ncase of the cross-entropy calculation, called categorical cross-entropy. To exemplify this — if\\n \\nwe take a softmax output of \\n\\u200b\\n[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n]\\n\\u200b\\n and targets of \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n, we can apply the\\n \\ncalculations as follows:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 108}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n9\\n \\nLet’s see the Python code for this:\\n \\nimport \\n\\u200b\\nmath\\n \\n \\n \\n# An example output from the output layer of the neural network\\n \\nsoftmax_output \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n]\\n \\n# Ground truth\\n \\ntarget_output \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n]\\n \\n \\nloss \\n\\u200b\\n= -\\n\\u200b\\n(math.log(softmax_output[\\n\\u200b\\n0\\n\\u200b\\n])\\n\\u200b\\n*\\n\\u200b\\ntarget_output[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n         \\n\\u200b\\nmath.log(softmax_output[\\n\\u200b\\n1\\n\\u200b\\n])\\n\\u200b\\n*\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 109}),\n",
       " Document(page_content='\\u200b\\n1\\n\\u200b\\n])\\n\\u200b\\n*\\n\\u200b\\ntarget_output[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+\\n \\n         \\n\\u200b\\nmath.log(softmax_output[\\n\\u200b\\n2\\n\\u200b\\n])\\n\\u200b\\n*\\n\\u200b\\ntarget_output[\\n\\u200b\\n2\\n\\u200b\\n])\\n \\n \\nprint\\n\\u200b\\n(loss)\\n \\n \\n \\n>>>\\n \\n0.35667494393873245\\n \\n \\n \\nThat’s the full categorical cross-entropy calculation, but we can make a few assumptions given\\n \\none-hot target vectors. First, what are the values for \\n\\u200b\\ntarget_output[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n \\n\\u200b\\nand\\n \\ntarget_output[\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n in this case? They’re both 0, and anything multiplied by 0 is 0. Thus, we', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 109}),\n",
       " Document(page_content='don’t need to calculate these indices. Next, what’s the value for \\n\\u200b\\ntarget_output[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n \\n\\u200b\\nin this\\n \\ncase? It’s 1. So this can be omitted as any number multiplied by 1 remains the same. The same\\n \\noutput then, in this example, can be calculated with:\\n \\nloss \\n\\u200b\\n= -\\n\\u200b\\nmath.log(softmax_output[\\n\\u200b\\n0\\n\\u200b\\n])\\n \\n \\nWhich still gives us:\\n \\n>>>\\n \\n0.35667494393873245\\n \\n \\nAs you can see with one-hot vector targets, or scalar values that represent them, we can make', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 109}),\n",
       " Document(page_content='some simple assumptions and use a more basic calculation — what was once an involved formula\\n \\nreduces to the negative log of the target class’ confidence score — the second formula presented\\n \\nat the beginning of this chapter.\\n \\nAs we’ve already discussed, the example confidence level might look like \\n\\u200b\\n[\\n\\u200b\\n0.22\\n\\u200b\\n, \\n\\u200b\\n0.6\\n\\u200b\\n, \\n\\u200b\\n0.18\\n\\u200b\\n]\\n \\nor \\n\\u200b\\n[\\n\\u200b\\n0.32\\n\\u200b\\n, \\n\\u200b\\n0.36\\n\\u200b\\n, \\n\\u200b\\n0.32\\n\\u200b\\n]\\n\\u200b\\n. In both cases, the \\n\\u200b\\nargmax\\n\\u200b\\n of these vectors will return the second', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 109}),\n",
       " Document(page_content='class as the prediction, but the model’s confidence about these predictions is high only for one of\\n \\nthem. The \\n\\u200b\\nCategorical Cross-Entropy Loss\\n\\u200b\\n accounts for that and outputs a larger loss the lower\\n \\nthe confidence is:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 109}),\n",
       " Document(page_content=\"Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n10\\n \\nimport \\n\\u200b\\nmath\\n \\n \\nprint\\n\\u200b\\n(math.log(\\n\\u200b\\n1.\\n\\u200b\\n))\\n \\nprint\\n\\u200b\\n(math.log(\\n\\u200b\\n0.95\\n\\u200b\\n))\\n \\nprint\\n\\u200b\\n(math.log(\\n\\u200b\\n0.9\\n\\u200b\\n))\\n \\nprint\\n\\u200b\\n(math.log(\\n\\u200b\\n0.8\\n\\u200b\\n))\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'...'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(math.log(\\n\\u200b\\n0.2\\n\\u200b\\n))\\n \\nprint\\n\\u200b\\n(math.log(\\n\\u200b\\n0.1\\n\\u200b\\n))\\n \\nprint\\n\\u200b\\n(math.log(\\n\\u200b\\n0.05\\n\\u200b\\n))\\n \\nprint\\n\\u200b\\n(math.log(\\n\\u200b\\n0.01\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\n0.0\\n \\n-\\n\\u200b\\n0.05129329438755058\\n \\n-\\n\\u200b\\n0.10536051565782628\\n \\n-\\n\\u200b\\n0.2231435513142097\\n \\n...\\n \\n-\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 110}),\n",
       " Document(page_content='...\\n \\n-\\n\\u200b\\n1.6094379124341003\\n \\n-\\n\\u200b\\n2.3025850929940455\\n \\n-\\n\\u200b\\n2.995732273553991\\n \\n-\\n\\u200b\\n4.605170185988091\\n \\n \\nWe’ve printed different log values for a few example confidences. When the confidence level\\n \\nequals \\n\\u200b\\n1\\n\\u200b\\n, meaning the model is 100% “sure” about its prediction, the loss value for this sample\\n \\nequals \\n\\u200b\\n0\\n\\u200b\\n. The loss value raises with the confidence level, approaching 0. You might also wonder\\n \\nwhy we did not print the result of \\n\\u200b\\nlog(0)\\n\\u200b\\n — we’ll explain that shortly.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 110}),\n",
       " Document(page_content='So far, we’ve applied log() to the softmax output, but have neither explained what “log” is nor\\n \\nwhy we use it. We will save the discussion of “why” until the next chapter, which covers\\n \\nderivatives, gradients, and optimizations; suffice it to say that the log function has some desirable\\n \\nproperties. \\n\\u200b\\nLog\\n\\u200b\\n is short for \\n\\u200b\\nlogarithm \\n\\u200b\\nand is defined as the solution for the x-term in an equation\\n \\nof the form a\\n\\u200b\\nx \\n\\u200b\\n= b. For example, \\n\\u200b\\n10\\n\\u200b\\nx \\n\\u200b\\n= 100\\n\\u200b\\n can be solved with a log: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 110}),\n",
       " Document(page_content='\\u200b\\nlog\\n\\u200b\\n10\\n\\u200b\\n(100)\\n\\u200b\\n, which evaluates to\\n \\n2. This property of the log function is \\n\\u200b\\nespecially \\n\\u200b\\nbeneficial when \\n\\u200b\\ne \\n\\u200b\\n(Euler’s number or \\n\\u200b\\n~2.71828\\n\\u200b\\n)\\n \\nis used in the base (where 10 is in the example). The logarithm with \\n\\u200b\\ne \\n\\u200b\\nas its base is referred to as\\n \\nthe \\n\\u200b\\nnatural logarithm\\n\\u200b\\n, \\n\\u200b\\nnatural log\\n\\u200b\\n, or simply \\n\\u200b\\nlog\\n\\u200b\\n — you may also see this written as \\n\\u200b\\nln\\n\\u200b\\n: \\n\\u200b\\nln(x) =\\n \\nlog(x) = log\\n\\u200b\\ne\\n\\u200b\\n(x)\\n\\u200b\\n The variety of conventions can make this confusing, so to simplify things,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 110}),\n",
       " Document(page_content='\\u200b\\nany\\n \\nmention of log will always be a natural logarithm throughout this book\\n\\u200b\\n. The natural log\\n \\nrepresents the solution for the x-term in the equation \\n\\u200b\\ne\\n\\u200b\\nx \\n\\u200b\\n= b\\n\\u200b\\n; for example, \\n\\u200b\\ne\\n\\u200b\\nx \\n\\u200b\\n= 5.2\\n\\u200b\\n is solved by\\n \\nlog(5.2)\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 110}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n11\\n \\nIn Python code:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\nb \\n\\u200b\\n= \\n\\u200b\\n5.2\\n \\nprint\\n\\u200b\\n(np.log(b))\\n \\n \\n \\n>>>\\n \\n1.6486586255873816\\n \\n \\nWe can confirm this by exponentiating our result:\\n \\nimport \\n\\u200b\\nmath\\n \\n \\nprint\\n\\u200b\\n(math.e \\n\\u200b\\n** \\n\\u200b\\n1.6486586255873816\\n\\u200b\\n)\\n \\n \\n \\n>>>\\n \\n5.199999999999999\\n \\n \\nThe small difference is the result of floating-point precision in Python. Getting back to the loss', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 111}),\n",
       " Document(page_content='calculation, we need to modify our output in two additional ways. First, we’ll update our process\\n \\nto work on batches of softmax output distributions; and second, make the negative log calculation\\n \\ndynamic to the target index (the target index has been hard-coded so far).\\n \\nConsider a scenario with a neural network that performs classification between three classes, and\\n \\nthe neural network classifies in batches of three. After running through the softmax activation', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 111}),\n",
       " Document(page_content='function with a batch of 3 samples and 3 classes, the network’s output layer yields:\\n \\n# Probabilities for 3 samples\\n \\nsoftmax_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.9\\n\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]])\\n \\n \\nWe need a way to dynamically calculate the categorical cross-entropy, which we now know is a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 111}),\n",
       " Document(page_content='negative log calculation. To determine which value in the softmax output to calculate the negative\\n \\nlog from, we simply need to know our target values. In this example, there are 3 classes; let’s say\\n \\nwe’re trying to classify something as a “dog,” “cat,” or “human.” A dog is class 0 (at index 0), a\\n \\ncat class 1 (index 1), and a human class 2 (index 2). Let’s assume the batch of three sample inputs', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 111}),\n",
       " Document(page_content='to this neural network is being mapped to the target values of a dog, cat, and cat. So the targets (as', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 111}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n12\\n \\na list of target indices) would be \\n\\u200b\\n[0, 1, 1]\\n\\u200b\\n.\\n \\nsoftmax_outputs \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],\\n \\n                   [\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n],\\n \\n                   [\\n\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.9\\n\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]]\\n \\n \\nclass_targets \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n  # dog, cat, cat\\n \\n \\nThe first value, 0, in \\n\\u200b\\nclass_targets\\n\\u200b\\n means the first softmax output distribution’s intended', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 112}),\n",
       " Document(page_content='prediction was the one at the 0th index of \\n\\u200b\\n[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n]\\n\\u200b\\n; the model has a \\n\\u200b\\n0.7\\n\\u200b\\n confidence\\n \\nscore that this observation is a dog. This continues throughout the batch, where the intended target\\n \\nof the 2nd softmax distribution, \\n\\u200b\\n[\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n]\\n\\u200b\\n, was at an index of 1; the model only has a\\n \\n0.5\\n\\u200b\\n confidence score that this is a cat — the model is less certain about this observation. In the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 112}),\n",
       " Document(page_content='last sample, it’s also the 2nd index from the softmax distribution, a value of \\n\\u200b\\n0.9\\n\\u200b\\n in this case — a\\n \\npretty high confidence.\\n \\nWith a collection of softmax outputs and their intended targets, we can map these indices to\\n \\nretrieve the values from the softmax distributions:\\n \\nsoftmax_outputs \\n\\u200b\\n= \\n\\u200b\\n[[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],\\n \\n                   [\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n],\\n \\n                   [\\n\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.9\\n\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]]\\n \\n \\nclass_targets \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 112}),\n",
       " Document(page_content='= \\n\\u200b\\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n]\\n \\n \\nfor \\n\\u200b\\ntarg_idx, distribution \\n\\u200b\\nin \\n\\u200b\\nzip\\n\\u200b\\n(class_targets, softmax_outputs):\\n \\n    \\n\\u200b\\nprint\\n\\u200b\\n(distribution[targ_idx])\\n \\n \\n \\n>>>\\n \\n0.7\\n \\n0.5\\n \\n0.9\\n \\n \\nThe \\n\\u200b\\nzip\\n\\u200b\\n()\\n\\u200b\\n function, again, lets us iterate over multiple iterables at the same time in Python. This\\n \\ncan be further simplified using NumPy (we’re creating a NumPy array of the Softmax outputs this\\n \\ntime):\\n \\nsoftmax_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 112}),\n",
       " Document(page_content='\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.9\\n\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]])\\n \\nclass_targets \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 112}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n13\\n \\nprint\\n\\u200b\\n(softmax_outputs[[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n], class_targets])\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0.7 0.5 0.9\\n\\u200b\\n]\\n \\n \\nWhat are the 0, 1, and 2 values? NumPy lets us index an array in multiple ways. One of them is to\\n \\nuse a list filled with indices and that’s convenient for us — we could use the \\n\\u200b\\nclass_targets\\n\\u200b\\n for', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 113}),\n",
       " Document(page_content='\\u200b\\n for\\n \\nthis purpose as it already contains the list of indices that we are interested in. The problem is that\\n \\nthis has to filter data rows in the array — the second dimension. To perform that, we also need to\\n \\nexplicitly filter this array in its first dimension. This dimension contains the predictions and we, of\\n \\ncourse, want to retain them all. We can achieve that by using a list containing numbers from 0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 113}),\n",
       " Document(page_content='through all of the indices. We know we’re going to have as many indices as distributions in our\\n \\nentire batch, so we can use a \\n\\u200b\\nrange\\n\\u200b\\n()\\n\\u200b\\n instead of typing each value ourselves:\\n \\nprint\\n\\u200b\\n(softmax_outputs[\\n \\n    range\\n\\u200b\\n(\\n\\u200b\\nlen\\n\\u200b\\n(softmax_outputs)), class_targets\\n \\n])\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0.7 0.5 0.9\\n\\u200b\\n]\\n \\n \\nThis returns a list of the confidences at the target indices for each of the samples. Now we apply\\n \\nthe negative log to this list:\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n-\\n\\u200b\\nnp.log(softmax_outputs[\\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 113}),\n",
       " Document(page_content='\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\nlen\\n\\u200b\\n(softmax_outputs)), class_targets\\n \\n]))\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0.35667494 0.69314718 0.10536052\\n\\u200b\\n]\\n \\n \\nFinally, we want an average loss per batch to have an idea about how our model is doing during\\n \\ntraining. There are many ways to calculate an average in Python; the most basic form of an\\n \\naverage is the \\n\\u200b\\narithmetic mean\\n\\u200b\\n: \\n\\u200b\\nsum(iterable) / len(iterable)\\n\\u200b\\n. NumPy has a method that', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 113}),\n",
       " Document(page_content='computes this average on arrays, so we will use that instead. We add NumPy’s average to the\\n \\ncode:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 113}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n14\\n \\nneg_log \\n\\u200b\\n= -\\n\\u200b\\nnp.log(softmax_outputs[\\n \\n              \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\nlen\\n\\u200b\\n(softmax_outputs)), class_targets\\n \\n          ])\\n \\naverage_loss \\n\\u200b\\n= \\n\\u200b\\nnp.mean(neg_log)\\n \\nprint\\n\\u200b\\n(average_loss)\\n \\n \\n \\n>>>\\n \\n0.38506088005216804\\n \\n \\nWe have already learned that targets can be one-hot encoded, where all values, except for one, are', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 114}),\n",
       " Document(page_content='zeros, and the correct label’s position is filled with 1. They can also be sparse, which means that\\n \\nthe numbers they contain are the correct class numbers — we are generating them this way with\\n \\nthe \\n\\u200b\\nspiral_data\\n\\u200b\\n()\\n\\u200b\\n function, and we can allow the loss calculation to accept any of these forms.\\n \\nSince we implemented this to work with sparse labels (as in our training data), we have to add a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 114}),\n",
       " Document(page_content='check if they are one-hot encoded and handle it a bit differently in this new case. The check can\\n \\nbe performed by counting the dimensions — if targets are single-dimensional (like a list), they are\\n \\nsparse, but if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded\\n \\nvectors. In this second case, we’ll implement a solution using the first equation from this chapter,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 114}),\n",
       " Document(page_content='instead of filtering out the confidences at the target labels. We have to multiply confidences by\\n \\nthe targets, zeroing out all values except the ones at correct labels, performing a sum along the\\n \\nrow axis (axis \\n\\u200b\\n1\\n\\u200b\\n). We have to add a test to the code we just wrote for the number of dimensions,\\n \\nmove calculations of the log values outside of this new \\n\\u200b\\nif\\n\\u200b\\n statement, and implement the solution\\n \\nfor the one-hot encoded labels following the first equation:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 114}),\n",
       " Document(page_content='\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\nsoftmax_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.9\\n\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]])\\n \\nclass_targets \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n],\\n \\n                          [\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n],\\n \\n                          [\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n]])\\n \\n \\n \\n# Probabilities for target values -\\n \\n# only if categorical labels\\n \\nif \\n\\u200b\\nlen\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 114}),\n",
       " Document(page_content='if \\n\\u200b\\nlen\\n\\u200b\\n(class_targets.shape) \\n\\u200b\\n== \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n    correct_confidences \\n\\u200b\\n= \\n\\u200b\\nsoftmax_outputs[\\n \\n        \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\nlen\\n\\u200b\\n(softmax_outputs)),\\n \\n        class_targets\\n \\n    ]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 114}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n15\\n \\n# Mask values - only for one-hot encoded labels\\n \\nelif \\n\\u200b\\nlen\\n\\u200b\\n(class_targets.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n    correct_confidences \\n\\u200b\\n= \\n\\u200b\\nnp.sum(\\n \\n        softmax_outputs \\n\\u200b\\n* \\n\\u200b\\nclass_targets,\\n \\n        \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n \\n    )\\n \\n \\n# Losses\\n \\nneg_log \\n\\u200b\\n= -\\n\\u200b\\nnp.log(correct_confidences)\\n \\n \\n \\naverage_loss \\n\\u200b\\n= \\n\\u200b\\nnp.mean(neg_log)\\n \\nprint\\n\\u200b\\n(average_loss)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 115}),\n",
       " Document(page_content='(average_loss)\\n \\n \\nBefore we move on, there is one additional problem to solve. The softmax output, which is also\\n \\nan input to this loss function, consists of numbers in the range from 0 to 1 - a list of confidences.\\n \\nIt is possible that the model will have full confidence for one label making all the remaining\\n \\nconfidences zero. Similarly, it is also possible that the model will assign full confidence to a value', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 115}),\n",
       " Document(page_content='that wasn’t the target. If we then try to calculate the loss of this confidence of 0:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n-\\n\\u200b\\nnp.log(\\n\\u200b\\n0\\n\\u200b\\n)\\n \\n \\n \\n>>>\\n \\n__main__:1: \\n\\u200b\\nRuntimeWarning\\n\\u200b\\n: divide by zero encountered in log\\n \\ninf\\n \\n \\nBefore we explain this, we need to talk about \\n\\u200b\\nlog(0)\\n\\u200b\\n. From the mathematical point of view, \\n\\u200b\\nlog(0)\\n \\nis undefined. We already know the following dependence: if \\n\\u200b\\ny=log(x)\\n\\u200b\\n, then \\n\\u200b\\ne\\n\\u200b\\ny\\n\\u200b\\n=x\\n\\u200b\\n. The question of\\n \\nwhat the resulting \\n\\u200b\\ny\\n\\u200b\\n is in \\n\\u200b\\ny=log(0)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 115}),\n",
       " Document(page_content='is in \\n\\u200b\\ny=log(0)\\n\\u200b\\n is the same as the question of what’s the \\n\\u200b\\ny\\n\\u200b\\n in \\n\\u200b\\ne\\n\\u200b\\ny\\n\\u200b\\n=0\\n\\u200b\\n. In\\n \\nsimplified terms, the constant \\n\\u200b\\ne\\n\\u200b\\n to any power is always a positive number, and there is no \\n\\u200b\\ny\\n \\nresulting in \\n\\u200b\\ne\\n\\u200b\\ny\\n\\u200b\\n=0\\n\\u200b\\n. This means the \\n\\u200b\\nlog(0)\\n\\u200b\\n is undefined. We need to be aware of what the \\n\\u200b\\nlog(0)\\n\\u200b\\n is,\\n \\nand “undefined” does not mean that we don’t know anything about it. Since \\n\\u200b\\nlog(0)\\n\\u200b\\n is undefined,\\n \\nwhat’s the result for a value very close to \\n\\u200b\\n0\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 115}),\n",
       " Document(page_content='\\u200b\\n0\\n\\u200b\\n? We can calculate the limit of a function. How to\\n \\nexactly calculate it exceeds this book, but the solution is:\\n \\n \\n \\nWe read it as the limit of a natural logarithm of \\n\\u200b\\nx\\n\\u200b\\n, with x approaching \\n\\u200b\\n0\\n\\u200b\\n from a positive (it is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 115}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n16\\n \\nimpossible to calculate the natural logarithm of a negative value) equals negative infinity. What\\n \\nthis means is that the limit is negative infinity for an infinitely small \\n\\u200b\\nx\\n\\u200b\\n, where \\n\\u200b\\nx\\n\\u200b\\n never reaches \\n\\u200b\\n0\\n\\u200b\\n.\\n \\nThe situation is a bit different in programming languages. We do not have limits here, just a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 116}),\n",
       " Document(page_content='function which, given a parameter, returns some value. The negative natural logarithm of \\n\\u200b\\n0\\n\\u200b\\n, in\\n \\nPython with NumPy, equals an infinitely big number, rather than undefined, and prints a warning\\n \\nabout a division by \\n\\u200b\\n0\\n\\u200b\\n (which is a result of how this calculation is done). If -\\n\\u200b\\nnp.log(\\n\\u200b\\n0\\n\\u200b\\n)\\n\\u200b\\n equals\\n \\ninf\\n\\u200b\\n, is it possible to calculate e to the power of negative infinity with Python?\\n \\nnp.e\\n\\u200b\\n**\\n\\u200b\\n(\\n\\u200b\\n-\\n\\u200b\\nnp.inf)\\n \\n \\n>>>\\n \\n0.0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 116}),\n",
       " Document(page_content='>>>\\n \\n0.0\\n \\n \\nIn programming, the fewer things that are undefined, the better. Later on, we’ll see similar\\n \\nsimplifications, for example when calculating a derivative of the absolute value function, which\\n \\ndoes not exist for an input of \\n\\u200b\\n0\\n\\u200b\\n and we’ll have to make some decisions to work around this.\\n \\nBack to the result of \\n\\u200b\\ninf\\n\\u200b\\n for \\n\\u200b\\n-\\n\\u200b\\nnp.log(\\n\\u200b\\n0\\n\\u200b\\n)\\n\\u200b\\n — as much as that makes sense, since the model', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 116}),\n",
       " Document(page_content='would be fully wrong, this will be a problem for us to do further calculations with. Later, with\\n \\noptimization, we will also have a problem calculating gradients, starting with a mean value of all\\n \\nsample-wise losses since a single infinite value in a list will cause the average of that list to also\\n \\nbe infinite:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nnp.mean([\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\nnp.log(\\n\\u200b\\n0\\n\\u200b\\n)])\\n \\n \\n \\n>>>\\n \\n__main__:\\n\\u200b\\n1\\n\\u200b\\n: \\n\\u200b\\nRuntimeWarning\\n\\u200b\\n: divide by zero encountered in log', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 116}),\n",
       " Document(page_content='inf\\n \\n \\nWe could add a very small value to the confidence to prevent it from being a zero, for example,\\n \\n1e-7\\n\\u200b\\n:\\n \\n-\\n\\u200b\\nnp.log(\\n\\u200b\\n1e-7\\n\\u200b\\n)\\n \\n \\n \\n>>>\\n \\n16.11809565095832', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 116}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n17\\n \\nAdding a very small value, one-tenth of a million, to the confidence at its far edge will\\n \\ninsignificantly impact the result, but this method yields an additional 2 issues. First, in the\\n \\ncase where the confidence value is \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n-\\n\\u200b\\nnp.log(\\n\\u200b\\n1\\n\\u200b\\n+\\n\\u200b\\n1e-7\\n\\u200b\\n)\\n \\n \\n \\n>>>\\n \\n-\\n\\u200b\\n9.999999505838704e-08', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 117}),\n",
       " Document(page_content='When the model is fully correct in a prediction and puts all the confidence in the correct label,\\n \\nloss becomes a negative value instead of being 0. The other problem here is shifting confidence\\n \\ntowards \\n\\u200b\\n1\\n\\u200b\\n, even if by a very small value. To prevent both issues, it’s better to clip values from\\n \\nboth sides by the same number, \\n\\u200b\\n1e-7\\n\\u200b\\n in our case. That means that the lowest possible value will\\n \\nbecome \\n\\u200b\\n1e-7\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 117}),\n",
       " Document(page_content='become \\n\\u200b\\n1e-7\\n\\u200b\\n (like in the demonstration we just performed) but the highest possible value, instead\\n \\nof being \\n\\u200b\\n1+1e-7\\n\\u200b\\n, will become \\n\\u200b\\n1-1e-7\\n\\u200b\\n (so slightly less than \\n\\u200b\\n1\\n\\u200b\\n):\\n \\n-\\n\\u200b\\nnp.log(\\n\\u200b\\n1\\n\\u200b\\n-\\n\\u200b\\n1e-7\\n\\u200b\\n)\\n \\n \\n>>>\\n \\n1.0000000494736474e-07\\n \\n \\nThis will prevent loss from being exactly \\n\\u200b\\n0\\n\\u200b\\n, making it a very small value instead, but won’t make\\n \\nit a negative value and won’t bias overall loss towards \\n\\u200b\\n1\\n\\u200b\\n. Within our code and using numpy, we’ll\\n \\naccomplish that using \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 117}),\n",
       " Document(page_content='\\u200b\\nnp.clip()\\n\\u200b\\n method:\\n \\ny_pred_clipped \\n\\u200b\\n= \\n\\u200b\\nnp.clip(y_pred, \\n\\u200b\\n1e-7\\n\\u200b\\n, \\n\\u200b\\n1 \\n\\u200b\\n- \\n\\u200b\\n1e-7\\n\\u200b\\n)\\n \\n \\nThis method can perform clipping on an array of values, so we can apply it to the predictions\\n \\ndirectly and save this as a separate array, which we’ll use shortly.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 117}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n18\\n \\n \\nThe Categorical Cross-Entropy Loss Class\\n \\nIn the later chapters, we’ll be adding more loss functions and some of the operations that we’ll be\\n \\nperforming are common for all of them. One of these operations is how we calculate the overall\\n \\nloss — no matter which loss function we’ll use, the overall loss is always a mean value of all\\n \\nsample losses. Let’s create the \\n\\u200b\\nLoss\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 118}),\n",
       " Document(page_content='\\u200b\\nLoss\\n\\u200b\\n class containing the \\n\\u200b\\ncalculate\\n\\u200b\\n method that will call our\\n \\nloss object’s forward method and calculate the mean value of the returned sample losses:\\n \\n# Common loss class\\n \\nclass \\n\\u200b\\nLoss\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Calculates the data and regularization losses\\n \\n    # given model output and ground truth values\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\ncalculate\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\noutput\\n\\u200b\\n, \\n\\u200b\\ny\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Calculate sample losses\\n \\n        \\n\\u200b\\nsample_losses \\n\\u200b\\n= \\n\\u200b\\nself.forward(output, y)\\n \\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 118}),\n",
       " Document(page_content='\\u200b\\n# Calculate mean loss\\n \\n        \\n\\u200b\\ndata_loss \\n\\u200b\\n= \\n\\u200b\\nnp.mean(sample_losses)\\n \\n \\n        \\n\\u200b\\n# Return loss\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\ndata_loss\\n \\n \\nIn later chapters, we’ll add more code to this class, and the reason for it to exist will become more\\n \\nclear. For now, we’ll use it for this single purpose.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 118}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n19\\n \\nLet’s convert our loss code into a class for convenience down the line:\\n \\n# Cross-entropy loss\\n \\nclass \\n\\u200b\\nLoss_CategoricalCrossentropy\\n\\u200b\\n(\\n\\u200b\\nLoss\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ny_pred\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples in a batch\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(y_pred)\\n \\n \\n        \\n\\u200b\\n# Clip data to prevent division by 0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 119}),\n",
       " Document(page_content='# Clip both sides to not drag mean towards any value\\n \\n        \\n\\u200b\\ny_pred_clipped \\n\\u200b\\n= \\n\\u200b\\nnp.clip(y_pred, \\n\\u200b\\n1e-7\\n\\u200b\\n, \\n\\u200b\\n1 \\n\\u200b\\n- \\n\\u200b\\n1e-7\\n\\u200b\\n)\\n \\n \\n        \\n\\u200b\\n# Probabilities for target values -\\n \\n        # only if categorical labels\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n            correct_confidences \\n\\u200b\\n= \\n\\u200b\\ny_pred_clipped[\\n \\n                \\n\\u200b\\nrange\\n\\u200b\\n(samples),\\n \\n                y_true\\n \\n            ]\\n \\n \\n        \\n\\u200b\\n# Mask values - only for one-hot encoded labels', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 119}),\n",
       " Document(page_content='\\u200b\\nelif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n            correct_confidences \\n\\u200b\\n= \\n\\u200b\\nnp.sum(\\n \\n                y_pred_clipped \\n\\u200b\\n* \\n\\u200b\\ny_true,\\n \\n                \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n \\n            \\n\\u200b\\n)\\n \\n \\n        \\n\\u200b\\n# Losses\\n \\n        \\n\\u200b\\nnegative_log_likelihoods \\n\\u200b\\n= -\\n\\u200b\\nnp.log(correct_confidences)\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\nnegative_log_likelihoods\\n \\n \\nThis class inherits the \\n\\u200b\\nLoss\\n\\u200b\\n class and performs all the error calculations that we derived', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 119}),\n",
       " Document(page_content='throughout this chapter and can be used as an object. For example, using the manually-created\\n \\noutput and targets:\\n \\nloss_function \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()\\n \\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_function.calculate(softmax_outputs, class_targets)\\n \\nprint\\n\\u200b\\n(loss)\\n \\n \\n \\n>>>\\n \\n0.38506088005216804', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 119}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n20\\n \\n \\nCombining everything up to this point:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nimport \\n\\u200b\\nnnfs\\n \\nfrom \\n\\u200b\\nnnfs.datasets \\n\\u200b\\nimport \\n\\u200b\\nspiral_data\\n \\n \\nnnfs.init()\\n \\n \\n \\n# Dense layer\\n \\nclass \\n\\u200b\\nLayer_Dense\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Layer initialization\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nn_inputs\\n\\u200b\\n, \\n\\u200b\\nn_neurons\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Initialize weights and biases\\n \\n        \\n\\u200b\\nself.weights \\n\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 120}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(n_inputs, n_neurons)\\n \\n        self.biases \\n\\u200b\\n= \\n\\u200b\\nnp.zeros((\\n\\u200b\\n1\\n\\u200b\\n, n_neurons))\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Calculate output values from inputs, weights and biases\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, self.weights) \\n\\u200b\\n+ \\n\\u200b\\nself.biases\\n \\n \\n \\n# ReLU activation\\n \\nclass \\n\\u200b\\nActivation_ReLU\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 120}),\n",
       " Document(page_content='\\u200b\\n):\\n \\n        \\n\\u200b\\n# Calculate output values from inputs\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nnp.maximum(\\n\\u200b\\n0\\n\\u200b\\n, inputs)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 120}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n21\\n \\n# Softmax activation\\n \\nclass \\n\\u200b\\nActivation_Softmax\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Get unnormalized probabilities\\n \\n        \\n\\u200b\\nexp_values \\n\\u200b\\n= \\n\\u200b\\nnp.exp(inputs \\n\\u200b\\n- \\n\\u200b\\nnp.max(inputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n))\\n \\n        \\n\\u200b\\n# Normalize them for each sample\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 121}),\n",
       " Document(page_content='\\u200b\\nprobabilities \\n\\u200b\\n= \\n\\u200b\\nexp_values \\n\\u200b\\n/ \\n\\u200b\\nnp.sum(exp_values, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nprobabilities\\n \\n \\n \\n# Common loss class\\n \\nclass \\n\\u200b\\nLoss\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Calculates the data and regularization losses\\n \\n    # given model output and ground truth values\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\ncalculate\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\noutput\\n\\u200b\\n, \\n\\u200b\\ny\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Calculate sample losses\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 121}),\n",
       " Document(page_content='\\u200b\\nsample_losses \\n\\u200b\\n= \\n\\u200b\\nself.forward(output, y)\\n \\n \\n        \\n\\u200b\\n# Calculate mean loss\\n \\n        \\n\\u200b\\ndata_loss \\n\\u200b\\n= \\n\\u200b\\nnp.mean(sample_losses)\\n \\n \\n        \\n\\u200b\\n# Return loss\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\ndata_loss\\n \\n \\n \\n# Cross-entropy loss\\n \\nclass \\n\\u200b\\nLoss_CategoricalCrossentropy\\n\\u200b\\n(\\n\\u200b\\nLoss\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ny_pred\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples in a batch\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(y_pred)\\n \\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 121}),\n",
       " Document(page_content='\\u200b\\n# Clip data to prevent division by 0\\n \\n        # Clip both sides to not drag mean towards any value\\n \\n        \\n\\u200b\\ny_pred_clipped \\n\\u200b\\n= \\n\\u200b\\nnp.clip(y_pred, \\n\\u200b\\n1e-7\\n\\u200b\\n, \\n\\u200b\\n1 \\n\\u200b\\n- \\n\\u200b\\n1e-7\\n\\u200b\\n)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 121}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n22\\n \\n        # Probabilities for target values -\\n \\n        # only if categorical labels\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n            correct_confidences \\n\\u200b\\n= \\n\\u200b\\ny_pred_clipped[\\n \\n                \\n\\u200b\\nrange\\n\\u200b\\n(samples),\\n \\n                y_true\\n \\n            ]\\n \\n \\n        \\n\\u200b\\n# Mask values - only for one-hot encoded labels\\n \\n        \\n\\u200b\\nelif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 122}),\n",
       " Document(page_content='\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n            correct_confidences \\n\\u200b\\n= \\n\\u200b\\nnp.sum(\\n \\n                y_pred_clipped \\n\\u200b\\n* \\n\\u200b\\ny_true,\\n \\n                \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n \\n            \\n\\u200b\\n)\\n \\n \\n        \\n\\u200b\\n# Losses\\n \\n        \\n\\u200b\\nnegative_log_likelihoods \\n\\u200b\\n= -\\n\\u200b\\nnp.log(correct_confidences)\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\nnegative_log_likelihoods\\n \\n \\n \\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 3 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 122}),\n",
       " Document(page_content='dense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 3 input features (as we take output\\n \\n# of previous layer here) and 3 output values\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax activation (to be used with Dense layer):\\n \\nactivation2 \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n \\n# Create loss function\\n \\nloss_function \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 122}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()\\n \\n \\n# Perform a forward pass of our training data through this layer\\n \\ndense1.forward(X)\\n \\n \\n# Perform a forward pass through activation function\\n \\n# it takes the output of first dense layer here\\n \\nactivation1.forward(dense1.output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 122}),\n",
       " Document(page_content=\"Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n23\\n \\n# Perform a forward pass through second Dense layer\\n \\n# it takes outputs of activation function of first layer as inputs\\n \\ndense2.forward(activation1.output)\\n \\n \\n# Perform a forward pass through activation function\\n \\n# it takes the output of second dense layer here\\n \\nactivation2.forward(dense2.output)\\n \\n \\n# Let's see output of the first few samples:\\n \\nprint\\n\\u200b\\n(activation2.output[:\\n\\u200b\\n5\\n\\u200b\\n])\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 123}),\n",
       " Document(page_content=\"\\u200b\\n5\\n\\u200b\\n])\\n \\n \\n# Perform a forward pass through loss function\\n \\n# it takes the output of second dense layer here and returns loss\\n \\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_function.calculate(activation2.output, y)\\n \\n \\n# Print loss value\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'loss:'\\n\\u200b\\n, loss)\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n0.33333334 0.33333334 0.33333334\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333316 0.3333332  0.33333364\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333287 0.3333329  0.33333418\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.3333326  0.33333263 0.33333477\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333233 0.3333324  0.33333528\\n\\u200b\\n]]\\n \\nloss: \\n\\u200b\\n1.0986104\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 123}),\n",
       " Document(page_content='\\u200b\\n1.0986104\\n \\n \\nAgain, we get \\n\\u200b\\n~0.33\\n\\u200b\\n values since the model is random, and its average loss is also not great for\\n \\nthese data, as we’ve not yet trained our model on how to correct its errors.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 123}),\n",
       " Document(page_content='Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n \\n24\\n \\n \\nAccuracy Calculation\\n \\nWhile loss is a useful metric for optimizing a model, the metric commonly used in practice along\\n \\nwith loss is the \\n\\u200b\\naccuracy\\n\\u200b\\n, which describes how often the largest confidence is the correct class\\n \\nin terms of a fraction. Conveniently, we can reuse existing variable definitions to calculate the\\n \\naccuracy metric. We will use the \\n\\u200b\\nargmax \\n\\u200b\\nvalues from the \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 124}),\n",
       " Document(page_content='values from the \\n\\u200b\\nsoftmax outputs \\n\\u200b\\nand then compare\\n \\nthese to the targets. This is as simple as doing (note that we slightly modified the\\n \\nsoftmax_outputs\\n\\u200b\\n for the purpose of this example):\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Probabilities of 3 samples\\n \\nsoftmax_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.9\\n\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]])', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 124}),\n",
       " Document(page_content='\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]])\\n \\n# Target (ground-truth) labels for 3 samples\\n \\nclass_targets \\n\\u200b\\n= \\n\\u200b\\nnp.array([\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n])\\n \\n \\n# Calculate values along second axis (axis of index 1)\\n \\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(softmax_outputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n# If targets are one-hot encoded - convert them\\n \\nif \\n\\u200b\\nlen\\n\\u200b\\n(class_targets.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n    class_targets \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(class_targets, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n# True evaluates to 1; False to 0\\n \\naccuracy \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 124}),\n",
       " Document(page_content=\"accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\nclass_targets)\\n \\n \\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'acc:'\\n\\u200b\\n, accuracy)\\n \\n \\n \\n>>>\\n \\nacc: \\n\\u200b\\n0.6666666666666666\\n \\n \\nWe are also handling one-hot encoded targets by converting them to sparse values using\\n \\nnp.argmax()\\n\\u200b\\n.\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 124}),\n",
       " Document(page_content=\"Chapter 5 - Calculating Network Error with Loss - Neural Networks from Scratch in Python\\n25 \\nWe can add the following to the end of our full script above to calculate its accuracy:  \\n# Calculate accuracy from output of activation2 and targets  \\n# calculate values along first axis  \\npredictions \\u200b= \\u200bnp.argmax(activation2.output, \\u200baxis \\u200b=\\u200b1\\u200b) \\nif \\u200blen\\u200b(y.shape) \\u200b== \\u200b2\\u200b:  \\n   y \\u200b= \\u200bnp.argmax(y, \\u200baxis \\u200b=\\u200b1\\u200b) \\naccuracy \\u200b= \\u200bnp.mean(predictions\\u200b==\\u200by)  \\n# Print accuracy  \\nprint\\u200b(\\u200b'acc:'\\u200b, accuracy)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 125}),\n",
       " Document(page_content='>>> \\nacc: \\u200b0.34  \\nNow that you’ve learned how to perform a forward pass through our network and calculate the  \\nmetrics to signal if the model is performing poorly, we will embark on optimization in the next  \\nchapter!  \\nSupplementary Material: \\u200bhttps://nnfs.io/ch5  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 125}),\n",
       " Document(page_content='Chapter 6 - Introducing Optimization - Neural Networks from Scratch in Python\\n \\n \\n6\\n \\n \\n \\n \\n \\nChapter 6\\n \\nIntroducing Optimization\\n \\nNow that the neural network is built, able to have data passed through it, and capable of\\n \\ncalculating loss, the next step is to determine how to adjust the weights and biases to decrease the\\n \\nloss. Finding an intelligent way to adjust the neurons’ input’s weights and biases to minimize loss\\n \\nis the main difficulty of neural networks.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 126}),\n",
       " Document(page_content='The first option one might think of is randomly changing the weights, checking the loss, and\\n \\nrepeating this until happy with the lowest loss found. To see this in action, we’ll use a simpler\\n \\ndataset than we’ve been working with so far:\\n \\nimport \\n\\u200b\\nmatplotlib.pyplot \\n\\u200b\\nas \\n\\u200b\\nplt\\n \\n \\nimport \\n\\u200b\\nnnfs\\n \\nfrom \\n\\u200b\\nnnfs.datasets \\n\\u200b\\nimport \\n\\u200b\\nvertical_data\\n \\n \\nnnfs.init()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 126}),\n",
       " Document(page_content=\"Chapter 6 - Introducing Optimization - Neural Networks from Scratch in Python\\n \\n \\n7\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nvertical_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\nplt.scatter(X[:, \\n\\u200b\\n0\\n\\u200b\\n], X[:, \\n\\u200b\\n1\\n\\u200b\\n], \\n\\u200b\\nc\\n\\u200b\\n=\\n\\u200b\\ny, \\n\\u200b\\ns\\n\\u200b\\n=\\n\\u200b\\n40\\n\\u200b\\n, \\n\\u200b\\ncmap\\n\\u200b\\n=\\n\\u200b\\n'brg'\\n\\u200b\\n)\\n \\nplt.show()\\n \\n \\nWhich looks like:\\n \\n \\nFig 6.01:\\n\\u200b\\n “Vertical data” graphed.\\n \\nUsing the previously created code up to this point, we can use this new dataset with a simple\\n \\nneural network:\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 127}),\n",
       " Document(page_content='X, y \\n\\u200b\\n= \\n\\u200b\\nvertical_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create model\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)  \\n\\u200b\\n# first dense layer, 2 inputs\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)  \\n\\u200b\\n# second dense layer, 3 inputs, 3 outputs\\n \\nactivation2 \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n \\n# Create loss function\\n \\nloss_function \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 127}),\n",
       " Document(page_content='Chapter 6 - Introducing Optimization - Neural Networks from Scratch in Python\\n \\n \\n8\\n \\nThen create some variables to track the best loss and the associated weights and biases:\\n \\n# Helper variables\\n \\nlowest_loss \\n\\u200b\\n= \\n\\u200b\\n9999999  \\n\\u200b\\n# some initial value\\n \\nbest_dense1_weights \\n\\u200b\\n= \\n\\u200b\\ndense1.weights.copy()\\n \\nbest_dense1_biases \\n\\u200b\\n= \\n\\u200b\\ndense1.biases.copy()\\n \\nbest_dense2_weights \\n\\u200b\\n= \\n\\u200b\\ndense2.weights.copy()\\n \\nbest_dense2_biases \\n\\u200b\\n= \\n\\u200b\\ndense2.biases.copy()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 128}),\n",
       " Document(page_content='We initialized the loss to a large value and will decrease it when a new, lower, loss is found. We\\n \\nare also copying weights and biases (\\n\\u200b\\ncopy()\\n\\u200b\\n ensures a full copy instead of a reference to the\\n \\nobject). Now we iterate as many times as desired, pick random values for weights and biases, and\\n \\nsave the weights and biases if they generate the lowest-seen loss:\\n \\nfor \\n\\u200b\\niteration \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10000\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Generate a new set of weights for iteration\\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 128}),\n",
       " Document(page_content='\\u200b\\ndense1.weights \\n\\u200b\\n= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense1.biases \\n\\u200b\\n= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense2.weights \\n\\u200b\\n= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense2.biases \\n\\u200b\\n= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of the training data through this layer\\n \\n    \\n\\u200b\\ndense1.forward(X)\\n \\n    activation1.forward(dense1.output)\\n \\n    dense2.forward(activation1.output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 128}),\n",
       " Document(page_content='activation2.forward(dense2.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\\n \\n    # it takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_function.calculate(activation2.output, y)\\n \\n \\n    \\n\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis\\n \\n    \\n\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(activation2.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 128}),\n",
       " Document(page_content=\"\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n    \\n\\u200b\\n# If loss is smaller - print and save weights and biases aside\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nloss \\n\\u200b\\n< \\n\\u200b\\nlowest_loss:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\n'New set of weights found, iteration:'\\n\\u200b\\n, iteration,\\n \\n              \\n\\u200b\\n'loss:'\\n\\u200b\\n, loss, \\n\\u200b\\n'acc:'\\n\\u200b\\n, accuracy)\\n \\n        best_dense1_weights \\n\\u200b\\n= \\n\\u200b\\ndense1.weights.copy()\\n \\n        best_dense1_biases \\n\\u200b\\n= \\n\\u200b\\ndense1.biases.copy()\\n \\n        best_dense2_weights \\n\\u200b\\n= \\n\\u200b\\ndense2.weights.copy()\\n \\n        best_dense2_biases \\n\\u200b\\n= \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 128}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\ndense2.biases.copy()\\n \\n        lowest_loss \\n\\u200b\\n= \\n\\u200b\\nloss', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 128}),\n",
       " Document(page_content='Chapter 6 - Introducing Optimization - Neural Networks from Scratch in Python\\n \\n \\n9\\n \\n>>>\\n \\nNew set of weights found, iteration: \\n\\u200b\\n0 \\n\\u200b\\nloss: \\n\\u200b\\n1.0986564 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n3 \\n\\u200b\\nloss: \\n\\u200b\\n1.098138 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n117 \\n\\u200b\\nloss: \\n\\u200b\\n1.0980115 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n124 \\n\\u200b\\nloss: \\n\\u200b\\n1.0977516 \\n\\u200b\\nacc: \\n\\u200b\\n0.6\\n \\nNew set of weights found, iteration: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 129}),\n",
       " Document(page_content='\\u200b\\n165 \\n\\u200b\\nloss: \\n\\u200b\\n1.097571 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n552 \\n\\u200b\\nloss: \\n\\u200b\\n1.0974693 \\n\\u200b\\nacc: \\n\\u200b\\n0.34\\n \\nNew set of weights found, iteration: \\n\\u200b\\n778 \\n\\u200b\\nloss: \\n\\u200b\\n1.0968257 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n4307 \\n\\u200b\\nloss: \\n\\u200b\\n1.0965533 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n4615 \\n\\u200b\\nloss: \\n\\u200b\\n1.0964499 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n9450 \\n\\u200b\\nloss: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 129}),\n",
       " Document(page_content='\\u200b\\n9450 \\n\\u200b\\nloss: \\n\\u200b\\n1.0964295 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\n \\nLoss certainly falls, though not by much. Accuracy did not improve, except for a singular\\n \\nsituation where the model randomly found a set of weights yielding better accuracy. Still, with a\\n \\nfairly large loss, this state is not stable. Running an additional 90,000 iterations for 100,000 in\\n \\ntotal:\\n \\nNew set of weights found, iteration: \\n\\u200b\\n13361 \\n\\u200b\\nloss: \\n\\u200b\\n1.0963014 \\n\\u200b\\nacc:\\n \\n0.3333333333333333', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 129}),\n",
       " Document(page_content='New set of weights found, iteration: \\n\\u200b\\n14001 \\n\\u200b\\nloss: \\n\\u200b\\n1.0959858 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n24598 \\n\\u200b\\nloss: \\n\\u200b\\n1.0947444 \\n\\u200b\\nacc:\\n \\n0.3333333333333333\\n \\n \\nLoss continued to drop, but accuracy did not change. This doesn’t appear to be a reliable method\\n \\nfor minimizing loss. After running for 1 billion iterations, the following was the best (lowest loss)\\n \\nresult:\\n \\nNew set of weights found, iteration: \\n\\u200b\\n229865000 \\n\\u200b\\nloss: \\n\\u200b\\n1.0911305 \\n\\u200b\\nacc:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 129}),\n",
       " Document(page_content='1.0911305 \\n\\u200b\\nacc:\\n \\n0.3333333333333333', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 129}),\n",
       " Document(page_content='Chapter 6 - Introducing Optimization - Neural Networks from Scratch in Python\\n \\n \\n10\\n \\nEven with this basic dataset, we see that randomly searching for weight and bias combinations\\n \\nwill take far too long to be an acceptable method. Another idea might be, instead of setting\\n \\nparameters with randomly-chosen values each iteration, apply a fraction of these values to\\n \\nparameters. With this, weights will be updated from what currently yields us the lowest loss', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 130}),\n",
       " Document(page_content='instead of aimlessly randomly. If the adjustment decreases loss, we will make it the new point to\\n \\nadjust from. If loss instead increases due to the adjustment, then we will revert to the previous\\n \\npoint. Using similar code from earlier, we will first change from randomly selecting weights and\\n \\nbiases to randomly \\n\\u200b\\nadjusting\\n\\u200b\\n them:\\n \\n    # Update weights with some small random values\\n \\n    \\n\\u200b\\ndense1.weights \\n\\u200b\\n+= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense1.biases \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 130}),\n",
       " Document(page_content=\"\\u200b\\n+= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense2.weights \\n\\u200b\\n+= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense2.biases \\n\\u200b\\n+= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\nThen we will change our ending \\n\\u200b\\nif\\n\\u200b\\n \\n\\u200b\\nstatement\\n\\u200b\\n \\n\\u200b\\nto be:\\n \\n    # If loss is smaller - print and save weights and biases aside\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nloss \\n\\u200b\\n< \\n\\u200b\\nlowest_loss:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\n'New set of weights found, iteration:'\\n\\u200b\\n, iteration,\\n \\n              \\n\\u200b\\n'loss:'\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 130}),\n",
       " Document(page_content=\"\\u200b\\n'loss:'\\n\\u200b\\n, loss, \\n\\u200b\\n'acc:'\\n\\u200b\\n, accuracy)\\n \\n        best_dense1_weights \\n\\u200b\\n= \\n\\u200b\\ndense1.weights.copy()\\n \\n        best_dense1_biases \\n\\u200b\\n= \\n\\u200b\\ndense1.biases.copy()\\n \\n        best_dense2_weights \\n\\u200b\\n= \\n\\u200b\\ndense2.weights.copy()\\n \\n        best_dense2_biases \\n\\u200b\\n= \\n\\u200b\\ndense2.biases.copy()\\n \\n        lowest_loss \\n\\u200b\\n= \\n\\u200b\\nloss\\n \\n    \\n\\u200b\\n# Revert weights and biases\\n \\n    \\n\\u200b\\nelse\\n\\u200b\\n:\\n \\n        dense1.weights \\n\\u200b\\n= \\n\\u200b\\nbest_dense1_weights.copy()\\n \\n        dense1.biases \\n\\u200b\\n= \\n\\u200b\\nbest_dense1_biases.copy()\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 130}),\n",
       " Document(page_content='dense2.weights \\n\\u200b\\n= \\n\\u200b\\nbest_dense2_weights.copy()\\n \\n        dense2.biases \\n\\u200b\\n= \\n\\u200b\\nbest_dense2_biases.copy()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 130}),\n",
       " Document(page_content='Chapter 6 - Introducing Optimization - Neural Networks from Scratch in Python\\n \\n \\n11\\n \\n \\nFull code up to this point:\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nvertical_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create model\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)  \\n\\u200b\\n# first dense layer, 2 inputs\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)  \\n\\u200b\\n# second dense layer, 3 inputs, 3 outputs\\n \\nactivation2 \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 131}),\n",
       " Document(page_content='# Create loss function\\n \\nloss_function \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()\\n \\n \\n# Helper variables\\n \\nlowest_loss \\n\\u200b\\n= \\n\\u200b\\n9999999  \\n\\u200b\\n# some initial value\\n \\nbest_dense1_weights \\n\\u200b\\n= \\n\\u200b\\ndense1.weights\\n\\u200b\\n.copy()\\n \\nbest_dense1_biases \\n\\u200b\\n= \\n\\u200b\\ndense1.biases\\n\\u200b\\n.copy()\\n \\nbest_dense2_weights \\n\\u200b\\n= \\n\\u200b\\ndense2.weights\\n\\u200b\\n.copy()\\n \\nbest_dense2_biases \\n\\u200b\\n= \\n\\u200b\\ndense2.biases\\n\\u200b\\n.copy()\\n \\n \\nfor \\n\\u200b\\niteration \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10000\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Update weights with some small random values\\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 131}),\n",
       " Document(page_content='\\u200b\\ndense1.weights \\n\\u200b\\n+= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense1.biases \\n\\u200b\\n+= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense2.weights \\n\\u200b\\n+= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n    dense2.biases \\n\\u200b\\n+= \\n\\u200b\\n0.05 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of our training data through this layer\\n \\n    \\n\\u200b\\ndense1.forward(X)\\n \\n    activation1.forward(dense1.output)\\n \\n    dense2.forward(activation1.output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 131}),\n",
       " Document(page_content='activation2.forward(dense2.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\\n \\n    # it takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_function.calculate(activation2.output, y)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 131}),\n",
       " Document(page_content='Chapter 6 - Introducing Optimization - Neural Networks from Scratch in Python\\n \\n \\n12\\n \\n    \\n\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis\\n \\n    \\n\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(activation2.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n    \\n\\u200b\\n# If loss is smaller - print and save weights and biases aside\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nloss \\n\\u200b\\n< \\n\\u200b\\nlowest_loss:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 132}),\n",
       " Document(page_content=\"\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\n'New set of weights found, iteration:'\\n\\u200b\\n, iteration,\\n \\n              \\n\\u200b\\n'loss:'\\n\\u200b\\n, loss, \\n\\u200b\\n'acc:'\\n\\u200b\\n, accuracy)\\n \\n        best_dense1_weights \\n\\u200b\\n= \\n\\u200b\\ndense1.weights\\n\\u200b\\n.copy()\\n \\n        best_dense1_biases \\n\\u200b\\n= \\n\\u200b\\ndense1.biases\\n\\u200b\\n.copy()\\n \\n        best_dense2_weights \\n\\u200b\\n= \\n\\u200b\\ndense2.weights\\n\\u200b\\n.copy()\\n \\n        best_dense2_biases \\n\\u200b\\n= \\n\\u200b\\ndense2.biases\\n\\u200b\\n.copy()\\n \\n        lowest_loss \\n\\u200b\\n= \\n\\u200b\\nloss\\n \\n    \\n\\u200b\\n# Revert weights and biases\\n \\n    \\n\\u200b\\nelse\\n\\u200b\\n:\\n \\n        dense1.weights \\n\\u200b\\n= \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 132}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\nbest_dense1_weights\\n\\u200b\\n.copy()\\n \\n        dense1.biases \\n\\u200b\\n= \\n\\u200b\\nbest_dense1_biases\\n\\u200b\\n.copy()\\n \\n        dense2.weights \\n\\u200b\\n= \\n\\u200b\\nbest_dense2_weights\\n\\u200b\\n.copy()\\n \\n        dense2.biases \\n\\u200b\\n= \\n\\u200b\\nbest_dense2_biases\\n\\u200b\\n.copy()\\n \\n \\n>>>\\n \\nNew set of weights found, iteration: \\n\\u200b\\n0 \\n\\u200b\\nloss: \\n\\u200b\\n1.0987684 \\n\\u200b\\nacc:\\n \\n0.3333333333333333 ...\\n \\nNew set of weights found, iteration: \\n\\u200b\\n29 \\n\\u200b\\nloss: \\n\\u200b\\n1.0725244 \\n\\u200b\\nacc:\\n \\n0.5266666666666666\\n \\nNew set of weights found, iteration: \\n\\u200b\\n30 \\n\\u200b\\nloss: \\n\\u200b\\n1.0724432 \\n\\u200b\\nacc:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 132}),\n",
       " Document(page_content='\\u200b\\n1.0724432 \\n\\u200b\\nacc:\\n \\n0.3466666666666667 ...\\n \\nNew set of weights found, iteration: \\n\\u200b\\n48 \\n\\u200b\\nloss: \\n\\u200b\\n1.0303522 \\n\\u200b\\nacc:\\n \\n0.6666666666666666\\n \\nNew set of weights found, iteration: \\n\\u200b\\n49 \\n\\u200b\\nloss: \\n\\u200b\\n1.0292586 \\n\\u200b\\nacc:\\n \\n0.6666666666666666 ...\\n \\nNew set of weights found, iteration: \\n\\u200b\\n97 \\n\\u200b\\nloss: \\n\\u200b\\n0.9277446 \\n\\u200b\\nacc:\\n \\n0.7333333333333333 ...\\n \\nNew set of weights found, iteration: \\n\\u200b\\n152 \\n\\u200b\\nloss: \\n\\u200b\\n0.73390484 \\n\\u200b\\nacc:\\n \\n0.8433333333333334\\n \\nNew set of weights found, iteration: \\n\\u200b\\n156 \\n\\u200b\\nloss: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 132}),\n",
       " Document(page_content='\\u200b\\n156 \\n\\u200b\\nloss: \\n\\u200b\\n0.7235515 \\n\\u200b\\nacc: \\n\\u200b\\n0.87\\n \\nNew set of weights found, iteration: \\n\\u200b\\n160 \\n\\u200b\\nloss: \\n\\u200b\\n0.7049076 \\n\\u200b\\nacc:\\n \\n0.9066666666666666 ...\\n \\nNew set of weights found, iteration: \\n\\u200b\\n7446 \\n\\u200b\\nloss: \\n\\u200b\\n0.17280102 \\n\\u200b\\nacc:\\n \\n \\n0.9333333333333333\\n \\nNew set of weights found, iteration: \\n\\u200b\\n9397 \\n\\u200b\\nloss: \\n\\u200b\\n0.17279711 \\n\\u200b\\nacc: \\n\\u200b\\n0.93', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 132}),\n",
       " Document(page_content='Chapter 6 - Introducing Optimization - Neural Networks from Scratch in Python\\n13 \\nLoss descended by a decent amount this time, and accuracy raised significantly. Applying a  \\nfraction of random values actually lead to a result that we could almost call a solution. If you try  \\n100,000 iterations, you will not progress much further:  \\n>>> \\n...\\nNew set of weights found, iteration: \\u200b14206 \\u200bloss: \\u200b0.1727932 \\u200bacc:  \\n0.9333333333333333', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 133}),\n",
       " Document(page_content='0.9333333333333333\\nNew set of weights found, iteration: \\u200b63704 \\u200bloss: \\u200b0.17278232 \\u200bacc:  \\n0.9333333333333333\\nLet’s try this with the previously-seen spiral dataset instead:  \\n# Create dataset  \\nX, y \\u200b= \\u200bspiral_data(\\u200bsamples \\u200b=\\u200b100\\u200b, \\u200bclasses \\u200b=\\u200b3\\u200b) \\n>>>\\nNew set of weights found, iteration: \\u200b0 \\u200bloss: \\u200b1.1008677 \\u200bacc:  \\n0.3333333333333333 \\u200b...\\nNew set of weights found, iteration: \\u200b31 \\u200bloss: \\u200b1.0982264 \\u200bacc:  \\n0.37333333333333335 \\u200b...', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 133}),\n",
       " Document(page_content='New set of weights found, iteration: \\u200b65 \\u200bloss: \\u200b1.0954362 \\u200bacc:  \\n0.38333333333333336\\nNew set of weights found, iteration: \\u200b67 \\u200bloss: \\u200b1.093989 \\u200bacc:  \\n0.4166666666666667 \\u200b...\\nNew set of weights found, iteration: \\u200b129 \\u200bloss: \\u200b1.0874122 \\u200bacc:  \\n0.42333333333333334 \\u200b...\\nNew set of weights found, iteration: \\u200b5415 \\u200bloss: \\u200b1.0790575 \\u200bacc: \\u200b0.39\\nThis training session ended with almost no progress. Loss decreased slightly and accuracy is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 133}),\n",
       " Document(page_content='barely above the initial value. Later, we’ll learn that the most probable reason for this is called a  \\nlocal minimum of loss. The data complexity is also not irrelevant here. It turns out hard problems  \\nare hard for a reason, and we need to approach this problem more intelligently.  \\nSupplementary Material: \\u200bhttps://nnfs.io/ch6  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 133}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n6\\n \\n \\n \\n \\n \\nChapter 7\\n \\nDerivatives\\n \\nRandomly changing and searching for optimal weights and biases did not prove fruitful for one\\n \\nmain reason: the number of possible combinations of weights and biases is infinite, and we need\\n \\nsomething smarter than pure luck to achieve any success. Each weight and bias may also have\\n \\ndifferent degrees of influence on the loss — this influence depends on the parameters themselves', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 134}),\n",
       " Document(page_content='as well as on the current sample, which is an input to the first layer. These input values are then\\n \\nmultiplied by the weights, so the input data affects the neuron’s output and affects the impact that\\n \\nthe weights make on the loss. The same principle applies to the biases and parameters in the next\\n \\nlayers, taking the previous layer’s outputs as inputs. This means that the impact on the output\\n \\nvalues depends on the parameters as well as the samples — which is why we are calculating the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 134}),\n",
       " Document(page_content='loss value per each sample separately. Finally, the function of \\n\\u200b\\nhow\\n\\u200b\\n a weight or bias impacts the\\n \\noverall loss is not necessarily linear. In order to know \\n\\u200b\\nhow\\n\\u200b\\n to adjust weights and biases, we first\\n \\nneed to understand their impact on the loss.\\n \\nOne concept to note is that we refer to weights and biases and their impact on the loss function.\\n \\nThe loss function doesn’t contain weights or biases, though. The input to this function is the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 134}),\n",
       " Document(page_content='output of the model, and the weights and biases of the neurons influence this output. Thus, even\\n \\nthough we calculate loss from the model’s output, not weights/biases, these weights and biases', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 134}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n7\\n \\ndirectly impact the loss.\\n \\nIn the coming chapters, we will describe exactly how this happens by explaining partial\\n \\nderivatives, gradients, gradient descent, and backpropagation. Basically, we’ll calculate how\\n \\nmuch each singular weight and bias changes the loss value (how much of an impact it has on it)\\n \\ngiven a sample (as each sample produces a separate output, thus also a separate loss value), and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 135}),\n",
       " Document(page_content='how to change this weight or bias for the loss value to decrease. Remember — our goal here is\\n \\nto decrease loss, and we’ll do this by using gradient descent. Gradient, on the other hand, is a\\n \\nresult of the calculation of the partial derivatives, and we’ll backpropagate it using the chain rule\\n \\nto update all of the weights and biases. Don’t worry if that doesn’t make much sense yet; we’ll\\n \\nexplain all of these terms and how to perform these actions in this and the coming chapters.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 135}),\n",
       " Document(page_content='To understand partial derivatives, we need to start with derivatives, which are a special case of\\n \\npartial derivatives — they are calculated from functions taking single parameters.\\n \\nThe Impact of a Parameter on the Output\\n \\nLet’s start with a simple function and discover what is meant by “impact.”\\n \\nA very simple function \\n\\u200b\\ny=2x\\n\\u200b\\n, which takes \\n\\u200b\\nx\\n\\u200b\\n as an input:\\n \\ndef \\n\\u200b\\nf\\n\\u200b\\n(\\n\\u200b\\nx\\n\\u200b\\n):\\n \\n    \\n\\u200b\\nreturn \\n\\u200b\\n2\\n\\u200b\\n*\\n\\u200b\\nx', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 135}),\n",
       " Document(page_content='\\u200b\\n2\\n\\u200b\\n*\\n\\u200b\\nx\\n \\n \\nNow let’s create some code around this to visualize the data — we’ll import NumPy and\\n \\nMatplotlib, create an array of 5 input values from 0 to 4, calculate the function output for each\\n \\nof these input values, and plot the result as lines between consecutive points. These points’\\n \\ncoordinates are inputs as \\n\\u200b\\nx\\n\\u200b\\n and function outputs as \\n\\u200b\\ny\\n\\u200b\\n:\\n \\nimport \\n\\u200b\\nmatplotlib.pyplot \\n\\u200b\\nas \\n\\u200b\\nplt\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\ndef \\n\\u200b\\nf\\n\\u200b\\n(\\n\\u200b\\nx\\n\\u200b\\n):\\n \\n    \\n\\u200b\\nreturn \\n\\u200b\\n2\\n\\u200b\\n*\\n\\u200b\\nx', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 135}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n8\\n \\nx \\n\\u200b\\n= \\n\\u200b\\nnp.array(\\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n5\\n\\u200b\\n))\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nf(x)\\n \\n \\n \\nprint\\n\\u200b\\n(x)\\n \\nprint\\n\\u200b\\n(y)\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0 1 2 3 4\\n\\u200b\\n]\\n \\n[\\n\\u200b\\n0 2 4 6 8\\n\\u200b\\n]\\n \\n \\n \\nplt.plot(x, y)\\n \\nplt.show()\\n \\n \\n \\nFig 7.01:\\n\\u200b\\n Linear function y=2x graphed', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 136}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n9\\n \\n \\nThe Slope\\n \\nThis looks like an output of the \\n\\u200b\\nf(x) = 2x\\n\\u200b\\n function, which is a line. How might you define the\\n \\nimpact\\n\\u200b\\n that \\n\\u200b\\nx\\n\\u200b\\n will have on \\n\\u200b\\ny\\n\\u200b\\n? Some will say, \\n\\u200b\\n“y \\n\\u200b\\nis double \\n\\u200b\\nx\\n\\u200b\\n” Another way to describe the \\n\\u200b\\nimpact\\n \\nof a linear function such as this comes from algebra: the \\n\\u200b\\nslope\\n\\u200b\\n. “Rise over run” might be a phrase\\n \\nyou recall from school. The slope of a line is:\\n \\n \\nIt is change in \\n\\u200b\\ny\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 137}),\n",
       " Document(page_content='\\u200b\\ny\\n\\u200b\\n divided by change in \\n\\u200b\\nx,\\n\\u200b\\n or, in math — \\n\\u200b\\ndelta y\\n\\u200b\\n divided by \\n\\u200b\\ndelta x\\n\\u200b\\n. What’s the slope\\n \\nof \\n\\u200b\\nf(x) = 2x\\n\\u200b\\n then?\\n \\nTo calculate the slope, first we have to take any two points lying on the function’s graph and\\n \\nsubtract them to calculate the change. Subtracting the points means to subtract their x and y\\n \\ndimensions respectively. Division of the change in y by the change in x returns the slope:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 137}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n10\\n \\nContinuing the code, we keep all values of \\n\\u200b\\nx\\n\\u200b\\n in a single-dimensional NumPy array, \\n\\u200b\\nx\\n\\u200b\\n, and all\\n \\nresults in a single-dimensional array, \\n\\u200b\\ny\\n\\u200b\\n. To perform the same operation, we’ll take \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n and\\n \\ny[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n for the first point, then \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n and \\n\\u200b\\ny[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n for the second one. Now we can calculate the slope\\n \\nbetween them:\\n \\nprint\\n\\u200b\\n((y[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n-\\n\\u200b\\ny[\\n\\u200b\\n0\\n\\u200b\\n]) \\n\\u200b\\n/ \\n\\u200b\\n(x[\\n\\u200b\\n1\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 138}),\n",
       " Document(page_content='\\u200b\\n/ \\n\\u200b\\n(x[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n-\\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n]))\\n \\n \\n \\n>>>\\n \\n2.0\\n \\n \\nIt is not surprising that the slope of this line is 2. We could say the measure of the impact that \\n\\u200b\\nx\\n \\nhas on \\n\\u200b\\ny\\n\\u200b\\n is 2. We can calculate the slope in the same way for any linear function, including linear\\n \\nfunctions that aren’t as obvious.\\n \\nWhat about a nonlinear function like \\n\\u200b\\nf(x)=2x\\n\\u200b\\n2 \\n\\u200b\\n?\\n \\ndef \\n\\u200b\\nf\\n\\u200b\\n(\\n\\u200b\\nx\\n\\u200b\\n):\\n \\n    \\n\\u200b\\nreturn \\n\\u200b\\n2\\n\\u200b\\n*\\n\\u200b\\nx\\n\\u200b\\n**\\n\\u200b\\n2\\n \\n \\nThis function creates a graph that does not form a straight line:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 138}),\n",
       " Document(page_content='Fig 7.02:\\n\\u200b\\n Approximation of the parabolic function y=2x\\n\\u200b\\n2\\n\\u200b\\n graphed', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 138}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n11\\n \\nCan we measure the slope of this curve? Depending on which 2 points we choose to use, we will\\n \\nmeasure varying slopes:\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nf(x)  \\n\\u200b\\n# Calculate function outputs for new function\\n \\n \\nprint\\n\\u200b\\n(x)\\n \\nprint\\n\\u200b\\n(y)\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0 1 2 3 4\\n\\u200b\\n]\\n \\n[ \\n\\u200b\\n0  2  8 18 32\\n\\u200b\\n]\\n \\n \\nNow for the first pair of points:\\n \\nprint\\n\\u200b\\n((y[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n-\\n\\u200b\\ny[\\n\\u200b\\n0\\n\\u200b\\n]) \\n\\u200b\\n/ \\n\\u200b\\n(x[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n-\\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n]))\\n \\n \\n \\n>>>\\n \\n2', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 139}),\n",
       " Document(page_content=\">>>\\n \\n2\\n \\n \\nAnd for another one:\\n \\nprint\\n\\u200b\\n((y[\\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n-\\n\\u200b\\ny[\\n\\u200b\\n2\\n\\u200b\\n]) \\n\\u200b\\n/ \\n\\u200b\\n(x[\\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n-\\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n]))\\n \\n>>>\\n \\n10\\n \\n \\nFig 7.03:\\n\\u200b\\n Approximation of the parabolic function's example tangents\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 139}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n12\\n \\n \\nAnim 7.03:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/bro\\n \\nHow might we measure the impact that x has on y in this nonlinear function? Calculus proposes\\n \\nthat we measure the slope of the \\n\\u200b\\ntangent line\\n\\u200b\\n at \\n\\u200b\\nx\\n\\u200b\\n (for a specific input value to the function),\\n \\ngiving us the\\n\\u200b\\n instantaneous slope\\n\\u200b\\n (slope at this point)\\n\\u200b\\n, \\n\\u200b\\nwhich is the \\n\\u200b\\nderivative\\n\\u200b\\n. The \\n\\u200b\\ntangent line', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 140}),\n",
       " Document(page_content='\\u200b\\ntangent line\\n \\nis created by drawing a line between two points that are “infinitely close” on a curve, but this\\n \\ncurve has to be differentiable at the derivation point. This means that it has to be continuous and\\n \\nsmooth (we cannot calculate the slope at something that we could describe as a “sharp corner,”\\n \\nsince it contains an infinite number of slopes). Then, because this is a curve, there is no single\\n \\nslope. Slope depends on where we measure it. To give an immediate example, we can', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 140}),\n",
       " Document(page_content='approximate a derivative of the function at \\n\\u200b\\nx\\n\\u200b\\n by using this point and another one also taken at x,\\n \\nbut with a very small delta added to it, such as \\n\\u200b\\n0.0001\\n\\u200b\\n. This number is a common choice as it does\\n \\nnot introduce too large an error (when estimating the derivative) or cause the whole expression to\\n \\nbe numerically unstable (Δ\\n\\u200b\\nx\\n\\u200b\\n might round to 0 due to floating-point number resolution). This lets', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 140}),\n",
       " Document(page_content='us perform the same calculation for the slope as before, but on two points that are very close to\\n \\neach other, resulting in a good approximation of a slope at x:\\n \\np2_delta \\n\\u200b\\n= \\n\\u200b\\n0.0001\\n \\n \\nx1 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\nx2 \\n\\u200b\\n= \\n\\u200b\\nx1 \\n\\u200b\\n+ \\n\\u200b\\np2_delta  \\n\\u200b\\n# add delta\\n \\n \\ny1 \\n\\u200b\\n= \\n\\u200b\\nf(x1)  \\n\\u200b\\n# result at the derivation point\\n \\ny2 \\n\\u200b\\n= \\n\\u200b\\nf(x2)  \\n\\u200b\\n# result at the other, close point\\n \\n \\napproximate_derivative \\n\\u200b\\n= \\n\\u200b\\n(y2\\n\\u200b\\n-\\n\\u200b\\ny1)\\n\\u200b\\n/\\n\\u200b\\n(x2\\n\\u200b\\n-\\n\\u200b\\nx1)\\n \\nprint\\n\\u200b\\n(approximate_derivative)\\n \\n \\n \\n>>>', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 140}),\n",
       " Document(page_content='>>>\\n \\n4.0001999999987845\\n \\n \\nAs we will soon learn, the derivative of \\n\\u200b\\n2x\\n\\u200b\\n2\\n\\u200b\\n at \\n\\u200b\\nx=1\\n\\u200b\\n should be exactly \\n\\u200b\\n4\\n\\u200b\\n. The difference we see\\n \\n \\n(~4.0002) comes from the method used to compute the tangent. We chose a delta small enough to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 140}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n13\\n \\napproximate the derivative as accurately as possible but large enough to prevent a rounding error.\\n \\nTo elaborate, an infinitely small delta value will approximate an accurate derivative; however, the\\n \\ndelta value needs to be numerically stable, meaning, our delta can not surpass the limitations of\\n \\nPython’s floating-point precision (can’t be too small as it might be rounded to \\n\\u200b\\n0\\n\\u200b\\n and, as we know,\\n \\ndividing by \\n\\u200b\\n0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 141}),\n",
       " Document(page_content='dividing by \\n\\u200b\\n0\\n\\u200b\\n is “illegal”). Our solution is, therefore, restricted between estimating the derivative\\n \\nand remaining numerically stable, thus introducing this small but visible error.\\n \\nThe Numerical Derivative\\n \\nThis method of calculating the derivative is called \\n\\u200b\\nnumerical differentiation \\n\\u200b\\n— calculating the\\n \\nslope of the tangent line using two \\n\\u200b\\ninfinitely\\n\\u200b\\n close points, or as with the code solution —', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 141}),\n",
       " Document(page_content='calculating the slope of a tangent line made from two points that were “sufficiently close.” We\\n \\ncan visualize why we perform this on two close points with the following:\\n \\n \\nFig 7.04:\\n\\u200b\\n Why we want to use 2 points that are sufficiently close — large delta inaccuracy.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 141}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n14\\n \\n \\nFig 7.05:\\n\\u200b\\n Why we want to use 2 points that are sufficiently close — very small delta accuracy.\\n \\n \\nAnim 7.04-7.05:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/cat\\n \\nWe can see that the closer these two points are to each other, the more correct the tangent line\\n \\nappears to be.\\n \\nContinuing with \\n\\u200b\\nnumerical differentiation\\n\\u200b\\n, let us visualize the tangent lines and how they', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 142}),\n",
       " Document(page_content='change depending on where we calculate them. To begin, we’ll make the graph of this function\\n \\nmore granular using Numpy’s \\n\\u200b\\narange()\\n\\u200b\\n, \\n\\u200b\\nallowing us to plot with smaller steps. The\\n \\nnp.arange()\\n\\u200b\\n \\n\\u200b\\nfunction takes in \\n\\u200b\\nstart, stop, \\n\\u200b\\nand \\n\\u200b\\nstep\\n\\u200b\\n parameters, allowing us to take fractions of a\\n \\nstep, such as \\n\\u200b\\n0.001\\n\\u200b\\n at a time:\\n \\nimport \\n\\u200b\\nmatplotlib.pyplot \\n\\u200b\\nas \\n\\u200b\\nplt\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n \\ndef \\n\\u200b\\nf\\n\\u200b\\n(\\n\\u200b\\nx\\n\\u200b\\n):\\n \\n    \\n\\u200b\\nreturn \\n\\u200b\\n2\\n\\u200b\\n*\\n\\u200b\\nx\\n\\u200b\\n**\\n\\u200b\\n2', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 142}),\n",
       " Document(page_content='\\u200b\\nx\\n\\u200b\\n**\\n\\u200b\\n2\\n \\n \\n \\n# np.arange(start, stop, step) to give us smoother line\\n \\nx \\n\\u200b\\n= \\n\\u200b\\nnp.arange(\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n5\\n\\u200b\\n, \\n\\u200b\\n0.001\\n\\u200b\\n)\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nf(x)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 142}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n15\\n \\n \\nplt.plot(x, y)\\n \\n \\nplt.show()\\n \\n \\n \\nFig 7.06:\\n\\u200b\\n Matplotlib output that you should see from graphing y=2x\\n\\u200b\\n2\\n\\u200b\\n.\\n \\nTo draw these tangent lines, we will derive the function for the tangent line at a point and plot\\n \\nthe tangent on the graph at this point. The function for a straight line is \\n\\u200b\\ny = mx+b\\n\\u200b\\n. Where \\n\\u200b\\nm\\n\\u200b\\n is\\n \\nthe slope or the \\n\\u200b\\napproximate_derivative\\n\\u200b\\n that we’ve already calculated. And\\n\\u200b\\n x \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 143}),\n",
       " Document(page_content='\\u200b\\n x \\n\\u200b\\nis the input which\\n \\nleaves \\n\\u200b\\nb\\n\\u200b\\n, or the y-intercept, for us to calculate. The slope remains unchanged, but currently, you\\n \\ncan “move” the line up or down using the y-intercept. We already know \\n\\u200b\\nx\\n\\u200b\\n and \\n\\u200b\\nm\\n\\u200b\\n, but \\n\\u200b\\nb\\n\\u200b\\n is still\\n \\nunknown. Let’s assume \\n\\u200b\\nm=1\\n\\u200b\\n for the purpose of the figure and see what exactly it means:\\n \\n \\nFig 7.07:\\n\\u200b\\n Various biases graphed where slope = 1.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 143}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n16\\n \\n \\nAnim 7.07:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/but\\n \\nTo calculate \\n\\u200b\\nb,\\n\\u200b\\n the formula is \\n\\u200b\\nb = y - mx\\n\\u200b\\n:\\n \\n \\nSo far we’ve used two points — the point that we want to calculate the derivative at and the\\n \\n“close enough” to it point to calculate the approximation of the derivative. Now, given the above\\n \\nequation for \\n\\u200b\\nb\\n\\u200b\\n, the approximation of the derivative and the same “close enough” point (its \\n\\u200b\\nx\\n\\u200b\\n and \\n\\u200b\\ny', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 144}),\n",
       " Document(page_content='\\u200b\\nx\\n\\u200b\\n and \\n\\u200b\\ny\\n \\ncoordinates to be specific), we can substitute them in the equation and get the y-intercept for the\\n \\ntangent line at the derivation point. Using code:\\n \\nb \\n\\u200b\\n= \\n\\u200b\\ny2 \\n\\u200b\\n-\\n\\u200b\\n approximate_derivative\\n\\u200b\\n*\\n\\u200b\\nx2\\n \\n \\nPutting everything together:\\n \\nimport \\n\\u200b\\nmatplotlib.pyplot \\n\\u200b\\nas \\n\\u200b\\nplt\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n \\ndef \\n\\u200b\\nf\\n\\u200b\\n(\\n\\u200b\\nx\\n\\u200b\\n):\\n \\n    \\n\\u200b\\nreturn \\n\\u200b\\n2\\n\\u200b\\n*\\n\\u200b\\nx\\n\\u200b\\n**\\n\\u200b\\n2\\n \\n \\n \\n# np.arange(start, stop, step) to give us smoother line\\n \\nx \\n\\u200b\\n= \\n\\u200b\\nnp.arange(\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n5\\n\\u200b\\n,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 144}),\n",
       " Document(page_content='\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n5\\n\\u200b\\n, \\n\\u200b\\n0.001\\n\\u200b\\n)\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nf(x)\\n \\n \\nplt.plot(x, y)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 144}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n17\\n \\n# The point and the \"close enough\" point\\n \\np2_delta \\n\\u200b\\n= \\n\\u200b\\n0.0001\\n \\nx1 \\n\\u200b\\n= \\n\\u200b\\n2\\n \\nx2 \\n\\u200b\\n= \\n\\u200b\\nx1\\n\\u200b\\n+\\n\\u200b\\np2_delta\\n \\n \\ny1 \\n\\u200b\\n= \\n\\u200b\\nf(x1)\\n \\ny2 \\n\\u200b\\n= \\n\\u200b\\nf(x2)\\n \\n \\nprint\\n\\u200b\\n((x1, y1), (x2, y2))\\n \\n \\n# Derivative approximation and y-intercept for the tangent line\\n \\napproximate_derivative \\n\\u200b\\n= \\n\\u200b\\n(y2\\n\\u200b\\n-\\n\\u200b\\ny1)\\n\\u200b\\n/\\n\\u200b\\n(x2\\n\\u200b\\n-\\n\\u200b\\nx1)\\n \\nb \\n\\u200b\\n= \\n\\u200b\\ny2 \\n\\u200b\\n- \\n\\u200b\\napproximate_derivative\\n\\u200b\\n*\\n\\u200b\\nx2', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 145}),\n",
       " Document(page_content='\\u200b\\n*\\n\\u200b\\nx2\\n \\n \\n \\n# We put the tangent line calculation into a function so we can call\\n \\n# it multiple times for different values of x\\n \\n# approximate_derivative and b are constant for given function\\n \\n# thus calculated once above this function\\n \\ndef \\n\\u200b\\ntangent_line\\n\\u200b\\n(\\n\\u200b\\nx\\n\\u200b\\n):\\n \\n    \\n\\u200b\\nreturn \\n\\u200b\\napproximate_derivative\\n\\u200b\\n*\\n\\u200b\\nx \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\n \\n \\n# plotting the tangent line\\n \\n# +/- 0.9 to draw the tangent line on our graph\\n \\n# then we calculate the y for given x using the tangent line function', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 145}),\n",
       " Document(page_content=\"# Matplotlib will draw a line for us through these points\\n \\nto_plot \\n\\u200b\\n= \\n\\u200b\\n[x1\\n\\u200b\\n-\\n\\u200b\\n0.9\\n\\u200b\\n, x1, x1\\n\\u200b\\n+\\n\\u200b\\n0.9\\n\\u200b\\n]\\n \\nplt.plot(to_plot, [tangent_line(i) \\n\\u200b\\nfor \\n\\u200b\\ni \\n\\u200b\\nin \\n\\u200b\\nto_plot])\\n \\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'Approximate derivative for f(x)'\\n\\u200b\\n,\\n \\n      \\n\\u200b\\nf\\n\\u200b\\n'where x = \\n\\u200b\\n{x1} \\n\\u200b\\nis \\n\\u200b\\n{approximate_derivative}\\n\\u200b\\n'\\n\\u200b\\n)\\n \\n \\nplt.show()\\n \\n \\n \\n>>>\\n \\n(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n8\\n\\u200b\\n) (\\n\\u200b\\n2.0001\\n\\u200b\\n, \\n\\u200b\\n8.000800020000002\\n\\u200b\\n)\\n \\nApproximate derivative for f(x) where x = 2 is 8.000199999998785\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 145}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n18\\n \\n \\nFig 7.08:\\n\\u200b\\n Graphed approximate derivative for f(x) where x=2\\n \\nThe orange line is the approximate tangent line at \\n\\u200b\\nx=2\\n\\u200b\\n for the function \\n\\u200b\\nf(x) = 2x\\n\\u200b\\n2\\n\\u200b\\n. Why do we care\\n \\nabout this? You will soon find that we care only about the \\n\\u200b\\nslope\\n\\u200b\\n of this tangent line but both\\n \\nvisualizing and understanding the \\n\\u200b\\ntangent line\\n\\u200b\\n are very important. We care about the slope of the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 146}),\n",
       " Document(page_content='tangent line because it informs us about the \\n\\u200b\\nimpact\\n\\u200b\\n that \\n\\u200b\\nx\\n\\u200b\\n has on this function at a particular point,\\n \\nreferred to as the \\n\\u200b\\ninstantaneous rate of change\\n\\u200b\\n. We will use this concept to determine the effect\\n \\nof a specific weight or bias on the overall loss function given a sample. For now, with different\\n \\nvalues for \\n\\u200b\\nx\\n\\u200b\\n, we can observe resulting impacts on the function. We can continue the previous code\\n \\nto see the tangent line for various inputs (\\n\\u200b\\nx)\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 146}),\n",
       " Document(page_content=\"\\u200b\\nx)\\n\\u200b\\n - we put a part of the code in a loop over example \\n\\u200b\\nx\\n \\nvalues and plot multiple tangent lines:\\n \\nimport \\n\\u200b\\nmatplotlib.pyplot \\n\\u200b\\nas \\n\\u200b\\nplt\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n \\ndef \\n\\u200b\\nf\\n\\u200b\\n(\\n\\u200b\\nx\\n\\u200b\\n):\\n \\n    \\n\\u200b\\nreturn \\n\\u200b\\n2\\n\\u200b\\n*\\n\\u200b\\nx\\n\\u200b\\n**\\n\\u200b\\n2\\n \\n \\n \\n# np.arange(start, stop, step) to give us a smoother curve\\n \\nx \\n\\u200b\\n= \\n\\u200b\\nnp.array(np.arange(\\n\\u200b\\n0\\n\\u200b\\n,\\n\\u200b\\n5\\n\\u200b\\n,\\n\\u200b\\n0.001\\n\\u200b\\n))\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nf(x)\\n \\n \\nplt.plot(x, y)\\n \\n \\ncolors \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n'k'\\n\\u200b\\n,\\n\\u200b\\n'g'\\n\\u200b\\n,\\n\\u200b\\n'r'\\n\\u200b\\n,\\n\\u200b\\n'b'\\n\\u200b\\n,\\n\\u200b\\n'c'\\n\\u200b\\n]\\n \\n \\n \\ndef \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 146}),\n",
       " Document(page_content='\\u200b\\n]\\n \\n \\n \\ndef \\n\\u200b\\napproximate_tangent_line\\n\\u200b\\n(\\n\\u200b\\nx\\n\\u200b\\n, \\n\\u200b\\napproximate_derivative\\n\\u200b\\n):\\n \\n    \\n\\u200b\\nreturn \\n\\u200b\\n(approximate_derivative\\n\\u200b\\n*\\n\\u200b\\nx) \\n\\u200b\\n+ \\n\\u200b\\nb', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 146}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n19\\n \\nfor \\n\\u200b\\ni \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n5\\n\\u200b\\n):\\n \\n    p2_delta \\n\\u200b\\n= \\n\\u200b\\n0.0001\\n \\n    \\n\\u200b\\nx1 \\n\\u200b\\n= \\n\\u200b\\ni\\n \\n    x2 \\n\\u200b\\n= \\n\\u200b\\nx1\\n\\u200b\\n+\\n\\u200b\\np2_delta\\n \\n \\n    y1 \\n\\u200b\\n= \\n\\u200b\\nf(x1)\\n \\n    y2 \\n\\u200b\\n= \\n\\u200b\\nf(x2)\\n \\n \\n    \\n\\u200b\\nprint\\n\\u200b\\n((x1, y1), (x2, y2))\\n \\n    approximate_derivative \\n\\u200b\\n= \\n\\u200b\\n(y2\\n\\u200b\\n-\\n\\u200b\\ny1)\\n\\u200b\\n/\\n\\u200b\\n(x2\\n\\u200b\\n-\\n\\u200b\\nx1)\\n \\n    b \\n\\u200b\\n= \\n\\u200b\\ny2\\n\\u200b\\n-\\n\\u200b\\n(approximate_derivative\\n\\u200b\\n*\\n\\u200b\\nx2)\\n \\n \\n    to_plot \\n\\u200b\\n= \\n\\u200b\\n[x1\\n\\u200b\\n-\\n\\u200b\\n0.9\\n\\u200b\\n, x1, x1\\n\\u200b\\n+\\n\\u200b\\n0.9\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 147}),\n",
       " Document(page_content=\"\\u200b\\n+\\n\\u200b\\n0.9\\n\\u200b\\n]\\n \\n \\n    plt.scatter(x1, y1, \\n\\u200b\\nc\\n\\u200b\\n=\\n\\u200b\\ncolors[i])\\n \\n    plt.plot([point \\n\\u200b\\nfor \\n\\u200b\\npoint \\n\\u200b\\nin \\n\\u200b\\nto_plot],\\n \\n             [approximate_tangent_line(point, approximate_derivative)\\n \\n                 \\n\\u200b\\nfor \\n\\u200b\\npoint \\n\\u200b\\nin \\n\\u200b\\nto_plot],\\n \\n             \\n\\u200b\\nc\\n\\u200b\\n=\\n\\u200b\\ncolors[i])\\n \\n \\n    \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\n'Approximate derivative for f(x)'\\n\\u200b\\n,\\n \\n          \\n\\u200b\\nf\\n\\u200b\\n'where x = \\n\\u200b\\n{x1} \\n\\u200b\\nis \\n\\u200b\\n{approximate_derivative}\\n\\u200b\\n'\\n\\u200b\\n)\\n \\n \\nplt.show()\\n \\n \\n \\n>>>\\n \\n(\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n0\\n\\u200b\\n) (\\n\\u200b\\n0.0001\\n\\u200b\\n, \\n\\u200b\\n2e-08\\n\\u200b\\n)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 147}),\n",
       " Document(page_content='\\u200b\\n, \\n\\u200b\\n2e-08\\n\\u200b\\n)\\n \\nApproximate derivative for f(x) where x = 0 is 0.00019999999999999998\\n \\n(\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n) (\\n\\u200b\\n1.0001\\n\\u200b\\n, \\n\\u200b\\n2.00040002\\n\\u200b\\n)\\n \\nApproximate derivative for f(x) where x = 1 is 4.0001999999987845\\n \\n(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n8\\n\\u200b\\n) (\\n\\u200b\\n2.0001\\n\\u200b\\n, \\n\\u200b\\n8.000800020000002\\n\\u200b\\n)\\n \\nApproximate derivative for f(x) where x = 2 is 8.000199999998785\\n \\n(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n18\\n\\u200b\\n) (\\n\\u200b\\n3.0001\\n\\u200b\\n, \\n\\u200b\\n18.001200020000002\\n\\u200b\\n)\\n \\nApproximate derivative for f(x) where x = 3 is 12.000199999998785\\n \\n(\\n\\u200b\\n4\\n\\u200b\\n, \\n\\u200b\\n32\\n\\u200b\\n) (\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 147}),\n",
       " Document(page_content='4\\n\\u200b\\n, \\n\\u200b\\n32\\n\\u200b\\n) (\\n\\u200b\\n4.0001\\n\\u200b\\n, \\n\\u200b\\n32.00160002\\n\\u200b\\n)\\n \\nApproximate derivative for f(x) where x = 4 is 16.000200000016548', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 147}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n20\\n \\n \\nFig 7.09:\\n\\u200b\\n Derivative calculated at various points.\\n \\nFor this simple function, \\n\\u200b\\nf(x) = 2x\\n\\u200b\\n2\\n\\u200b\\n, we didn’t pay a high penalty by approximating the derivative\\n \\n(i.e., the slope of the tangent line) like this, and received a value that was close enough for our\\n \\nneeds.\\n \\nThe problem is that the \\n\\u200b\\nactual\\n\\u200b\\n function employed in our neural network is not so simple. The loss', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 148}),\n",
       " Document(page_content='function contains all of the layers, weights, and biases — it’s an absolutely massive function\\n \\noperating in multiple dimensions! Calculating derivatives using \\n\\u200b\\nnumerical differentiation\\n \\nrequires multiple forward passes for a single parameter update (we’ll talk about parameter updates\\n \\nin chapter 10). We need to perform the forward pass as a reference, then update a single\\n \\nparameter by the delta value and perform the forward pass through our model again to see the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 148}),\n",
       " Document(page_content='change of the loss value. Next, we need to calculate the \\n\\u200b\\nderivative\\n\\u200b\\n and revert the parameter\\n \\nchange that we made for this calculation. We have to repeat this for every weight and bias and for\\n \\nevery sample, which will be very time-consuming. We can also think of this method as\\n \\nbrute-forcing the derivative calculations. To reiterate, as we quickly covered many terms, the\\n \\nderivative\\n\\u200b\\n is the \\n\\u200b\\nslope\\n\\u200b\\n of the \\n\\u200b\\ntangent line\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 148}),\n",
       " Document(page_content='\\u200b\\ntangent line\\n\\u200b\\n for a function that takes a single parameter as an input.\\n \\nWe’ll use this ability to calculate the slopes of the loss function at each of the weight and bias\\n \\npoints — this brings us to the multivariate function, which is a function that takes multiple\\n \\nparameters and is a topic for the next chapter — the partial derivative.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 148}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n21\\n \\n \\nThe Analytical Derivative\\n \\nNow that we have a better idea of what a derivative \\n\\u200b\\nis\\n\\u200b\\n, how to calculate the numerical (also called\\n \\nuniversal) derivative, and why it’s not a good approach for us, we can move on to the \\n\\u200b\\nAnalytical\\n \\nDerivative\\n\\u200b\\n, the actual solution to the derivative that we’ll implement in our code.\\n \\nIn mathematics, there are two general ways to solve problems: \\n\\u200b\\nnumerical \\n\\u200b\\nand \\n\\u200b\\nanalytical', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 149}),\n",
       " Document(page_content='and \\n\\u200b\\nanalytical\\n \\nmethods. Numerical solution methods involve coming up with a number to find a solution, like\\n \\nthe above approach with \\n\\u200b\\napproximate_derivative\\n\\u200b\\n. The numerical solution is also an\\n \\napproximation. On the other hand, the analytical method offers the exact and much quicker, in\\n \\nterms of calculation, solution. However, identifying the analytical solution for the derivative of a\\n \\ngiven function, as we’ll quickly learn, will vary in complexity, whereas the numerical approach', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 149}),\n",
       " Document(page_content='never gets more complicated — it’s always calling the method twice with two inputs to calculate\\n \\nthe approximate derivative at a point. Some analytical solutions are quite obvious, some can be\\n \\ncalculated with simple rules, and some complex functions can be broken down into simpler parts\\n \\nand calculated using the so-called \\n\\u200b\\nchain rule\\n\\u200b\\n. We can leverage already-proven derivative\\n \\nsolutions for certain functions, and others — like our loss function — can be solved with', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 149}),\n",
       " Document(page_content='combinations of the above.\\n \\nTo compute the derivative of functions using the analytical method, we can split them into simple,\\n \\nelemental functions, finding the derivatives of those and then applying the \\n\\u200b\\nchain rule\\n\\u200b\\n, which we\\n \\nwill explain soon, to get the full derivative. To start building an intuition, let’s start with simple\\n \\nfunctions and their respective derivatives.\\n \\nThe derivative of a simple constant function:\\n \\n \\n \\nFig 7.10:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 149}),\n",
       " Document(page_content='Fig 7.10:\\n\\u200b\\n Derivative of a constant function — calculation steps.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 149}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n22\\n \\n \\nAnim 7.10:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/cow\\n \\nWhen calculating the derivative of a function, recall that the derivative can be interpreted as a\\n \\nslope. In this example, the result of this function is a horizontal line as the output value for any x\\n \\nis 1:\\n \\nBy looking at it, it becomes evident that the derivative equals 0 since there’s no change from one\\n \\nvalue of x to any other value of x (i.e., there’s no slope).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 150}),\n",
       " Document(page_content=\"So far, we are calculating derivatives of the functions by taking a single parameter, \\n\\u200b\\nx\\n\\u200b\\n in our case,\\n \\nin each example. This changes with partial derivatives since they take functions with multiple\\n \\nparameters, and we’ll be calculating the derivative with respect to only one of them at a time. For\\n \\nnow, with derivatives, it’s always with respect to a single parameter. To denote the derivative,\\n \\nwe can use prime notation, where, for the function \\n\\u200b\\nf(x),\\n\\u200b\\n we add a prime (') like f\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 150}),\n",
       " Document(page_content=\"\\u200b\\n'(x)\\n\\u200b\\n. For our\\n \\nexample, \\n\\u200b\\nf(x) = 1\\n\\u200b\\n, the derivative \\n\\u200b\\nf'(x) = 0\\n\\u200b\\n. Another notation we can use is called the Leibniz’s\\n \\nnotation — the dependence on the prime notation and multiple ways of writing the derivative\\n \\nwith the Leibniz’s notation is as follows:\\n \\n \\n \\nEach of these notations has the same meaning — the derivative of a function (with respect to \\n\\u200b\\nx\\n\\u200b\\n).\\n \\nIn the following examples, we use both notations, since sometimes it’s convenient to use one\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 150}),\n",
       " Document(page_content='notation or another. We can also use both of them in a single equation.\\n \\nIn summary: the derivative of a constant function equals \\n\\u200b\\n0\\n\\u200b\\n:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 150}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n23\\n \\nThe derivative of a linear function:\\n \\n \\n \\n \\nFig 7.11:\\n\\u200b\\n Derivative of a linear function — calculation steps.\\n \\n \\nAnim 7.11:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/tob\\n \\nIn this case, the derivative is 1, and the intuition behind this is that for every change of x, y\\n \\nchanges by the same amount, so y changes one times the x.\\n \\nThe derivative of the linear function equals \\n\\u200b\\n1\\n\\u200b\\n (but not in every case, which we’ll explain next):', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 151}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n24\\n \\nWhat if we try 2x, which is also a linear function?\\n \\n \\n \\n \\nFig 7.12:\\n\\u200b\\n Derivative of another linear function — calculation steps.\\n \\n \\nAnim 7.12:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/pop\\n \\nWhen calculating the derivative, we can take any constant that function is multiplied by and move\\n \\nit outside of the derivative — in this case it’s 2 multiplied by the derivative of x. Since we already\\n \\ndetermined that the derivative of \\n\\u200b\\nf(x) = x', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 152}),\n",
       " Document(page_content='\\u200b\\nf(x) = x\\n\\u200b\\n was \\n\\u200b\\n1\\n\\u200b\\n, we now multiply it by \\n\\u200b\\n2\\n\\u200b\\n to give us the result.\\n \\nThe derivative of a linear function equals the slope, \\n\\u200b\\nm\\n\\u200b\\n In this case \\n\\u200b\\nm = 2\\n\\u200b\\n:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 152}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n25\\n \\nIf you associate this with numerical differentiation, you’re absolutely right — we already\\n \\nconcluded that the derivative of a linear function equals its slope:\\n \\n \\n \\nm,\\n\\u200b\\n in this case, is a constant, no different than the value \\n\\u200b\\n2,\\n\\u200b\\n as it’s not a parameter — every\\n \\nnon-parameter to the function can’t change its value; thus, we consider it to be a constant. We', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 153}),\n",
       " Document(page_content='have just found a simpler way to calculate the derivative of a linear function and also generalized\\n \\nit for the equations of different slopes, \\n\\u200b\\nm\\n\\u200b\\n. It’s also an exact derivative, not an approximation, as\\n \\nwith the numerical differentiation.\\n \\nWhat happens when we introduce exponents to the function?\\n \\n \\n \\n \\nFig 7.13:\\n\\u200b\\n Derivative of quadratic function — calculation steps.\\n \\n \\nAnim 7.13:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/rok', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 153}),\n",
       " Document(page_content=\"Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n26\\n \\nFirst, we are applying the rule of a constant — we can move the coefficient (the value that\\n \\nmultiplies the other value) outside of the derivative. The rule for handling exponents is as follows:\\n \\ntake the exponent, in this case a \\n\\u200b\\n2\\n\\u200b\\n, and use it as a coefficient for the derived value, then, subtract 1\\n \\nfrom the exponent, as seen here: \\n\\u200b\\n2 - 1 = 1\\n\\u200b\\n.\\n \\nIf \\n\\u200b\\nf(x) = 3x\\n\\u200b\\n2\\n\\u200b\\n \\n\\u200b\\nthen \\n\\u200b\\nf'(x) = 3·2x\\n\\u200b\\n1 \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 154}),\n",
       " Document(page_content=\"f'(x) = 3·2x\\n\\u200b\\n1 \\n\\u200b\\nor simply \\n\\u200b\\n6x. \\n\\u200b\\nThis means the slope of the tangent line, at any point,\\n \\nx\\n\\u200b\\n, for this quadratic function, will be \\n\\u200b\\n6x\\n\\u200b\\n. As discussed with the numerical solution of the quadratic\\n \\nfunction differentiation, the derivative of a quadratic function depends on the \\n\\u200b\\nx\\n\\u200b\\n and in this case it\\n \\nequals \\n\\u200b\\n6x\\n\\u200b\\n:\\n \\n \\n \\nA commonly used operator in functions is addition, how do we calculate the derivative in this\\n \\ncase?\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 154}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n27\\n \\n \\nFig 7.14:\\n\\u200b\\n Derivative of quadratic function with addition — calculation steps.\\n \\n \\nAnim 7.14:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/mob\\n \\nThe derivative of a sum operation is the sum of derivatives, so we can split the derivative of a\\n \\nmore complex sum operation into a sum of the derivatives of each term of the equation and solve\\n \\nthe rest of the derivative using methods we already know.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 155}),\n",
       " Document(page_content='The derivative of a sum of functions equals their derivatives:\\n \\n \\n \\nIn this case, we’ve shown the rule using both notations.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 155}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n28\\n \\nLet’s try a couple more examples:\\n \\n \\n \\nFig 7.15:\\n\\u200b\\n Analytical derivative of multi-dimensional function example — calculation steps.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 156}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n29\\n \\n \\nAnim 7.15:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/tom\\n \\nThe derivative of a constant 5 equals 0, as we already discussed at the beginning of this chapter.\\n \\n \\nWe also have to apply the other rules that we’ve learned so far to perform this calculation.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 157}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n30\\n \\n \\nFig 7.16:\\n\\u200b\\n Analytical derivative of another multi-dimensional function example — calculation\\n \\n \\nsteps.\\n \\n \\nAnim 7.16:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/sun\\n \\nThis looks relatively straight-forward so far, but, with neural networks, we’ll work with functions\\n \\nthat take multiple parameters as inputs, so we’re going to calculate the partial derivatives as well.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 158}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n \\n \\n31\\n \\n \\nSummary\\n \\nLet’s summarize some of the solutions and rules that we have learned in this chapter.\\n \\nSolutions:\\n \\nThe derivative of a constant equals 0 (m is a constant in this case, as it’s not a parameter that we\\n \\nare deriving with respect to, which is \\n\\u200b\\nx\\n\\u200b\\n in this example):\\n \\n \\nThe derivative of \\n\\u200b\\nx\\n\\u200b\\n equals 1:\\n \\n \\n \\nThe derivative of a linear function equals its slope:\\n \\n \\n \\n \\n \\nRules:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 159}),\n",
       " Document(page_content='Rules:\\n \\nThe derivative of a constant multiple of the function equals the constant multiple of the function’s', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 159}),\n",
       " Document(page_content='Chapter 7 - Derivatives - Neural Networks from Scratch in Python\\n32 \\nderivative:  \\nThe derivative of a sum of functions equals the sum of their derivatives:  \\nThe same concept applies to subtraction:  \\nThe derivative of an exponentiation:  \\nWe used the value x instead of the whole function f(x) here since the derivative of an entire  \\nfunction is calculated a bit differently. We’ll explain this concept along with the chain rule in the  \\nnext chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 160}),\n",
       " Document(page_content='next chapter.  \\nSince we’ve already learned what derivatives are and how to calculate them analytically, which  \\nwe’ll later implement in code, we can go a step further and cover partial derivatives in the next  \\nchapter.  \\nSupplementary Material: \\u200bhttps://nnfs.io/ch7  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 160}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n6 \\nChapter 8\\nGradients, Partial Derivatives,  \\nand the Chain Rule  \\nTwo of the last pieces of the puzzle, before we continue coding our neural network, are the related  \\nconcepts of \\u200bgradients \\u200b and \\u200bpartial derivatives \\u200b. The derivatives that we’ve solved so far have been  \\ncases where there is only one independent variable in the function — that is, the result depended', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 161}),\n",
       " Document(page_content='solely on, in our case, \\u200bx \\u200b. However, our neural network consists, for example, of neurons, which  \\nhave multiple inputs. Each input gets multiplied by the corresponding weight (a function of 2  \\nparameters), and they get summed with the bias (a function of as many parameters as there are  \\ninputs, plus one for a bias). As we’ll explain soon in detail, to learn the impact of all of the inputs,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 161}),\n",
       " Document(page_content='weights, and biases to the neuron output and at the end of the loss function, we need to calculate  \\nthe derivative of each operation performed during the forward pass in the neuron and the whole  \\nmodel. To do that and get answers, we’ll need to use the \\u200bchain rule \\u200b, which we’ll explain soon in  \\nthis chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 161}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n7\\n \\n \\nThe Partial Derivative\\n \\nThe \\n\\u200b\\npartial derivative\\n\\u200b\\n measures how much impact a single input has on a function’s output. The\\n \\nmethod for calculating a partial derivative is the same as for derivatives explained in the previous\\n \\nchapter; we simply have to repeat this process for each of the independent inputs.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 162}),\n",
       " Document(page_content='Each of the function’s inputs has some impact on this function’s output, even if the impact is 0.\\n \\nWe need to know these impacts; this means that we have to calculate the derivative with respect\\n \\nto each input separately to learn about each of them. That’s why we call these partial derivatives\\n \\nwith respect to given input — we are calculating a partial of the derivative, related to a singular', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 162}),\n",
       " Document(page_content='input. The partial derivative is a single equation, and the full multivariate function’s derivative\\n \\nconsists of a set of equations called the \\n\\u200b\\ngradient\\n\\u200b\\n. In other words, the \\n\\u200b\\ngradient\\n\\u200b\\n is a vector of the\\n \\nsize of inputs containing partial derivative solutions with respect to each of the inputs. We’ll get\\n \\nback to gradients shortly.\\n \\nTo denote the partial derivative, we’ll be using Euler’s notation. It’s very similar to Leibniz’s', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 162}),\n",
       " Document(page_content='notation, as we only need to replace the differential operator \\n\\u200b\\nd\\n\\u200b\\n with \\n\\u200b\\n∂\\n\\u200b\\n. While the \\n\\u200b\\nd\\n\\u200b\\n operator might\\n \\nbe used to denote the differentiation of a multivariate function, its meaning is a bit different — it\\n \\ncan mean the rate of the function’s change in relation to the given input, but when other inputs\\n \\nmight change as well, and it is used mostly in physics. We are interested in the partial derivatives,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 162}),\n",
       " Document(page_content='a situation where we try to find the impact of the given input to the output while treating all of the\\n \\nother inputs as constants. We are interested in the impact of singular inputs since our goal, in the\\n \\nmodel, is to update parameters. The ∂ operator means explicitly that — the partial derivative:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 162}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n8\\n \\n \\nThe Partial Derivative of a Sum\\n \\nCalculating the partial derivative with respect to a given input means to calculate it like the\\n \\nregular derivative of one input, just while treating other inputs as constants. For example:\\n \\n \\n \\nFirst, we applied the sum rule — the derivative of a sum is the sum of derivatives. Then, we\\n \\nalready know that the derivative of \\n\\u200b\\nx\\n\\u200b\\n with respect to \\n\\u200b\\nx\\n\\u200b\\n equals \\n\\u200b\\n1', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 163}),\n",
       " Document(page_content='\\u200b\\nx\\n\\u200b\\n equals \\n\\u200b\\n1\\n\\u200b\\n. The new thing is the derivative of\\n \\ny\\n\\u200b\\n with respect to \\n\\u200b\\nx\\n\\u200b\\n. As we mentioned, \\n\\u200b\\ny\\n\\u200b\\n is treated as a constant, as it does not change when we are\\n \\nderiving with respect to \\n\\u200b\\nx\\n\\u200b\\n, and the derivative of a constant equals \\n\\u200b\\n0\\n\\u200b\\n. In the second case, we\\n \\nderived with respect to \\n\\u200b\\ny\\n\\u200b\\n, thus treating \\n\\u200b\\nx\\n\\u200b\\n as constant. Put another way, regardless of the value of y\\n \\nin this example, the slope of \\n\\u200b\\nx\\n\\u200b\\n does not depend on \\n\\u200b\\ny\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 163}),\n",
       " Document(page_content='\\u200b\\ny\\n\\u200b\\n. This will not always be the case, though, as\\n \\nwe will soon see.\\n \\nLet’s try another example:\\n \\n \\n \\nIn this example, we also applied the sum rule first, then moved constants to the outside of the\\n \\nderivatives and calculated what remained with respect to \\n\\u200b\\nx\\n\\u200b\\n and \\n\\u200b\\ny\\n\\u200b\\n individually. The only difference\\n \\nto the non-multivariate derivatives from the previous chapter is the “partial” part, which means', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 163}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n9\\n \\nwe are deriving with respect to each of the variables separately. Other than that, there is nothing\\n \\nnew here.\\n \\nLet’s try something seemingly more complicated:\\n \\n \\n \\nPretty straight-forward — we’re constantly applying the same rules over and over again, and we\\n \\ndid not add any new calculation or rules in this example.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 164}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n10\\n \\n \\nThe Partial Derivative of Multiplication\\n \\nBefore we move on, let’s introduce the partial derivative of multiplication operation:\\n \\n \\n \\nWe have already mentioned that we need to treat the other independent variables as constants, and\\n \\nwe also have learned that we can move constants to the outside of the derivative. That’s exactly', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 165}),\n",
       " Document(page_content='how we solve the calculation of the partial derivative of multiplication — we treat other variables\\n \\nas constants, like numbers, and move them outside of the derivative. It turns out that when we\\n \\nderive with respect to \\n\\u200b\\nx\\n\\u200b\\n, \\n\\u200b\\ny\\n\\u200b\\n is treated as a constant, and the result equals \\n\\u200b\\ny\\n\\u200b\\n multiplied by the\\n \\nderivative of \\n\\u200b\\nx\\n\\u200b\\n with respect to \\n\\u200b\\nx,\\n\\u200b\\n which is \\n\\u200b\\n1\\n\\u200b\\n. The whole derivative then results with \\n\\u200b\\ny\\n\\u200b\\n. The', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 165}),\n",
       " Document(page_content='\\u200b\\ny\\n\\u200b\\n. The\\n \\nintuition behind this example is when calculating the partial derivative with respect to \\n\\u200b\\nx\\n\\u200b\\n, every\\n \\nchange of \\n\\u200b\\nx\\n\\u200b\\n by \\n\\u200b\\n1\\n\\u200b\\n changes the function’s output by \\n\\u200b\\ny\\n\\u200b\\n. For example, if \\n\\u200b\\ny=3\\n\\u200b\\n and \\n\\u200b\\nx=1\\n\\u200b\\n, the result is\\n \\n1·3=3\\n\\u200b\\n. When we change \\n\\u200b\\nx\\n\\u200b\\n by \\n\\u200b\\n1\\n\\u200b\\n so \\n\\u200b\\ny=3\\n\\u200b\\n and \\n\\u200b\\nx=2\\n\\u200b\\n, the result is \\n\\u200b\\n2·3=6\\n\\u200b\\n. We changed \\n\\u200b\\nx\\n\\u200b\\n by \\n\\u200b\\n1\\n\\u200b\\n and the\\n \\nresult changed by \\n\\u200b\\n3\\n\\u200b\\n, by the \\n\\u200b\\ny\\n\\u200b\\n. That’s what the partial derivative of this function with respect to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 165}),\n",
       " Document(page_content='\\u200b\\nx\\n \\ntells us.\\n \\nLet’s introduce a third input variable and add multiplication of variables for another example:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 165}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n11\\n \\n \\n \\nThe only new operation here is, as mentioned, moving variables other than the one that we derive\\n \\nwith respect to, outside of the derivative. The results in this example appear more complicated,\\n \\nbut only because of the existence of other variables in them — variables that are treated as\\n \\nconstants during derivation. Equations of the derivatives are longer, but not necessarily more\\n \\ncomplicated.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 166}),\n",
       " Document(page_content='complicated.\\n \\nThe reason to learn about partial derivatives is we’ll be calculating the partial derivatives of\\n \\nmultivariate functions soon, an example of which is the neuron. From the code perspective and\\n \\nthe \\n\\u200b\\nDense\\n\\u200b\\n layer class, more specifically, the \\n\\u200b\\nforward\\n\\u200b\\n method of this class, we’re passing in a\\n \\nsingle variable — the input array, containing either a batch of samples or outputs from the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 166}),\n",
       " Document(page_content='previous layer. From the math perspective, each value of this single variable (an array) is a\\n \\nseparate input — it contains as many inputs as we have data in the input array. For example, if we\\n \\npass a vector of \\n\\u200b\\n4\\n\\u200b\\n values to the neuron, it’s a singular variable in the code, but \\n\\u200b\\n4\\n\\u200b\\n separate inputs in\\n \\nthe equation. This forms a function that takes multiple inputs. To learn about the impact that each', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 166}),\n",
       " Document(page_content='input makes to the function’s output, we’ll need to calculate the partial derivative of this function\\n \\nwith respect to each of its inputs, which we’ll explain in detail in the next chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 166}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n12\\n \\n \\nThe Partial Derivative of \\n\\u200b\\nMax\\n \\nDerivatives and partial derivatives are not limited to addition and multiplication operations, or\\n \\nconstants. We need to derive them for the other functions that we used in the forward pass, one of\\n \\nwhich is the derivative of the \\n\\u200b\\nmax()\\n\\u200b\\n function:\\n \\n \\n \\nThe max function returns the greatest input. We know that the derivative of \\n\\u200b\\nx\\n\\u200b\\n with respect to x', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 167}),\n",
       " Document(page_content='equals \\n\\u200b\\n1, \\n\\u200b\\nso the derivative of this function with respect to \\n\\u200b\\nx\\n\\u200b\\n equals 1 if \\n\\u200b\\nx \\n\\u200b\\nis greater than \\n\\u200b\\ny\\n\\u200b\\n, since the\\n \\nfunction will return \\n\\u200b\\nx\\n\\u200b\\n. In the other case, where \\n\\u200b\\ny\\n\\u200b\\n is greater than \\n\\u200b\\nx\\n\\u200b\\n and will get returned instead, the\\n \\nderivative of \\n\\u200b\\nmax()\\n\\u200b\\n with respect to \\n\\u200b\\nx\\n\\u200b\\n equals 0 — we treat \\n\\u200b\\ny\\n\\u200b\\n as a constant, and the derivative of \\n\\u200b\\ny\\n \\nwith respect to \\n\\u200b\\nx\\n\\u200b\\n equals 0. We can denote that as \\n\\u200b\\n1(x > y)\\n\\u200b\\n, which means \\n\\u200b\\n1\\n\\u200b\\n if the condition is met,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 167}),\n",
       " Document(page_content='and \\n\\u200b\\n0\\n\\u200b\\n otherwise. We could also calculate the partial derivative of \\n\\u200b\\nmax()\\n\\u200b\\n with respect to \\n\\u200b\\ny\\n\\u200b\\n, but we\\n \\nwon’t need it anywhere in this book.\\n \\nOne special case for the derivative of the \\n\\u200b\\nmax()\\n\\u200b\\n function is when we have only one variable\\n \\nparameter, and the other parameter is always constant at \\n\\u200b\\n0\\n\\u200b\\n. This means that we want whichever is\\n \\nbigger in return — \\n\\u200b\\n0\\n\\u200b\\n or the input value, effectively clipping the input value at \\n\\u200b\\n0\\n\\u200b\\n from the positive', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 167}),\n",
       " Document(page_content='side. Handling this is going to be useful when we calculate the derivative of the \\n\\u200b\\nReLU\\n\\u200b\\n activation\\n \\nfunction since that activation function is defined as \\n\\u200b\\nmax(x, 0)\\n\\u200b\\n:\\n \\n \\n \\nNotice that since this function takes a single parameter, we used the \\n\\u200b\\nd\\n\\u200b\\n operator instead of the \\n\\u200b\\n∂\\n \\nto calculate the non-partial derivative. In this case, the derivative is \\n\\u200b\\n1\\n\\u200b\\n when \\n\\u200b\\nx\\n\\u200b\\n is greater than \\n\\u200b\\n0,\\n \\notherwise, it’s \\n\\u200b\\n0\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 167}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n13\\n \\n \\nThe Gradient\\n \\nAs we mentioned at the beginning of this chapter, the gradient is a \\n\\u200b\\nvector\\n\\u200b\\n composed of all of the\\n \\npartial derivatives of a function, calculated with respect to each input variable.\\n \\nLet’s return to one of the partial derivatives of the sum operation that we calculated earlier:\\n \\n \\n \\nIf we calculate all of the partial derivatives, we can form a gradient of the function. Using', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 168}),\n",
       " Document(page_content='different notations, it looks as follows:\\n \\n \\n \\nThat’s all we have to know about the \\n\\u200b\\ngradient\\n\\u200b\\n - it’s a vector of all of the possible partial\\n \\nderivatives of the function, and we denote it using the \\n\\u200b\\n∇\\n\\u200b\\n — nabla symbol that looks like an\\n \\ninverted delta symbol.\\n \\nWe’ll be using \\n\\u200b\\nderivatives\\n\\u200b\\n of single-parameter functions and \\n\\u200b\\ngradients\\n\\u200b\\n of multivariate functions\\n \\nto perform \\n\\u200b\\ngradient descent\\n\\u200b\\n using the \\n\\u200b\\nchain rule, \\n\\u200b\\nor, in other words, to perform the \\n\\u200b\\nbackward', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 168}),\n",
       " Document(page_content='\\u200b\\nbackward\\n \\npass\\n\\u200b\\n, which is a part of the model training. How exactly we’ll do that is the subject of the next\\n \\nchapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 168}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n14\\n \\n \\nThe Chain Rule\\n \\nDuring the forward pass, we’re passing the data through the neurons, then through the activation\\n \\nfunction, then through the neurons in the next layer, then through another activation function, and\\n \\nso on. We’re calling a function with an input parameter, taking an output, and using that output as\\n \\nan input to another function. For this simple example, let’s take 2 functions: \\n\\u200b\\nf', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 169}),\n",
       " Document(page_content='\\u200b\\nf\\n\\u200b\\n and \\n\\u200b\\ng\\n\\u200b\\n:\\n \\n \\n \\nx is the input data, z is an output of the function f, but also an input for the function g, and y is an\\n \\noutput of the function g. We could write the same calculation as:\\n \\n \\n \\nIn this form, we do not use the intermediate z variable, showing that function g takes the output of\\n \\nfunction f directly as an input. This does not differ much from the above 2 equations but shows an', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 169}),\n",
       " Document(page_content='important property of functions chained this way — since x is an input to the function f and then\\n \\nthe output of the function f is an input to the function g, the output of the function g is influenced\\n \\nby x in some way, so there must exist a derivative which can inform us of this influence.\\n \\nThe forward pass through our model is a chain of functions similar to these examples. We are\\n \\npassing in samples, the data flows through all of the layers, and activation functions to form an', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 169}),\n",
       " Document(page_content='output. Let’s bring the equation and the code of the example model from chapter 1:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 169}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n15\\n \\n \\nFig 8.01:\\n\\u200b\\n Code for a forward pass of an example neural network model.\\n \\nIf you look closely, you’ll see that we are presenting the loss as a big function, or a chain of\\n \\nfunctions, of multiple inputs — input data, weights, and biases. We are passing input data to the\\n \\nfirst layer where we also have that layer’s weights and biases, then the outputs flow through the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 170}),\n",
       " Document(page_content='ReLU activation function, and another layer, which brings more weights and biases, and another\\n \\nReLU activation, up to the end — the output layer and softmax activation. The model output,\\n \\nalong with the targets, is passed to the loss function, which returns the model’s error. We can look\\n \\nat the loss function not only as a function that takes the model’s output and targets as parameters\\n \\nto produce the error, but also as a function that takes targets, samples, and all of the weights and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 170}),\n",
       " Document(page_content='biases as inputs if we chain all of the functions performed during the forward pass as we’ve just\\n \\nshown in the images. To improve loss, we need to learn how each weight and bias impacts it.\\n \\nHow to do that for a chain of functions? By using the chain rule. This rule says that the derivative\\n \\nof a function chain is a product of derivatives of all of the functions in this chain, for example:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 170}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n16\\n \\n \\nFirst, we wrote the derivative of the outer function, \\n\\u200b\\nf(g(x))\\n\\u200b\\n, with respect to the inner function, \\n\\u200b\\ng(x)\\n\\u200b\\n,\\n \\nas this inner function is its parameter. Next, we multiplied it by the derivative of the inner\\n \\nfunction, \\n\\u200b\\ng(x)\\n\\u200b\\n, with respect to its parameters, \\n\\u200b\\nx\\n\\u200b\\n. We also denoted this derivative using 2 different', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 171}),\n",
       " Document(page_content='notations. With 3 functions and multiple inputs, the partial derivative of this function with respect\\n \\nto \\n\\u200b\\nx\\n\\u200b\\n is as follows (we can’t use the prime notation in this case since we have to mention which\\n \\nvariable we are deriving with respect to):\\n \\n \\n \\nTo calculate the partial derivative of a chain of functions with respect to some parameter, we take\\n \\nthe partial derivative of the outer function with respect to the inner function in a chain to the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 171}),\n",
       " Document(page_content='parameter. Then multiply this partial derivative by the partial derivative of the inner function with\\n \\nrespect to the more inner function in a chain to the parameter, then multiply this by the partial\\n \\nderivative of the more inner function with respect to the other function in the chain. We repeat\\n \\nthis all the way down to the parameter in question. Notice, for example, how the middle\\n \\nderivative is with respect to \\n\\u200b\\nh(x, z)\\n\\u200b\\n and not \\n\\u200b\\ny\\n\\u200b\\n as \\n\\u200b\\nh(x, z)\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 171}),\n",
       " Document(page_content='\\u200b\\n as \\n\\u200b\\nh(x, z)\\n\\u200b\\n is in the chain to the parameter \\n\\u200b\\nx\\n\\u200b\\n. The\\n \\nchain rule\\n\\u200b\\n turns out to be the most important rule in finding the impact of singular input to the\\n \\noutput of a chain of functions, which is the calculation of loss in our case. We’ll use it again in the\\n \\nnext chapter when we discuss and code backpropagation. For now, let’s cover an example of the\\n \\nchain rule.\\n \\nLet’s solve the derivative of \\n\\u200b\\nh(x) = 3(2x\\n\\u200b\\n2\\n\\u200b\\n)\\n\\u200b\\n5\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 171}),\n",
       " Document(page_content='\\u200b\\n2\\n\\u200b\\n)\\n\\u200b\\n5\\n\\u200b\\n. The first thing that we can notice here is that we have\\n \\na complex function that can be split into two simpler functions. First is an equation part\\n \\ncontained inside the parentheses, which we can write as \\n\\u200b\\ng(x) = 2x\\n\\u200b\\n2\\n\\u200b\\n. That’s the inside function\\n \\nthat we exponentiate and multiply with the rest of the equation. The remaining part of the\\n \\nequation can then be written as \\n\\u200b\\nf(y) = 3(y)\\n\\u200b\\n5\\n\\u200b\\n. \\n\\u200b\\ny\\n\\u200b\\n in this case is what we denoted as \\n\\u200b\\ng(x)=2x\\n\\u200b\\n2\\n\\u200b\\n and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 171}),\n",
       " Document(page_content='\\u200b\\n2\\n\\u200b\\n and\\n \\nwhen we combine it back, we get \\n\\u200b\\nh(x) = f(g(x)) = 3(2x\\n\\u200b\\n2\\n\\u200b\\n)\\n\\u200b\\n5\\n\\u200b\\n \\n\\u200b\\n. To calculate a derivative of this\\n \\nfunction, we start by taking that outside exponent, the \\n\\u200b\\n5\\n\\u200b\\n, and place it in front of the component\\n \\nthat we are exponentiating to multiply it later by the leading 3, giving us 15. We then subtract 1\\n \\nfrom the \\n\\u200b\\n5\\n\\u200b\\n exponent, leaving us with a \\n\\u200b\\n4\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 171}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n17\\n \\nThen the chain rule informs us to multiply the above derivative of the outer function, with the\\n \\nderivative of the interior function, giving us:\\n \\n \\n \\nRecall that \\n\\u200b\\n4x\\n\\u200b\\n was the derivative of \\n\\u200b\\n2x\\n\\u200b\\n2\\n\\u200b\\n, which is the inner function, \\n\\u200b\\ng(x)\\n\\u200b\\n. This highlights the\\n \\nchain rule\\n\\u200b\\n concept in an example, allowing us to calculate the derivatives of more complex', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 172}),\n",
       " Document(page_content='functions by chaining together the derivatives. Note that we multiplied by the derivative of that\\n \\ninterior function, but left the interior function \\n\\u200b\\nunchanged \\n\\u200b\\nwithin the derivative of the outer\\n \\nfunction.\\n \\n \\nIn theory, we could just stop here with a perfectly-usable derivative of the function. We can enter\\n \\nsome input into \\n\\u200b\\n15(2x\\n\\u200b\\n2\\n\\u200b\\n)\\n\\u200b\\n4\\n\\u200b\\n · 4x \\n\\u200b\\nand get the answer. That said, we can also go ahead and simplify this', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 172}),\n",
       " Document(page_content='function for more practice. Coming back to the original problem, so far we’ve found:\\n \\n \\n \\nTo simplify this derivative function, we first take \\n\\u200b\\n(2x\\n\\u200b\\n2\\n\\u200b\\n)\\n\\u200b\\n4\\n\\u200b\\n and distribute the \\n\\u200b\\n4\\n\\u200b\\n exponent:\\n \\n \\n \\nCombine the \\n\\u200b\\nx’s\\n\\u200b\\n:\\n \\n \\n \\nAnd the constants:\\n \\n \\n \\nWe’ll simplify derivatives later as well for faster computation — there’s no reason to repeat the\\n \\nsame operations when we can solve them in advance.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 172}),\n",
       " Document(page_content='Hopefully, now you understand what derivatives and partial derivatives are, what the gradient is,\\n \\nwhat the derivative of the loss function with respect to weights and biases means, and how to use\\n \\nthe chain rule. For now, these terms might sound disconnected, but we’re going to use them all to\\n \\nperform gradient descent in the backpropagation step, which is the subject of the next chapters.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 172}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n \\n18\\n \\n \\nSummary\\n \\nLet’s summarize the rules that we have learned in this chapter.\\n \\nThe partial derivative of the sum with respect to any input equals 1:\\n \\n \\n \\nThe partial derivative of the multiplication operation with 2 inputs, with respect to any input,\\n \\nequals the other input:\\n \\n \\n \\nThe partial derivative of the max function of 2 variables with respect to any of them is 1 if this', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 173}),\n",
       " Document(page_content='variable is the biggest and 0 otherwise. An example of x:\\n \\n \\n \\nThe derivative of the max function of a single variable and 0 equals 1 if the variable is greater\\n \\nthan 0 and 0 otherwise:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 173}),\n",
       " Document(page_content='Chapter 8 - Gradients and Partial Derivatives - Neural Networks from Scratch in Python\\n19 \\nThe derivative of chained functions equals the product of the partial derivatives of the subsequent  \\nfunctions:  \\nThe same applies to the partial derivatives. For example:  \\nThe gradient is a vector of all possible partial derivatives. An example of a triple-input function:  \\nSupplementary Material: \\u200bhttps://nnfs.io/ch8  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 174}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n6\\n \\n \\n \\n \\n \\nChapter 9\\n \\nBackpropagation\\n \\nNow that we have an idea of how to measure the impact of variables on a function’s output, we\\n \\ncan begin to write the code to calculate these partial derivatives to see their role in minimizing\\n \\nthe model’s loss. Before applying this to a complete neural network, let’s start with a simplified', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 175}),\n",
       " Document(page_content='forward pass with just one neuron. Rather than backpropagating from the loss function for a full\\n \\nneural network, let’s backpropagate the ReLU function for a single neuron and act as if we\\n \\nintend to minimize the output for this single neuron. We’re first doing this only as a\\n \\ndemonstration to simplify the explanation, since minimizing the output from a ReLU activated\\n \\nneuron doesn’t serve any purpose other than as an exercise. Minimizing the loss value is our end', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 175}),\n",
       " Document(page_content='goal, but in this case, we’ll start by showing how we can leverage the chain rule with derivatives\\n \\nand partial derivatives to calculate the impact of each variable on the ReLU activated output.\\n \\nWe’ll also start by minimizing this more basic output before jumping to the full network and\\n \\noverall loss.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 175}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n7\\n \\nLet’s quickly recall the forward pass and atomic operations that we need to perform for this single\\n \\nneuron and ReLU activation. We’ll use an example neuron with 3 inputs, which means that it also\\n \\nhas 3 weights and a bias:\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 176}),\n",
       " Document(page_content='1.0  \\n\\u200b\\n# bias\\n \\n \\nWe then start with the first input, \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n, and the related weight, \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n:\\n \\n \\nFig 9.01:\\n\\u200b\\n Beginning a forward pass with the first input and weight.\\n \\nWe have to multiply the input by the weight:\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias\\n \\n \\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nprint\\n\\u200b\\n(xw0)\\n \\n \\n \\n>>>\\n \\n-\\n\\u200b\\n3.0\\n \\n \\nVisually:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 176}),\n",
       " Document(page_content='\\u200b\\n3.0\\n \\n \\nVisually:\\n \\n \\nFig 9.02:\\n\\u200b\\n The first input and weight multiplication.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 176}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n8\\n \\nWe repeat this operation for \\n\\u200b\\nx1\\n\\u200b\\n, \\n\\u200b\\nw1\\n\\u200b\\n and \\n\\u200b\\nx2\\n\\u200b\\n, \\n\\u200b\\nw2\\n\\u200b\\n pairs:\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\nprint\\n\\u200b\\n(xw1, xw2)\\n \\n \\n \\n>>>\\n \\n2.0 6.0\\n \\n \\nVisually:\\n \\n \\nFig 9.03:\\n\\u200b\\n Input and weight multiplication of all of the inputs.\\n \\nCode all together:\\n \\n# Forward pass\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 177}),\n",
       " Document(page_content='= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias\\n \\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\nprint\\n\\u200b\\n(xw0, xw1, xw2)\\n \\n \\n \\n>>>\\n \\n-\\n\\u200b\\n3.0 2.0 6.0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 177}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n9\\n \\nThe next operation to perform is a sum of all weighted inputs with a bias:\\n \\n# Forward pass\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias\\n \\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 178}),\n",
       " Document(page_content='\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\nprint\\n\\u200b\\n(xw0, xw1, xw2, b)\\n \\n \\n# Adding weighted inputs and a bias\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nxw0 \\n\\u200b\\n+ \\n\\u200b\\nxw1 \\n\\u200b\\n+ \\n\\u200b\\nxw2 \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\nprint\\n\\u200b\\n(z)\\n \\n \\n \\n>>>\\n \\n-\\n\\u200b\\n3.0 2.0 6.0 1.0\\n \\n6.0\\n \\n \\nFig 9.04:\\n\\u200b\\n Weighted inputs and bias addition.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 178}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n10\\n \\nThis forms the neuron’s output. The last step is to apply the ReLU activation function on this\\n \\noutput:\\n \\n# Forward pass\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias\\n \\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 179}),\n",
       " Document(page_content='1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\nprint\\n\\u200b\\n(xw0, xw1, xw2, b)\\n \\n \\n# Adding weighted inputs and a bias\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nxw0 \\n\\u200b\\n+ \\n\\u200b\\nxw1 \\n\\u200b\\n+ \\n\\u200b\\nxw2 \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\nprint\\n\\u200b\\n(z)\\n \\n \\n# ReLU activation function\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nmax\\n\\u200b\\n(z, \\n\\u200b\\n0\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(y)\\n \\n \\n \\n>>>\\n \\n-\\n\\u200b\\n3.0 2.0 6.0 1.0\\n \\n6.0\\n \\n6.0\\n \\n \\nFig 9.05:\\n\\u200b\\n ReLU activation applied to the neuron output.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 179}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n11\\n \\nThis is the full forward pass through a single neuron and a ReLU activation function. Let’s treat\\n \\nall of these chained functions as one big function which takes input values (\\n\\u200b\\nx\\n\\u200b\\n), weights (\\n\\u200b\\nw\\n\\u200b\\n), and\\n \\nbias (\\n\\u200b\\nb\\n\\u200b\\n), as inputs, and outputs \\n\\u200b\\ny\\n\\u200b\\n. This big function consists of multiple simpler functions — there\\n \\nis a multiplication of input values and weights, sum of these values and bias, as well as a \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 180}),\n",
       " Document(page_content='\\u200b\\nmax\\n \\nfunction as the ReLU activation — 3 chained functions in total:\\n \\nThe first step is to backpropagate our gradients by calculating derivatives and partial derivatives\\n \\nwith respect to each of our parameters and inputs. To do this, we’re going to use the \\n\\u200b\\nchain rule\\n\\u200b\\n.\\n \\nRecall that the chain rule for a function stipulates that the derivative for nested functions like\\n \\nf(g(x))\\n\\u200b\\n solves to:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 180}),\n",
       " Document(page_content='\\u200b\\n solves to:\\n \\n \\n \\nThis big function that we just mentioned can be, in the context of our neural network, loosely\\n \\ninterpreted as:\\n \\n \\n \\nOr in the form that matches code more precisely as:\\n \\n \\n \\nOur current task is to calculate how much each of the inputs, weights, and a bias impacts the\\n \\noutput. We’ll start by considering what we need to calculate for the partial derivative of \\n\\u200b\\nw\\n\\u200b\\n0\\n\\u200b\\n, for', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 180}),\n",
       " Document(page_content='\\u200b\\nw\\n\\u200b\\n0\\n\\u200b\\n, for\\n \\nexample. But first, let’s rewrite our equation to the form that will allow us to determine how to\\n \\ncalculate the derivatives more easily:\\n \\ny = ReLU(sum(mul(x\\n\\u200b\\n0\\n\\u200b\\n, w\\n\\u200b\\n0\\n\\u200b\\n), mul(x\\n\\u200b\\n1\\n\\u200b\\n, w\\n\\u200b\\n1\\n\\u200b\\n), mul(x\\n\\u200b\\n2\\n\\u200b\\n, w\\n\\u200b\\n2\\n\\u200b\\n), b))\\n \\nThe above equation contains 3 nested functions: \\n\\u200b\\nReLU\\n\\u200b\\n, a sum of weighted inputs and a bias, and\\n \\nmultiplications of the inputs and weights. To calculate the impact of the example weight, \\n\\u200b\\nw\\n\\u200b\\n0\\n\\u200b\\n, on', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 180}),\n",
       " Document(page_content='\\u200b\\nw\\n\\u200b\\n0\\n\\u200b\\n, on\\n \\nthe output, the chain rule tells us to calculate the derivative of \\n\\u200b\\nReLU\\n\\u200b\\n with respect to its parameter,\\n \\nwhich is the sum, then multiply it with the partial derivative of the sum operation with respect to\\n \\nits \\n\\u200b\\nmul(x\\n\\u200b\\n0\\n\\u200b\\n, w\\n\\u200b\\n0\\n\\u200b\\n)\\n\\u200b\\n input, as this input contains the parameter in question. Then, multiply this with the\\n \\npartial derivative of the multiplication operation with respect to the \\n\\u200b\\nx\\n\\u200b\\n0\\n\\u200b\\n input. Let’s see this in a\\n \\nsimplified equation:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 180}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n12\\n \\n \\nFor legibility, we did not denote the \\n\\u200b\\nReLU\\n\\u200b\\n() parameter, which is the full sum, and the sum\\n \\nparameters, which are all of the multiplications of inputs and weights. We excluded this because\\n \\nthe equation would be longer and harder to read. This equation shows that we have to calculate\\n \\nthe derivatives and partial derivatives of all of the atomic operations and multiply them to acquire\\n \\nthe impact that x\\n\\u200b\\n0\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 181}),\n",
       " Document(page_content='\\u200b\\n0\\n\\u200b\\n makes on the output. We can then repeat this to calculate all of the other\\n \\nremaining impacts. The derivatives with respect to the weights and a bias will inform us about\\n \\ntheir impact and will be used to update these weights and bias. The derivatives with respect to\\n \\ninputs are used to chain more layers by passing them to the previous function in the chain.\\n \\nWe’ll have multiple chained layers of neurons in the neural network model, followed by the loss', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 181}),\n",
       " Document(page_content='function. We want to know the impact of a given weight or bias on the loss. That means that we\\n \\nwill have to calculate the derivative of the loss function (which we’ll do later in this chapter) and\\n \\napply the chain rule with the derivatives of all activation functions and neurons in all of the\\n \\nconsecutive layers. The derivative with respect to the layer’s inputs, as opposed to the derivative', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 181}),\n",
       " Document(page_content='with respect to the weights and biases, is not used to update any parameters. Instead, it is used to\\n \\nchain to another layer (which is why we backpropagate to the previous layer in a chain).\\n \\nDuring the backward pass, we’ll calculate the derivative of the loss function, and use it to\\n \\nmultiply with the derivative of the activation function of the output layer, then use this result to\\n \\nmultiply by the derivative of the output layer, and so on, through all of the hidden layers and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 181}),\n",
       " Document(page_content='activation functions. Inside these layers, the derivative with respect to the weights and biases will\\n \\nform the gradients that we’ll use to update the weights and biases. The derivatives with respect to\\n \\ninputs will form the gradient to chain with the previous layer. This layer can calculate the impact\\n \\nof its weights and biases on the loss and backpropagate gradients on inputs further.\\n \\nFor this example, let’s assume that our neuron receives a gradient of \\n\\u200b\\n1\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 181}),\n",
       " Document(page_content='\\u200b\\n1\\n\\u200b\\n from the next layer. We’re\\n \\nmaking up this value for demonstration purposes, and a value of \\n\\u200b\\n1\\n\\u200b\\n won’t change the values, which\\n \\nmeans that we can more easily show all of the processes. We are going to use the color of red for\\n \\nderivatives:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 181}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n13\\n \\n \\nFig 9.06:\\n\\u200b\\n Initial gradient (received during backpropagation).\\n \\nRecall that the derivative of \\n\\u200b\\nReLU()\\n\\u200b\\n with respect to its input is \\n\\u200b\\n1,\\n\\u200b\\n if the input is greater than \\n\\u200b\\n0,\\n\\u200b\\n and\\n \\n0\\n\\u200b\\n otherwise:\\n \\n \\nWe can write that in Python as:\\n \\nrelu_dz \\n\\u200b\\n= \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\nif \\n\\u200b\\nz \\n\\u200b\\n> \\n\\u200b\\n0 \\n\\u200b\\nelse \\n\\u200b\\n0.\\n\\u200b\\n)\\n \\n \\nWhere the \\n\\u200b\\ndrelu_dz\\n\\u200b\\n means the derivative of the \\n\\u200b\\nReLU\\n\\u200b\\n function with respect to \\n\\u200b\\nz\\n\\u200b\\n — we used \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 182}),\n",
       " Document(page_content='\\u200b\\nz\\n\\u200b\\n — we used \\n\\u200b\\nz\\n \\ninstead of \\n\\u200b\\nx\\n\\u200b\\n from the equation since the equation denotes the \\n\\u200b\\nmax\\n\\u200b\\n function in general, and we are\\n \\napplying it to the neuron’s output, which is \\n\\u200b\\nz\\n\\u200b\\n.\\n \\nThe input value to the \\n\\u200b\\nReLU\\n\\u200b\\n function is \\n\\u200b\\n6\\n\\u200b\\n, so the derivative equals \\n\\u200b\\n1\\n\\u200b\\n. We have to use the chain\\n \\nrule and multiply this derivative with the derivative received from the next layer, which is \\n\\u200b\\n1\\n\\u200b\\n for\\n \\nthe purpose of this example:\\n \\n# Forward pass\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 182}),\n",
       " Document(page_content='\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 182}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n14\\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\n \\n# Adding weighted inputs and a bias\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nxw0 \\n\\u200b\\n+ \\n\\u200b\\nxw1 \\n\\u200b\\n+ \\n\\u200b\\nxw2 \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\n \\n# ReLU activation function\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nmax\\n\\u200b\\n(z, \\n\\u200b\\n0\\n\\u200b\\n)\\n \\n \\n# Backward pass\\n \\n \\n# The derivative from the next layer\\n \\ndvalue \\n\\u200b\\n= \\n\\u200b\\n1.0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 183}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\n1.0\\n \\n \\n# Derivative of ReLU and the chain rule\\n \\ndrelu_dz \\n\\u200b\\n= \\n\\u200b\\ndvalue \\n\\u200b\\n* \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\nif \\n\\u200b\\nz \\n\\u200b\\n> \\n\\u200b\\n0 \\n\\u200b\\nelse \\n\\u200b\\n0.\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(drelu_dz)\\n \\n \\n \\n>>>\\n \\n1.0\\n \\n \\nFig 9.07:\\n\\u200b\\n Derivative of the ReLU function and chain rule.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 183}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n15\\n \\nThis results with the derivative of \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n \\nFig 9.08:\\n\\u200b\\n ReLU and chain rule gradient.\\n \\nMoving backward through our neural network, what is the function that comes immediately\\n \\nbefore we perform the activation function?\\n \\nIt’s a sum of the weighted inputs and bias. This means that we want to calculate the partial\\n \\nderivative of the sum function, and then, using the chain rule, multiply this by the partial', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 184}),\n",
       " Document(page_content='derivative of the subsequent, outer, function, which is \\n\\u200b\\nReLU\\n\\u200b\\n. We’ll call these results the:\\n \\n-\\ndrelu_dxw0\\n\\u200b\\n — the partial \\n\\u200b\\nd\\n\\u200b\\nerivative of the \\n\\u200b\\nReLU\\n\\u200b\\n w.r.t. the first weighed input, \\n\\u200b\\nw\\n\\u200b\\n0\\n\\u200b\\nx\\n\\u200b\\n0\\n\\u200b\\n,\\n \\n-\\ndrelu_dxw1\\n\\u200b\\n — the partial \\n\\u200b\\nd\\n\\u200b\\nerivative of the \\n\\u200b\\nReLU\\n\\u200b\\n w.r.t. the second weighed input, \\n\\u200b\\nw\\n\\u200b\\n1\\n\\u200b\\nx\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n-\\ndrelu_dxw2\\n\\u200b\\n — the partial \\n\\u200b\\nd\\n\\u200b\\nerivative of the \\n\\u200b\\nReLU \\n\\u200b\\nw.r.t. the third weighed input, \\n\\u200b\\nw\\n\\u200b\\n2\\n\\u200b\\nx\\n\\u200b\\n2\\n\\u200b\\n,\\n \\n-\\ndrelu_db\\n\\u200b\\n — the partial \\n\\u200b\\nd\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 184}),\n",
       " Document(page_content='\\u200b\\nd\\n\\u200b\\nerivative of the \\n\\u200b\\nReLU\\n\\u200b\\n with respect to the bias, \\n\\u200b\\nb\\n\\u200b\\n.\\n \\nThe partial derivative of the sum operation is always \\n\\u200b\\n1\\n\\u200b\\n, no matter the inputs:\\n \\n \\n \\nThe weighted inputs and bias are summed at this stage. So we will calculate the partial derivatives\\n \\nof the sum operation with respect to each of these, multiplied by the partial derivative for the\\n \\nsubsequent function (using the chain rule), which is the \\n\\u200b\\nReLU\\n\\u200b\\n function, denoted by \\n\\u200b\\ndrelu_dz', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 184}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n16\\n \\nFor the first partial derivative:\\n \\ndsum_dxw0 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndrelu_dxw0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw0\\n \\n \\nTo be clear, the \\n\\u200b\\ndsum_dxw0\\n\\u200b\\n above means the partial \\n\\u200b\\nd\\n\\u200b\\nerivative of the \\n\\u200b\\nsum\\n\\u200b\\n with respect to the \\n\\u200b\\nx\\n \\n(input), \\n\\u200b\\nw\\n\\u200b\\neighted, for the \\n\\u200b\\n0\\n\\u200b\\nth pair of inputs and weights. \\n\\u200b\\n1\\n\\u200b\\n is the value of this partial derivative,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 185}),\n",
       " Document(page_content='which we multiply, using the chain rule, with the derivative of the subsequent function, which is\\n \\nthe \\n\\u200b\\nReLU\\n\\u200b\\n function.\\n \\nAgain, we have to apply the chain rule and multiply the derivative of the ReLU function with the\\n \\npartial derivative of the sum, with respect to the first weighted input:\\n \\n \\n \\n# Forward pass\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 185}),\n",
       " Document(page_content='1.0  \\n\\u200b\\n# bias\\n \\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\n \\n# Adding weighted inputs and a bias\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nxw0 \\n\\u200b\\n+ \\n\\u200b\\nxw1 \\n\\u200b\\n+ \\n\\u200b\\nxw2 \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\n \\n# ReLU activation function\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nmax\\n\\u200b\\n(z, \\n\\u200b\\n0\\n\\u200b\\n)\\n \\n \\n# Backward pass\\n \\n \\n# The derivative from the next layer\\n \\ndvalue \\n\\u200b\\n= \\n\\u200b\\n1.0\\n \\n \\n# Derivative of ReLU and the chain rule\\n \\ndrelu_dz \\n\\u200b\\n= \\n\\u200b\\ndvalue \\n\\u200b\\n* \\n\\u200b\\n(\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 185}),\n",
       " Document(page_content='dvalue \\n\\u200b\\n* \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\nif \\n\\u200b\\nz \\n\\u200b\\n> \\n\\u200b\\n0 \\n\\u200b\\nelse \\n\\u200b\\n0.\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(drelu_dz)\\n \\n \\n# Partial derivatives of the multiplication, the chain rule\\n \\ndsum_dxw0 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndrelu_dxw0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw0\\n \\nprint\\n\\u200b\\n(drelu_dxw0)\\n \\n \\n \\n>>>\\n \\n1.0\\n \\n1.0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 185}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n17\\n \\n \\nFig 9.09:\\n\\u200b\\n Partial derivative of the sum function w.r.t. the first weighted input; the chain rule.\\n \\nThis results with a partial derivative of 1 again:\\n \\n \\nFig 9.10:\\n\\u200b\\n The sum and chain rule gradient (for the first weighted input).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 186}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n18\\n \\nWe can then perform the same operation with the next weighed input:\\n \\ndsum_dxw1 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndrelu_dxw1 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw1\\n \\n \\n \\nFig 9.11:\\n\\u200b\\n Partial derivative of the sum function w.r.t. the second weighted input; the chain rule.\\n \\nWhich results with the next calculated partial derivative:\\n \\n \\nFig 9.12:\\n\\u200b\\n The sum and chain rule gradient (for the second weighted input).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 187}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n19\\n \\nAnd the last weighted input:\\n \\ndsum_dxw2 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndrelu_dxw2 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw2\\n \\n \\n \\nFig 9.13:\\n\\u200b\\n Partial derivative of the sum function w.r.t. the third weighted input; the chain rule.\\n \\n \\nFig 9.14:\\n\\u200b\\n The sum and chain rule gradient (for the third weighted input).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 188}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n20\\n \\nThen the bias:\\n \\ndsum_db \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndrelu_db \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_db\\n \\n \\n \\nFig 9.15:\\n\\u200b\\n Partial derivative of the sum function w.r.t. the bias; the chain rule.\\n \\n \\nFig 9.16:\\n\\u200b\\n The sum and chain rule gradient (for the bias).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 189}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n21\\n \\nLet’s add these partial derivatives, with the applied chain rule, to our code:\\n \\n# Forward pass\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias\\n \\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 190}),\n",
       " Document(page_content='\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\n \\n# Adding weighted inputs and a bias\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nxw0 \\n\\u200b\\n+ \\n\\u200b\\nxw1 \\n\\u200b\\n+ \\n\\u200b\\nxw2 \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\n \\n# ReLU activation function\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nmax\\n\\u200b\\n(z, \\n\\u200b\\n0\\n\\u200b\\n)\\n \\n \\n# Backward pass\\n \\n \\n# The derivative from the next layer\\n \\ndvalue \\n\\u200b\\n= \\n\\u200b\\n1.0\\n \\n \\n# Derivative of ReLU and the chain rule\\n \\ndrelu_dz \\n\\u200b\\n= \\n\\u200b\\ndvalue \\n\\u200b\\n* \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\nif \\n\\u200b\\nz \\n\\u200b\\n> \\n\\u200b\\n0 \\n\\u200b\\nelse \\n\\u200b\\n0.\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(drelu_dz)\\n \\n \\n# Partial derivatives of the multiplication, the chain rule\\n \\ndsum_dxw0 \\n\\u200b\\n= \\n\\u200b\\n1', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 190}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_dxw1 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_dxw2 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_db \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndrelu_dxw0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw0\\n \\ndrelu_dxw1 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw1\\n \\ndrelu_dxw2 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw2\\n \\ndrelu_db \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_db\\n \\nprint\\n\\u200b\\n(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\\n \\n \\n \\n>>>\\n \\n1.0\\n \\n1.0 1.0 1.0 1.0\\n \\n \\nContinuing backward, the function that comes before the sum is the multiplication of weights and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 190}),\n",
       " Document(page_content='inputs. The derivative for a product is whatever the input is being multiplied by. Recall:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 190}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n22\\n \\n \\nThe partial derivative of \\n\\u200b\\nf\\n\\u200b\\n with respect to \\n\\u200b\\nx\\n\\u200b\\n equals \\n\\u200b\\ny\\n\\u200b\\n. The partial derivative of \\n\\u200b\\nf\\n\\u200b\\n with respect to \\n\\u200b\\ny\\n \\nequals \\n\\u200b\\nx\\n\\u200b\\n. Following this rule, the partial derivative of the first \\n\\u200b\\nweighted input\\n\\u200b\\n with respect to the\\n \\ninput\\n\\u200b\\n equals the \\n\\u200b\\nweight\\n\\u200b\\n (the other input of this function). Then, we have to apply the chain rule', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 191}),\n",
       " Document(page_content='and multiply this partial derivative with the partial derivative of the subsequent function, which is\\n \\nthe sum (we just calculated its partial derivative earlier in this chapter):\\n \\ndmul_dx0 \\n\\u200b\\n= \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\ndrelu_dx0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw0 \\n\\u200b\\n* \\n\\u200b\\ndmul_dx0\\n \\n \\nThis means that we are calculating the partial derivative with respect to the \\n\\u200b\\nx\\n\\u200b\\n0\\n\\u200b\\n input, the value of\\n \\nwhich is \\n\\u200b\\nw\\n\\u200b\\n0\\n\\u200b\\n, and we are applying the chain rule with the derivative of the subsequent function,\\n \\nwhich is \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 191}),\n",
       " Document(page_content='which is \\n\\u200b\\ndrelu_dxw0\\n\\u200b\\n.\\n \\nThis is a good time to point out that, as we apply the chain rule in this way — working\\n \\nbackward by taking the \\n\\u200b\\nReLU()\\n\\u200b\\n derivative, taking the summing operation’s derivative,\\n \\nmultiplying both, and so on, this is a process called \\n\\u200b\\nbackpropagation\\n\\u200b\\n using the \\n\\u200b\\nchain rule\\n\\u200b\\n. As\\n \\nthe name implies, the resulting output function’s gradients are passed back through the neural', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 191}),\n",
       " Document(page_content='network, using multiplication of the gradient of subsequent functions from later layers with the\\n \\ncurrent one. Let’s add this partial derivative to the code and show it on the chart:\\n \\n# Forward pass\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias\\n \\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 191}),\n",
       " Document(page_content='w[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\n \\n# Adding weighted inputs and a bias\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nxw0 \\n\\u200b\\n+ \\n\\u200b\\nxw1 \\n\\u200b\\n+ \\n\\u200b\\nxw2 \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\n \\n# ReLU activation function\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nmax\\n\\u200b\\n(z, \\n\\u200b\\n0\\n\\u200b\\n)\\n \\n \\n# Backward pass', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 191}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n23\\n \\n# The derivative from the next layer\\n \\ndvalue \\n\\u200b\\n= \\n\\u200b\\n1.0\\n \\n \\n# Derivative of ReLU and the chain rule\\n \\ndrelu_dz \\n\\u200b\\n= \\n\\u200b\\ndvalue \\n\\u200b\\n* \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\nif \\n\\u200b\\nz \\n\\u200b\\n> \\n\\u200b\\n0 \\n\\u200b\\nelse \\n\\u200b\\n0.\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(drelu_dz)\\n \\n \\n# Partial derivatives of the multiplication, the chain rule\\n \\ndsum_dxw0 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_dxw1 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_dxw2 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_db \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndrelu_dxw0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw0\\n \\ndrelu_dxw1 \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 192}),\n",
       " Document(page_content='drelu_dxw1 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw1\\n \\ndrelu_dxw2 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw2\\n \\ndrelu_db \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_db\\n \\nprint\\n\\u200b\\n(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\\n \\n \\n# Partial derivatives of the multiplication, the chain rule\\n \\ndmul_dx0 \\n\\u200b\\n= \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\ndrelu_dx0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw0 \\n\\u200b\\n* \\n\\u200b\\ndmul_dx0\\n \\nprint\\n\\u200b\\n(drelu_dx0)\\n \\n \\n \\n>>>\\n \\n1.0\\n \\n1.0 1.0 1.0 1.0\\n \\n-\\n\\u200b\\n3.0\\n \\n \\nFig 9.17:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 192}),\n",
       " Document(page_content='3.0\\n \\n \\nFig 9.17:\\n\\u200b\\n Partial derivative of the multiplication function w.r.t. the first input; the chain rule.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 192}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n24\\n \\n \\nFig 9.18:\\n\\u200b\\n The multiplication and chain rule gradient (for the first input).\\n \\nWe perform the same operation for other inputs and weights:\\n \\n# Forward pass\\n \\nx \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n2.0\\n\\u200b\\n, \\n\\u200b\\n3.0\\n\\u200b\\n]  \\n\\u200b\\n# input values\\n \\nw \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n]  \\n\\u200b\\n# weights\\n \\nb \\n\\u200b\\n= \\n\\u200b\\n1.0  \\n\\u200b\\n# bias\\n \\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 193}),\n",
       " Document(page_content='xw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\n \\n# Adding weighted inputs and a bias\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nxw0 \\n\\u200b\\n+ \\n\\u200b\\nxw1 \\n\\u200b\\n+ \\n\\u200b\\nxw2 \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\n \\n# ReLU activation function\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nmax\\n\\u200b\\n(z, \\n\\u200b\\n0\\n\\u200b\\n)\\n \\n \\n# Backward pass\\n \\n \\n# The derivative from the next layer\\n \\ndvalue \\n\\u200b\\n= \\n\\u200b\\n1.0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 193}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n25\\n \\n# Derivative of ReLU and the chain rule\\n \\ndrelu_dz \\n\\u200b\\n= \\n\\u200b\\ndvalue \\n\\u200b\\n* \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\nif \\n\\u200b\\nz \\n\\u200b\\n> \\n\\u200b\\n0 \\n\\u200b\\nelse \\n\\u200b\\n0.\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(drelu_dz)\\n \\n \\n# Partial derivatives of the multiplication, the chain rule\\n \\ndsum_dxw0 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_dxw1 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_dxw2 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndsum_db \\n\\u200b\\n= \\n\\u200b\\n1\\n \\ndrelu_dxw0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw0\\n \\ndrelu_dxw1 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw1\\n \\ndrelu_dxw2 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 194}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw2\\n \\ndrelu_db \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_db\\n \\nprint\\n\\u200b\\n(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\\n \\n \\n# Partial derivatives of the multiplication, the chain rule\\n \\ndmul_dx0 \\n\\u200b\\n= \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\ndmul_dx1 \\n\\u200b\\n= \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\ndmul_dx2 \\n\\u200b\\n= \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\ndmul_dw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\ndmul_dw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\ndmul_dw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\ndrelu_dx0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw0 \\n\\u200b\\n* \\n\\u200b\\ndmul_dx0\\n \\ndrelu_dw0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw0 \\n\\u200b\\n* \\n\\u200b\\ndmul_dw0\\n \\ndrelu_dx1 \\n\\u200b\\n=', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 194}),\n",
       " Document(page_content='drelu_dx1 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw1 \\n\\u200b\\n* \\n\\u200b\\ndmul_dx1\\n \\ndrelu_dw1 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw1 \\n\\u200b\\n* \\n\\u200b\\ndmul_dw1\\n \\ndrelu_dx2 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw2 \\n\\u200b\\n* \\n\\u200b\\ndmul_dx2\\n \\ndrelu_dw2 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw2 \\n\\u200b\\n* \\n\\u200b\\ndmul_dw2\\n \\nprint\\n\\u200b\\n(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\\n \\n \\n \\n>>>\\n \\n1.0\\n \\n1.0 1.0 1.0 1.0\\n \\n-\\n\\u200b\\n3.0 1.0 \\n\\u200b\\n-\\n\\u200b\\n1.0 \\n\\u200b\\n-\\n\\u200b\\n2.0 2.0 3.0', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 194}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n26\\n \\n \\nFig 9.19:\\n\\u200b\\n Complete backpropagation graph.\\n \\n \\nAnim 9.01-9.19:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/pro\\n \\nThat’s the complete set of the activated neuron’s partial derivatives with respect to the inputs,\\n \\nweights and a bias.\\n \\nRecall the equation from the beginning of this chapter:\\n \\n \\n \\nSince we have the complete code and we are applying the chain rule from this equation, let’s', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 195}),\n",
       " Document(page_content='see what we can optimize in these calculations. We applied the chain rule to calculate the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 195}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n27\\n \\npartial derivative of the ReLU activation function with respect to the first input, \\n\\u200b\\nx\\n\\u200b\\n0\\n\\u200b\\n. In our code,\\n \\nlet’s take the related lines of the code and simplify them:\\n \\n \\ndrelu_dx0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw0 \\n\\u200b\\n* \\n\\u200b\\ndmul_dx0\\n \\n \\nwhere:\\n \\n \\ndmul_dx0 \\n\\u200b\\n= \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\n \\nthen:\\n \\n \\ndrelu_dx0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dxw0 \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\n \\nwhere:\\n \\n \\ndrelu_dxw0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw0\\n \\n \\nthen:\\n \\n \\ndrelu_dx0 \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 196}),\n",
       " Document(page_content='drelu_dx0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\ndsum_dxw0 \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\n \\nwhere:\\n \\n \\ndsum_dxw0 \\n\\u200b\\n= \\n\\u200b\\n1\\n \\n \\nthen:\\n \\n \\ndrelu_dx0 \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\n1 \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n= \\n\\u200b\\ndrelu_dz\\n\\u200b\\n \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\n \\nwhere:\\n \\n \\ndrelu_dz \\n\\u200b\\n= \\n\\u200b\\ndvalue \\n\\u200b\\n* \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\nif \\n\\u200b\\nz \\n\\u200b\\n> \\n\\u200b\\n0 \\n\\u200b\\nelse \\n\\u200b\\n0.\\n\\u200b\\n)\\n \\n \\nthen:\\n \\n \\ndrelu_dx0 \\n\\u200b\\n= \\n\\u200b\\ndvalue \\n\\u200b\\n* \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\nif \\n\\u200b\\nz \\n\\u200b\\n> \\n\\u200b\\n0 \\n\\u200b\\nelse \\n\\u200b\\n0.\\n\\u200b\\n) \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 196}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n28\\n \\n \\nFig 9.20:\\n\\u200b\\n How to apply the chain rule for the partial derivative of ReLU w.r.t. first input\\n \\n \\nFig 9.21:\\n\\u200b\\n The chain rule applied for the partial derivative of ReLU w.r.t. first input\\n \\n \\nAnim 9.20-9.21:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/com\\n \\nIn this equation, starting from the left-hand side, is the derivative calculated in the next layer, with', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 197}),\n",
       " Document(page_content='respect to its inputs — this is the gradient backpropagated to the current layer, which is the\\n \\nderivative of the \\n\\u200b\\nReLU\\n\\u200b\\n function, and the partial derivative of the neuron’s function with respect to\\n \\nthe \\n\\u200b\\nx\\n\\u200b\\n0\\n\\u200b\\n input. This is all multiplied by applying the chain rule to calculate the impact of the input to\\n \\nthe neuron on the whole function’s output.\\n \\nThe partial derivative of a neuron’s function, with respect to the weight, is the input related to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 197}),\n",
       " Document(page_content='this weight, and, with respect to the input, is the related weight. The partial derivative of the\\n \\nneuron’s function with respect to the bias is always 1. We multiply them with the derivative of\\n \\nthe subsequent function (which was \\n\\u200b\\n1\\n\\u200b\\n in this example) to get the final derivatives. We are going\\n \\nto code all of these derivatives in the Dense layer’s class and the ReLU activation class for the\\n \\nbackpropagation step.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 197}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n29\\n \\nAll together, the partial derivatives above, combined into a vector, make up our gradients. Our\\n \\ngradients could be represented as:\\n \\ndx \\n\\u200b\\n= \\n\\u200b\\n[drelu_dx0, drelu_dx1, drelu_dx2]  \\n\\u200b\\n# gradients on inputs\\n \\ndw \\n\\u200b\\n= \\n\\u200b\\n[drelu_dw0, drelu_dw1, drelu_dw2]  \\n\\u200b\\n# gradients on weights\\n \\ndb \\n\\u200b\\n= \\n\\u200b\\ndrelu_db  \\n\\u200b\\n# gradient on bias...just 1 bias here.\\n \\n \\nFor this single neuron example, we also won’t need our \\n\\u200b\\ndx\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 198}),\n",
       " Document(page_content='\\u200b\\ndx\\n\\u200b\\n. With many layers, we will continue\\n \\nbackpropagating to preceding layers with the partial derivative with respect to our inputs.\\n \\nContinuing the single neuron example, we can now apply these gradients to the weights to\\n \\nhopefully minimize the output. This is typically the purpose of the\\n\\u200b\\n optimizer \\n\\u200b\\n(discussed in the\\n \\nfollowing chapter), but we can show a simplified version of this task by directly applying a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 198}),\n",
       " Document(page_content='negative fraction of the gradient to our weights. We apply a negative fraction to this gradient\\n \\nsince we want to decrease the final output value, and the gradient shows the direction of the\\n \\nsteepest ascent. For example, our current weights and bias are:\\n \\nprint\\n\\u200b\\n(w, b)\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n-\\n\\u200b\\n3.0\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.0\\n\\u200b\\n, \\n\\u200b\\n2.0\\n\\u200b\\n] \\n\\u200b\\n1.0\\n \\n \\nWe can then apply a fraction of the gradients to these values:\\n \\nw[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n+= -\\n\\u200b\\n0.001 \\n\\u200b\\n* \\n\\u200b\\ndw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nw[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n+= -\\n\\u200b\\n0.001 \\n\\u200b\\n* \\n\\u200b\\ndw[\\n\\u200b\\n1', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 198}),\n",
       " Document(page_content='\\u200b\\n* \\n\\u200b\\ndw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nw[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n+= -\\n\\u200b\\n0.001 \\n\\u200b\\n* \\n\\u200b\\ndw[\\n\\u200b\\n2\\n\\u200b\\n]\\n \\nb \\n\\u200b\\n+= -\\n\\u200b\\n0.001 \\n\\u200b\\n* \\n\\u200b\\ndb\\n \\n \\nprint\\n\\u200b\\n(w, b)\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n-\\n\\u200b\\n3.001\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.998\\n\\u200b\\n, \\n\\u200b\\n1.997\\n\\u200b\\n] \\n\\u200b\\n0.999\\n \\n \\nNow, we’ve slightly changed the weights and bias in such a way so as to decrease the output\\n \\nsomewhat intelligently. We can see the effects of our tweaks on the output by doing another\\n \\nforward pass:\\n \\n# Multiplying inputs by weights\\n \\nxw0 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\nxw1 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 198}),\n",
       " Document(page_content='= \\n\\u200b\\nx[\\n\\u200b\\n1\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\nxw2 \\n\\u200b\\n= \\n\\u200b\\nx[\\n\\u200b\\n2\\n\\u200b\\n] \\n\\u200b\\n* \\n\\u200b\\nw[\\n\\u200b\\n2\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 198}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n30\\n \\n# Adding\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nxw0 \\n\\u200b\\n+ \\n\\u200b\\nxw1 \\n\\u200b\\n+ \\n\\u200b\\nxw2 \\n\\u200b\\n+ \\n\\u200b\\nb\\n \\n \\n# ReLU activation function\\n \\ny \\n\\u200b\\n= \\n\\u200b\\nmax\\n\\u200b\\n(z, \\n\\u200b\\n0\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(y)\\n \\n \\n \\n>>>\\n \\n5.985\\n \\n \\nWe’ve successfully decreased this neuron’s output from 6.000 to 5.985. Note that it does not\\n \\nmake sense to decrease the neuron’s output in a real neural network; we were doing this purely as', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 199}),\n",
       " Document(page_content='a simpler exercise than the full network. We want to decrease the loss value, which is the last\\n \\ncalculation in the chain of calculations during the forward pass, and it’s the first one to calculate\\n \\nthe gradient during the backpropagation. We’ve minimized the ReLU output of a single neuron\\n \\nonly for the purpose of this example to show that we actually managed to decrease the value of\\n \\nchained functions intelligently using the derivatives, partial derivatives, and chain rule. Now,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 199}),\n",
       " Document(page_content='we’ll apply the one-neuron example to the list of samples and expand it to an entire layer of\\n \\nneurons. To begin, let’s set a list of 3 samples for input, where each sample consists of 4 features.\\n \\nFor this example, our network will consist of a single hidden layer, containing 3 neurons (lists of\\n \\n3 weight sets and 3 biases). We’re not going to describe the forward pass again, but the backward\\n \\npass, in this case, needs further explanation.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 199}),\n",
       " Document(page_content='So far, we have performed an example backward pass with a single neuron, which received a\\n \\nsingular derivative to apply the chain rule. Let’s consider multiple neurons in the following\\n \\nlayer. A single neuron of the current layer connects to all of them — they all receive the output\\n \\nof this neuron. What will happen during backpropagation? Each neuron from the next layer will\\n \\nreturn a partial derivative of its function with respect to this input. The neuron in the current', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 199}),\n",
       " Document(page_content='layer will receive a vector consisting of these derivatives. We need this to be a singular value for\\n \\na singular neuron. To continue backpropagation, we need to sum this vector.\\n \\nNow, let’s replace the current singular neuron with a layer of neurons. As opposed to a single\\n \\nneuron, a layer outputs a vector of values instead of a singular value. Each neuron in a layer\\n \\nconnects to all of the neurons in the next layer. During backpropagation, each neuron from the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 199}),\n",
       " Document(page_content='current layer will receive a vector of partial derivatives the same way that we described for a\\n \\nsingle neuron. With a layer of neurons, it’ll take the form of a list of these vectors, or a 2D array.\\n \\nWe know that we need to perform a sum, but what should we sum and what is the result supposed\\n \\nto be? Each neuron is going to output a gradient of the partial derivatives with respect to all of its', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 199}),\n",
       " Document(page_content='inputs, and all neurons will form a list of these vectors. We need to sum along the inputs — the\\n \\nfirst input to all of the neurons, the second input, and so on. We’ll have to sum columns.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 199}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n31\\n \\nTo calculate the partial derivatives with respect to inputs, we need the weights — the partial\\n \\nderivative with respect to the input equals the related weight. This means that the array of\\n \\npartial derivatives with respect to all of the inputs equals the array of weights. Since this array is\\n \\ntransposed, we’ll need to sum its rows instead of columns. To apply the chain rule, we need to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 200}),\n",
       " Document(page_content='multiply them by the gradient from the subsequent function.\\n \\nIn the code to show this, we take the transposed weights, which are the transposed array of the\\n \\nderivatives with respect to inputs, and multiply them by their respective gradients (related to\\n \\ngiven neurons) to apply the chain rule. Then we sum along with the inputs. Then we calculate\\n \\nthe gradient for the next layer in backpropagation. The “next”  layer in backpropagation is the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 200}),\n",
       " Document(page_content=\"previous layer in the order of creation of the model:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Passed in gradient from the next layer\\n \\n# for the purpose of this example we're going to use\\n \\n# a vector of 1s\\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n]])\\n \\n \\n# We have 3 sets of weights - one set for each neuron\\n \\n# we have 4 inputs, thus 4 weights\\n \\n# recall that we keep weights transposed\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 200}),\n",
       " Document(page_content='0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]).T\\n \\n \\n# sum weights of given input\\n \\n# and multiply by the passed in gradient for this neuron\\n \\ndx0 \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(weights[\\n\\u200b\\n0\\n\\u200b\\n])\\n\\u200b\\n*\\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\ndx1 \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(weights[\\n\\u200b\\n1\\n\\u200b\\n])\\n\\u200b\\n*\\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\ndx2 \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(weights[\\n\\u200b\\n2\\n\\u200b\\n])\\n\\u200b\\n*\\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\ndx3 \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(weights[\\n\\u200b\\n3\\n\\u200b\\n])\\n\\u200b\\n*', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 200}),\n",
       " Document(page_content='\\u200b\\n3\\n\\u200b\\n])\\n\\u200b\\n*\\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n]\\n \\n \\ndinputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([dx0, dx1, dx2, dx3])\\n \\n \\nprint\\n\\u200b\\n(dinputs)\\n \\n \\n \\n>>>\\n \\n[ \\n\\u200b\\n0.44 \\n\\u200b\\n-\\n\\u200b\\n0.38 \\n\\u200b\\n-\\n\\u200b\\n0.07  1.37\\n\\u200b\\n]\\n \\n \\ndinputs\\n\\u200b\\n is a gradient of the neuron function with respect to inputs.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 200}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n32\\n \\nWe defined the gradient of the subsequent function (dvalues) as a row vector, which we’ll explain\\n \\nshortly. From NumPy’s perspective, and since both weights and dvalues are NumPy arrays, we\\n \\ncan simplify the dx0 to dx3 calculation. Since the weights array is formatted so that the rows\\n \\ncontain weights related to each input (weights for all neurons for the given input), we can', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 201}),\n",
       " Document(page_content=\"multiply them by the gradient vector directly:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Passed in gradient from the next layer\\n \\n# for the purpose of this example we're going to use\\n \\n# a vector of 1s\\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n]])\\n \\n \\n# We have 3 sets of weights - one set for each neuron\\n \\n# we have 4 inputs, thus 4 weights\\n \\n# recall that we keep weights transposed\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 201}),\n",
       " Document(page_content='\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]).T\\n \\n \\n# sum weights of given input\\n \\n# and multiply by the passed in gradient for this neuron\\n \\ndx0 \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(weights[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n])\\n \\ndx1 \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(weights[\\n\\u200b\\n1\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n])\\n \\ndx2 \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(weights[\\n\\u200b\\n2\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n])\\n \\ndx3 \\n\\u200b\\n= \\n\\u200b\\nsum\\n\\u200b\\n(weights[\\n\\u200b\\n3\\n\\u200b\\n]\\n\\u200b\\n*\\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n])\\n \\n \\ndinputs \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 201}),\n",
       " Document(page_content='dinputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([dx0, dx1, dx2, dx3])\\n \\n \\nprint\\n\\u200b\\n(dinputs)\\n \\n \\n \\n>>>\\n \\n[ \\n\\u200b\\n0.44 \\n\\u200b\\n-\\n\\u200b\\n0.38 \\n\\u200b\\n-\\n\\u200b\\n0.07  1.37\\n\\u200b\\n]\\n \\n \\nYou might already see where we are going with this — the sum of the multiplication of the\\n \\nelements is the dot product. We can achieve the same result by using the \\n\\u200b\\nnp.dot\\n\\u200b\\n function. For\\n \\nthis to be possible, we need to match the “inner” shapes and decide the first dimension of the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 201}),\n",
       " Document(page_content='result, which is the first dimension of the first parameter. We want the output of this calculation to\\n \\nbe of the shape of the gradient from the subsequent function — recall that we have one partial\\n \\nderivative for each neuron and multiply it by the neuron’s partial derivative with respect to its\\n \\ninput. We then want to multiply each of these gradients with each of the partial derivatives that\\n \\nare related to this neuron’s inputs, and we already noticed that they are rows. The dot product', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 201}),\n",
       " Document(page_content='takes rows from the first argument and columns from the second to perform multiplication and\\n \\nsum; thus, we need to transpose the weights for this calculation:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 201}),\n",
       " Document(page_content=\"Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n33\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Passed in gradient from the next layer\\n \\n# for the purpose of this example we're going to use\\n \\n# a vector of 1s\\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n]])\\n \\n \\n# We have 3 sets of weights - one set for each neuron\\n \\n# we have 4 inputs, thus 4 weights\\n \\n# recall that we keep weights transposed\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 202}),\n",
       " Document(page_content='0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]).T\\n \\n \\n# sum weights of given input\\n \\n# and multiply by the passed in gradient for this neuron\\n \\ndinputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(dvalues[\\n\\u200b\\n0\\n\\u200b\\n], weights.T)\\n \\n \\nprint\\n\\u200b\\n(dinputs)\\n \\n \\n \\n>>>\\n \\n[ \\n\\u200b\\n0.44 \\n\\u200b\\n-\\n\\u200b\\n0.38 \\n\\u200b\\n-\\n\\u200b\\n0.07  1.37\\n\\u200b\\n]\\n \\n \\nWe have to account for one more thing — a batch of samples. So far, we have been using a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 202}),\n",
       " Document(page_content='single sample responsible for a single gradient vector that is backpropagated between layers. The\\n \\nrow vector that we created for \\n\\u200b\\ndvalues\\n\\u200b\\n is in preparation for a batch of data. With more\\n \\nsamples, the layer will return a list of gradients, which we \\n\\u200b\\nalmost\\n\\u200b\\n handle correctly for. Let’s\\n \\nreplace the singular gradient \\n\\u200b\\ndvalues[\\n\\u200b\\n0\\n\\u200b\\n]\\n\\u200b\\n with a full list of gradients, \\n\\u200b\\ndvalues\\n\\u200b\\n, and add more\\n \\nexample gradients to this list:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 202}),\n",
       " Document(page_content=\"\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Passed in gradient from the next layer\\n \\n# for the purpose of this example we're going to use\\n \\n# an array of an incremental gradient values\\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n2.\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n3.\\n\\u200b\\n, \\n\\u200b\\n3.\\n\\u200b\\n, \\n\\u200b\\n3.\\n\\u200b\\n]])\\n \\n \\n# We have 3 sets of weights - one set for each neuron\\n \\n# we have 4 inputs, thus 4 weights\\n \\n# recall that we keep weights transposed\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 202}),\n",
       " Document(page_content='= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]).T', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 202}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n34\\n \\n# sum weights of given input\\n \\n# and multiply by the passed in gradient for this neuron\\n \\ndinputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(dvalues, weights.T)\\n \\n \\nprint\\n\\u200b\\n(dinputs)\\n \\n \\n \\n>>>\\n \\n[[ \\n\\u200b\\n0.44 \\n\\u200b\\n-\\n\\u200b\\n0.38 \\n\\u200b\\n-\\n\\u200b\\n0.07  1.37\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0.88 \\n\\u200b\\n-\\n\\u200b\\n0.76 \\n\\u200b\\n-\\n\\u200b\\n0.14  2.74\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n1.32 \\n\\u200b\\n-\\n\\u200b\\n1.14 \\n\\u200b\\n-\\n\\u200b\\n0.21  4.11\\n\\u200b\\n]]\\n \\n \\nCalculating the gradients with respect to weights is very similar, but, in this case, we’re going to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 203}),\n",
       " Document(page_content='be using gradients to update the weights, so we need to match the shape of weights, not inputs.\\n \\nSince the derivative with respect to the weights equals inputs, weights are transposed, so we need\\n \\nto transpose inputs to receive the derivative of the neuron with respect to weights. Then we use\\n \\nthese transposed inputs as the first parameter to the dot product — the dot product is going to\\n \\nmultiply rows by inputs, where each row, as it is transposed, contains data for a given input for all', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 203}),\n",
       " Document(page_content=\"of the samples, by the columns of \\n\\u200b\\ndvalues\\n\\u200b\\n. These columns are related to the outputs of singular\\n \\nneurons for all of the samples, so the result will contain an array with the shape of the weights,\\n \\ncontaining the gradients with respect to the inputs, multiplied with the incoming gradient for all of\\n \\nthe samples in the batch:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Passed in gradient from the next layer\\n \\n# for the purpose of this example we're going to use\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 203}),\n",
       " Document(page_content='# an array of an incremental gradient values\\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n2.\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n3.\\n\\u200b\\n, \\n\\u200b\\n3.\\n\\u200b\\n, \\n\\u200b\\n3.\\n\\u200b\\n]])\\n \\n \\n# We have 3 sets of inputs - samples\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n],\\n \\n                   [\\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n5.\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n],\\n \\n                   [\\n\\u200b\\n-\\n\\u200b\\n1.5\\n\\u200b\\n, \\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.8\\n\\u200b\\n]])\\n \\n \\n# sum weights of given input', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 203}),\n",
       " Document(page_content='# and multiply by the passed in gradient for this neuron\\n \\ndweights \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs.T, dvalues)\\n \\n \\nprint\\n\\u200b\\n(dweights)\\n \\n \\n \\n>>>\\n \\n[[ \\n\\u200b\\n0.5  0.5  0.5\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n20.1 20.1 20.1\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n10.9 10.9 10.9\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n4.1  4.1  4.1\\n\\u200b\\n]]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 203}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n35\\n \\nThis output’s shape matches the shape of weights because we summed the inputs for each weight\\n \\nand then multiplied them by the input gradient. \\n\\u200b\\ndweights\\n\\u200b\\n is a gradient of the neuron function\\n \\nwith respect to the weights.\\n \\nFor the biases and derivatives with respect to them, the derivatives come from the sum operation', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 204}),\n",
       " Document(page_content=\"and always equal 1, multiplied by the incoming gradients to apply the chain rule. Since gradients\\n \\nare a list of gradients (a vector of gradients for each neuron for all samples), we just have to sum\\n \\nthem with the neurons, column-wise, along axis 0.\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Passed in gradient from the next layer\\n \\n# for the purpose of this example we're going to use\\n \\n# an array of an incremental gradient values\\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n],\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 204}),\n",
       " Document(page_content='1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n2.\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n3.\\n\\u200b\\n, \\n\\u200b\\n3.\\n\\u200b\\n, \\n\\u200b\\n3.\\n\\u200b\\n]])\\n \\n \\n# One bias for each neuron\\n \\n# biases are the row vector with a shape (1, neurons)\\n \\nbiases \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n]])\\n \\n \\n# dbiases - sum values, do this over samples (first axis), keepdims\\n \\n# since this by default will produce a plain list -\\n \\n# we explained this in the chapter 4\\n \\ndbiases \\n\\u200b\\n= \\n\\u200b\\nnp.sum(dvalues, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 204}),\n",
       " Document(page_content='axis\\n\\u200b\\n=\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n \\nprint\\n\\u200b\\n(dbiases)\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n6. 6. 6.\\n\\u200b\\n]]\\n \\n \\nkeepdims\\n\\u200b\\n lets us keep the gradient as a row vector — recall the shape of biases array.\\n \\nThe last thing to cover here is the derivative of the ReLU function. It equals \\n\\u200b\\n1\\n\\u200b\\n if the input is\\n \\ngreater than \\n\\u200b\\n0\\n\\u200b\\n and \\n\\u200b\\n0\\n\\u200b\\n otherwise. The layer passes its outputs through the \\n\\u200b\\nReLU()\\n\\u200b\\n activation during\\n \\nthe forward pass. For the backward pass, \\n\\u200b\\nReLU()\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 204}),\n",
       " Document(page_content='\\u200b\\nReLU()\\n\\u200b\\n receives a gradient of the same shape. The\\n \\nderivative of the ReLU function will form an array of the same shape, filled with 1 when the\\n \\nrelated input is greater than 0, and 0 otherwise. To apply the chain rule, we need to multiply this\\n \\narray with the gradients of the following function:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 204}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n36\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Example layer output\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n4\\n\\u200b\\n],\\n \\n              [\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n7\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n],\\n \\n              [\\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n]])\\n \\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n4\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n5\\n\\u200b\\n, \\n\\u200b\\n6\\n\\u200b\\n, \\n\\u200b\\n7\\n\\u200b\\n, \\n\\u200b\\n8\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n9\\n\\u200b\\n, \\n\\u200b\\n10\\n\\u200b\\n, \\n\\u200b\\n11', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 205}),\n",
       " Document(page_content=\"\\u200b\\n, \\n\\u200b\\n10\\n\\u200b\\n, \\n\\u200b\\n11\\n\\u200b\\n, \\n\\u200b\\n12\\n\\u200b\\n]])\\n \\n \\n# ReLU activation's derivative\\n \\ndrelu \\n\\u200b\\n= \\n\\u200b\\nnp.zeros_like(z)\\n \\ndrelu[z \\n\\u200b\\n> \\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n= \\n\\u200b\\n1\\n \\n \\nprint\\n\\u200b\\n(drelu)\\n \\n \\n# The chain rule\\n \\ndrelu \\n\\u200b\\n*= \\n\\u200b\\ndvalues\\n \\n \\nprint\\n\\u200b\\n(drelu)\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n1 1 0 0\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n1 0 0 1\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0 1 1 0\\n\\u200b\\n]]\\n \\n[[ \\n\\u200b\\n1  2  0  0\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n5  0  0  8\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0 10 11  0\\n\\u200b\\n]]\\n \\n \\nTo calculate the ReLU derivative, we created an array filled with zeros. \\n\\u200b\\nnp.zeros_like\\n\\u200b\\n is a\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 205}),\n",
       " Document(page_content='\\u200b\\n is a\\n \\nNumPy function that creates an array filled with zeros, with the shape of the array from its\\n \\nparameter, the \\n\\u200b\\nz\\n\\u200b\\n array in our case, which is an example output of the neuron. Following the\\n \\nReLU()\\n\\u200b\\n derivative, we then set the values related to the inputs greater than \\n\\u200b\\n0 \\n\\u200b\\nas 1.  We then\\n \\nprint this table to see and compare it to the gradients. In the end, we multiply this array with\\n \\nthe gradient of the subsequent function and print the result.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 205}),\n",
       " Document(page_content='We can now simplify this operation. Since the \\n\\u200b\\nReLU()\\n\\u200b\\n derivative array is filled with 1s, which do\\n \\nnot change the values multiplied by them, and 0s that zero the multiplying value, this means that\\n \\nwe can take the gradients of the subsequent function and set to 0 all of the values that correspond\\n \\nto the \\n\\u200b\\nReLU()\\n\\u200b\\n input and are equal to or less than 0:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 205}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n37\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Example layer output\\n \\nz \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n4\\n\\u200b\\n],\\n \\n              [\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n7\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n],\\n \\n              [\\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n]])\\n \\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n4\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n5\\n\\u200b\\n, \\n\\u200b\\n6\\n\\u200b\\n, \\n\\u200b\\n7\\n\\u200b\\n, \\n\\u200b\\n8\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n9\\n\\u200b\\n, \\n\\u200b\\n10\\n\\u200b\\n, \\n\\u200b\\n11', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 206}),\n",
       " Document(page_content=\"\\u200b\\n, \\n\\u200b\\n10\\n\\u200b\\n, \\n\\u200b\\n11\\n\\u200b\\n, \\n\\u200b\\n12\\n\\u200b\\n]])\\n \\n \\n# ReLU activation's derivative\\n \\n# with the chain rule applied\\n \\ndrelu \\n\\u200b\\n= \\n\\u200b\\ndvalues.copy()\\n \\ndrelu[z \\n\\u200b\\n<= \\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n= \\n\\u200b\\n0\\n \\n \\nprint\\n\\u200b\\n(drelu)\\n \\n \\n \\n>>>\\n \\n[[ \\n\\u200b\\n1  2  0  0\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n5  0  0  8\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0 10 11  0\\n\\u200b\\n]]\\n \\n \\nThe copy of \\n\\u200b\\ndvalues\\n\\u200b\\n ensures that we don’t modify it during the ReLU derivative calculation.\\n \\nLet’s combine the forward and backward pass of a single neuron with a full layer and batch-based\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 206}),\n",
       " Document(page_content=\"partial derivatives. We’ll minimize ReLU’s output, once again, only for this example:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\n# Passed in gradient from the next layer\\n \\n# for the purpose of this example we're going to use\\n \\n# an array of an incremental gradient values\\n \\ndvalues \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n2.\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n3.\\n\\u200b\\n, \\n\\u200b\\n3.\\n\\u200b\\n, \\n\\u200b\\n3.\\n\\u200b\\n]])\\n \\n \\n# We have 3 sets of inputs - samples\\n \\ninputs \\n\\u200b\\n= \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 206}),\n",
       " Document(page_content='inputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n2.5\\n\\u200b\\n],\\n \\n                   [\\n\\u200b\\n2.\\n\\u200b\\n, \\n\\u200b\\n5.\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n],\\n \\n                   [\\n\\u200b\\n-\\n\\u200b\\n1.5\\n\\u200b\\n, \\n\\u200b\\n2.7\\n\\u200b\\n, \\n\\u200b\\n3.3\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.8\\n\\u200b\\n]])\\n \\n# We have 3 sets of weights - one set for each neuron\\n \\n# we have 4 inputs, thus 4 weights\\n \\n# recall that we keep weights transposed\\n \\nweights \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.2\\n\\u200b\\n, \\n\\u200b\\n0.8\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.91\\n\\u200b\\n, \\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 206}),\n",
       " Document(page_content='\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.5\\n\\u200b\\n],\\n \\n                    [\\n\\u200b\\n-\\n\\u200b\\n0.26\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.27\\n\\u200b\\n, \\n\\u200b\\n0.17\\n\\u200b\\n, \\n\\u200b\\n0.87\\n\\u200b\\n]]).T', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 206}),\n",
       " Document(page_content=\"Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n38\\n \\n# One bias for each neuron\\n \\n# biases are the row vector with a shape (1, neurons)\\n \\nbiases \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n]])\\n \\n \\n# Forward pass\\n \\nlayer_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, weights) \\n\\u200b\\n+ \\n\\u200b\\nbiases  \\n\\u200b\\n# Dense layer\\n \\nrelu_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.maximum(\\n\\u200b\\n0\\n\\u200b\\n, layer_outputs)  \\n\\u200b\\n# ReLU activation\\n \\n \\n# Let's optimize and test backpropagation here\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 207}),\n",
       " Document(page_content='# ReLU activation - simulates derivative with respect to input values\\n \\n# from next layer passed to current layer during backpropagation\\n \\ndrelu \\n\\u200b\\n= \\n\\u200b\\nrelu_outputs.copy()\\n \\ndrelu[layer_outputs \\n\\u200b\\n<= \\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n= \\n\\u200b\\n0\\n \\n \\n# Dense layer\\n \\n# dinputs - multiply by weights\\n \\ndinputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(drelu, weights.T)\\n \\n# dweights - multiply by inputs\\n \\ndweights \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs.T, drelu)\\n \\n# dbiases - sum values, do this over samples (first axis), keepdims', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 207}),\n",
       " Document(page_content='# since this by default will produce a plain list -\\n \\n# we explained this in the chapter 4\\n \\ndbiases \\n\\u200b\\n= \\n\\u200b\\nnp.sum(drelu, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n \\n# Update parameters\\n \\nweights \\n\\u200b\\n+= -\\n\\u200b\\n0.001 \\n\\u200b\\n* \\n\\u200b\\ndweights\\n \\nbiases \\n\\u200b\\n+= -\\n\\u200b\\n0.001 \\n\\u200b\\n* \\n\\u200b\\ndbiases\\n \\n \\nprint\\n\\u200b\\n(weights)\\n \\nprint\\n\\u200b\\n(biases)\\n \\n \\n>>>\\n \\n[[ \\n\\u200b\\n0.179515   0.5003665 \\n\\u200b\\n-\\n\\u200b\\n0.262746 \\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0.742093  \\n\\u200b\\n-\\n\\u200b\\n0.9152577 \\n\\u200b\\n-\\n\\u200b\\n0.2758402\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n-\\n\\u200b\\n0.510153   0.2529017  0.1629592\\n\\u200b\\n]\\n \\n [ \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 207}),\n",
       " Document(page_content='\\u200b\\n]\\n \\n [ \\n\\u200b\\n0.971328  \\n\\u200b\\n-\\n\\u200b\\n0.5021842  0.8636583\\n\\u200b\\n]]\\n \\n[[\\n\\u200b\\n1.98489  2.997739 0.497389\\n\\u200b\\n]]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 207}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n39\\n \\nIn this code, we replaced the plain Python functions with NumPy variants, created example data,\\n \\ncalculated the forward and backward passes, and updated the parameters. Now we will update the\\n \\ndense layer and ReLU activation code with a \\n\\u200b\\nbackward\\n\\u200b\\n method (for backpropagation), which\\n \\nwe’ll call during the backpropagation phase of our model.\\n \\n# Dense layer\\n \\nclass \\n\\u200b\\nLayer_Dense\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 208}),\n",
       " Document(page_content='\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Layer initialization\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n, \\n\\u200b\\nneurons\\n\\u200b\\n):\\n \\n        self.weights \\n\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(inputs, neurons)\\n \\n        self.biases \\n\\u200b\\n= \\n\\u200b\\nnp.zeros((\\n\\u200b\\n1\\n\\u200b\\n, neurons))\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, self.weights) \\n\\u200b\\n+ \\n\\u200b\\nself.biases\\n \\n \\n# ReLU activation\\n \\nclass \\n\\u200b\\nActivation_ReLU\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 208}),\n",
       " Document(page_content='\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nnp.maximum(\\n\\u200b\\n0\\n\\u200b\\n, inputs)\\n \\n \\nDuring the \\n\\u200b\\nforward\\n\\u200b\\n method for our \\n\\u200b\\nLayer_Dense\\n\\u200b\\n class, we will want to remember what the\\n \\ninputs were (recall that we’ll need them when calculating the partial derivative with respect to\\n \\nweights during backpropagation), which can be easily implemented using an object property\\n \\n(\\n\\u200b\\nself.inputs\\n\\u200b\\n):\\n \\n# Dense layer\\n \\nclass \\n\\u200b\\nLayer_Dense\\n\\u200b\\n:\\n \\n    \\n\\u200b\\n...\\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 208}),\n",
       " Document(page_content='\\u200b\\n...\\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n...\\n \\n        \\n\\u200b\\nself.inputs \\n\\u200b\\n= \\n\\u200b\\ninputs', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 208}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n40\\n \\nNext, we will add our backward pass (backpropagation) code that we developed previously into a\\n \\nnew method in the layer class, which we’ll call \\n\\u200b\\nbackward\\n\\u200b\\n:\\n \\nclass \\n\\u200b\\nLayer_Dense\\n\\u200b\\n:\\n \\n    \\n\\u200b\\n...\\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Gradients on parameters\\n \\n        \\n\\u200b\\nself.dweights \\n\\u200b\\n= \\n\\u200b\\nnp.dot(self.inputs.T, dvalues)\\n \\n        self.dbiases \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 209}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\nnp.sum(dvalues, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n        \\n\\u200b\\n# Gradient on values\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(dvalues, self.weights.T)\\n \\n \\nWe then do the same for our ReLU class:\\n \\n# ReLU activation\\n \\nclass \\n\\u200b\\nActivation_ReLU\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Remember input values\\n \\n        \\n\\u200b\\nself.inputs \\n\\u200b\\n= \\n\\u200b\\ninputs\\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nnp.maximum(\\n\\u200b\\n0\\n\\u200b\\n, inputs)\\n \\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 209}),\n",
       " Document(page_content=\"\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Since we need to modify the original variable,\\n \\n        # let's make a copy of the values first\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\ndvalues.copy()\\n \\n \\n        \\n\\u200b\\n# Zero gradient where input values were negative\\n \\n        \\n\\u200b\\nself.dinputs[self.inputs \\n\\u200b\\n<= \\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n= \\n\\u200b\\n0\\n \\n \\nBy this point, we’ve covered everything we need to perform backpropagation, except for the\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 209}),\n",
       " Document(page_content='derivative of the Softmax activation function and the derivative of the cross-entropy loss function.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 209}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n41\\n \\n \\nCategorical Cross-Entropy loss derivative\\n \\nIf you are not interested in the mathematical derivation of the Categorical Cross-Entropy loss, feel\\n \\nfree to skip to the code implementation, as derivatives are known for common loss functions, and\\n \\nyou won’t necessarily need to know how to solve them. It is a good exercise if you plan to create\\n \\ncustom loss functions, though.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 210}),\n",
       " Document(page_content='As we learned in chapter 5, the Categorical Cross-Entropy loss function’s formula is:\\n \\n \\n \\nWhere \\n\\u200b\\nL\\n\\u200b\\ni\\n\\u200b\\n denotes sample loss value, \\n\\u200b\\ni\\n\\u200b\\n — \\n\\u200b\\ni\\n\\u200b\\n-th sample in a set, \\n\\u200b\\nk\\n\\u200b\\n — index of the target label\\n \\n(ground-true label), \\n\\u200b\\ny\\n\\u200b\\n — target values and \\n\\u200b\\ny-hat\\n\\u200b\\n — predicted values.\\n \\nThis formula is convenient when calculating the loss value itself, as all we need is the output of\\n \\nthe Softmax activation function at the index of the correct class. For the purpose of the derivative', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 210}),\n",
       " Document(page_content='calculation, we’ll use the full equation mentioned back in chapter 5:\\n \\n \\n \\nWhere \\n\\u200b\\nL\\n\\u200b\\ni\\n\\u200b\\n denotes sample loss value, \\n\\u200b\\ni\\n\\u200b\\n — \\n\\u200b\\ni\\n\\u200b\\n-th sample in a set, \\n\\u200b\\nj\\n\\u200b\\n — label/output index, \\n\\u200b\\ny\\n\\u200b\\n — target\\n \\nvalues and \\n\\u200b\\ny-hat\\n\\u200b\\n — predicted values.\\n \\nWe’ll use this full function because our current goal is to calculate the gradient, which is\\n \\ncomposed of the partial derivatives of the loss function with respect to each of its inputs (being', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 210}),\n",
       " Document(page_content='the outputs of the Softmax activation function). This means that we cannot use the equation,\\n \\nwhich takes just the value at the index of the correct class (the first equation above). To calculate\\n \\npartial derivatives with respect to each of the inputs, we need an equation that takes all of them as\\n \\nparameters, thus the choice to use the full equation.\\n \\nFirst, let’s define the gradient equation:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 210}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n42\\n \\nWe defined the equation here as the partial derivative of the loss function with respect to each of\\n \\nits inputs. We already learned that the derivative of the sum equals the sum of the derivatives. We\\n \\nalso learned that we can move constants. An example is \\n\\u200b\\ny\\n\\u200b\\ni,j\\n\\u200b\\n, as it is not what we are calculating the\\n \\nderivative with respect to. Let’s apply these transforms:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 211}),\n",
       " Document(page_content='Now we have to solve the derivative of the logarithmic function, which is the reciprocal of its\\n \\nparameter, multiplied (using the chain rule) by the partial derivative of this parameter — using\\n \\nprime (also called Lagrange’s) notation:\\n \\n \\n \\nWe can solve it further (using Leibniz’s notation in this case):\\n \\n \\n \\nLet’s apply this derivative:\\n \\n \\n \\nThe partial derivative of a value with respect to this value equals 1:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 211}),\n",
       " Document(page_content='Since we are calculating the partial derivative with respect to the \\n\\u200b\\ny\\n\\u200b\\n given \\n\\u200b\\nj\\n\\u200b\\n, the sum is being\\n \\nperformed over a single element and can be omitted:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 211}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n43\\n \\nFull solution:\\n \\n \\n \\nThe derivative of this loss function with respect to its inputs (predicted values at the i-th sample,\\n \\nsince we are interested in a gradient with respect to the predicted values) equals the negative\\n \\nground-truth vector, divided by the vector of the predicted values (which is also the output vector\\n \\nof the softmax function).', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 212}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n44\\n \\n \\nCategorical Cross-Entropy loss derivative code\\n \\nimplementation\\n \\nSince we derived this equation and have found that it solves to a simple division operation of 2\\n \\nvalues, we know that, with NumPy, we can extend this operation to the sample-wise vectors of\\n \\nground truth and predicted values, and further to the batch-wise arrays of them. From the coding', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 213}),\n",
       " Document(page_content='perspective, we need to add a backward method to the Loss_CategoricalCrossentropy class. We\\n \\nneed to pass the array of predictions and the array of true values into it and calculate the negated\\n \\ndivision of them:\\n \\n# Cross-entropy loss\\n \\nclass \\n\\u200b\\nLoss_CategoricalCrossentropy\\n\\u200b\\n(\\n\\u200b\\nLoss\\n\\u200b\\n):\\n \\n    \\n\\u200b\\n...\\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues)\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 213}),\n",
       " Document(page_content=\"\\u200b\\n# Number of labels in every sample\\n \\n        # We'll use the first sample to count them\\n \\n        \\n\\u200b\\nlabels \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues[\\n\\u200b\\n0\\n\\u200b\\n])\\n \\n \\n        \\n\\u200b\\n# If labels are sparse, turn them into one-hot vector\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n            y_true \\n\\u200b\\n= \\n\\u200b\\nnp.eye(labels)[y_true]\\n \\n \\n        \\n\\u200b\\n# Calculate gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= -\\n\\u200b\\ny_true \\n\\u200b\\n/ \\n\\u200b\\ndvalues\\n \\n        \\n\\u200b\\n# Normalize gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 213}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\nself.dinputs \\n\\u200b\\n/ \\n\\u200b\\nsamples\\n \\n \\nAlong with the partial derivative calculation, we are performing two additional operations. First,\\n \\nwe’re turning numerical labels into one-hot encoded vectors — prior to this, we need to check\\n \\nhow many dimensions y_true consists of. If the shape of the labels returns a single dimension', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 213}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n45\\n \\n(which means that they are shaped like a list and not like an array), they consist of discrete\\n \\nnumbers and need to be converted to a list of one-hot encoded vectors — a two-dimensional\\n \\narray. If that’s the case, we need to turn them into one-hot encoded vectors. We’ll use the \\n\\u200b\\nnp.eye\\n \\nmethod which, given a number, \\n\\u200b\\nn\\n\\u200b\\n, returns an \\n\\u200b\\nn\\n\\u200b\\nx\\n\\u200b\\nn\\n\\u200b\\n array filled with ones on the diagonal and zeros', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 214}),\n",
       " Document(page_content='everywhere else. For example:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nnp.eye(\\n\\u200b\\n5\\n\\u200b\\n)\\n \\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n]])\\n \\n \\nWe can then index this table with the numerical label to get the one-hot encoded vector that', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 214}),\n",
       " Document(page_content='represents it:\\n \\nnp.eye(\\n\\u200b\\n5\\n\\u200b\\n)[\\n\\u200b\\n1\\n\\u200b\\n]\\n \\n \\n \\n>>>\\n \\narray([\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n])\\n \\n \\n \\nnp.eye(\\n\\u200b\\n5\\n\\u200b\\n)[\\n\\u200b\\n4\\n\\u200b\\n]\\n \\n \\n \\n>>>\\n \\narray([\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n])\\n \\n \\nIf \\n\\u200b\\ny_true\\n\\u200b\\n is already one-hot encoded, we do not perform this step.\\n \\nThe second operation is the gradient normalization. As we’ll learn in the next chapter, optimizers\\n \\nsum all of the gradients related to each weight and bias before multiplying them by the learning', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 214}),\n",
       " Document(page_content='rate (or some other factor). What this means, in our case, is that the more samples we have in a\\n \\ndataset, the more gradient sets we’ll receive at this step, and the bigger this sum will become. As a\\n \\nconsequence, we’ll have to adjust the learning rate according to each set of samples. To solve this\\n \\nproblem, we can divide all of the gradients by the number of samples. A sum of elements divided\\n \\nby a count of them is their mean value (and, as we mentioned,  the optimizer will perform the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 214}),\n",
       " Document(page_content='sum) — this way, we’ll effectively normalize the gradients and make their sum’s magnitude\\n \\ninvariant to the number of samples.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 214}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n46\\n \\n \\nSoftmax activation derivative\\n \\nThe next calculation that we need to perform is the partial derivative of the Softmax function,\\n \\nwhich is a bit more complicated task than the derivative of the Categorical Cross-Entropy\\n \\nloss. Let’s remind ourselves of the equation of the Softmax activation function and define the\\n \\nderivative:\\n \\n \\nWhere \\n\\u200b\\nS\\n\\u200b\\ni,j\\n\\u200b\\n denotes \\n\\u200b\\nj\\n\\u200b\\n-th Softmax’s output of \\n\\u200b\\ni\\n\\u200b\\n-th sample, \\n\\u200b\\nz\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 215}),\n",
       " Document(page_content='-th sample, \\n\\u200b\\nz\\n\\u200b\\n — input array which is a list of input\\n \\nvectors (output vectors from the previous layer), \\n\\u200b\\nz\\n\\u200b\\ni,j\\n\\u200b\\n — \\n\\u200b\\nj\\n\\u200b\\n-th Softmax’s input of \\n\\u200b\\ni\\n\\u200b\\n-th sample, \\n\\u200b\\nL\\n\\u200b\\n —\\n \\nnumber of inputs, \\n\\u200b\\nz\\n\\u200b\\ni,k\\n\\u200b\\n — \\n\\u200b\\nk\\n\\u200b\\n-th Softmax’s input of \\n\\u200b\\ni\\n\\u200b\\n-th sample.\\n \\nAs we described in chapter 4, the Softmax function equals the exponentiated input divided by the\\n \\nsum of all exponentiated inputs. In other words, we need to exponentiate all of the values first,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 215}),\n",
       " Document(page_content='then divide each of them by the sum of all of them to perform the normalization. Each input to the\\n \\nSoftmax impacts each of the outputs, and we need to calculate the partial derivative of each\\n \\noutput with respect to each input. From the programming side of things, if we calculate the impact\\n \\nof one list on the other list, we’ll receive a matrix of values as a result. That’s exactly what we’ll\\n \\ncalculate here — we’ll calculate the \\n\\u200b\\nJacobian matrix\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 215}),\n",
       " Document(page_content='\\u200b\\nJacobian matrix\\n\\u200b\\n (which we’ll explain later) of the vectors,\\n \\nwhich we’ll dive deeper into soon.\\n \\nTo calculate this derivative, we need to first define the derivative of the division operation:\\n \\n \\n \\nIn order to calculate the derivative of the division operation, we need to take the derivative of the\\n \\nnumerator multiplied by the denominator, subtract the numerator multiplied by the derivative of\\n \\nthe denominator from it, and then divide the result by the squared denominator.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 215}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n47\\n \\nWe can now start solving the derivative:\\n \\n \\n \\nLet’s apply the derivative of the division operation:\\n \\n \\n \\nAt this step, we have two partial derivatives present in the equation. For the one on the right side\\n \\nof the numerator (right side of the subtraction operator):\\n \\n \\n \\nWe need to calculate the derivative of the sum of the constant,\\n\\u200b\\ne\\n\\u200b\\n (Euler’s number), raised to power\\n \\nz\\n\\u200b\\ni,l\\n\\u200b\\n (where \\n\\u200b\\nl\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 216}),\n",
       " Document(page_content='\\u200b\\n (where \\n\\u200b\\nl\\n\\u200b\\n denotes consecutive indices from \\n\\u200b\\n1\\n\\u200b\\n to the number of the Softmax outputs — \\n\\u200b\\nL\\n\\u200b\\n) with\\n \\nrespect to the \\n\\u200b\\nz\\n\\u200b\\ni,k\\n\\u200b\\n. The derivative of the sum operation is the sum o f derivatives, and the derivative\\n \\nof the constant \\n\\u200b\\ne\\n\\u200b\\n raised to power \\n\\u200b\\nn\\n\\u200b\\n (\\n\\u200b\\ne\\n\\u200b\\nn\\n\\u200b\\n) with respect to \\n\\u200b\\nn\\n\\u200b\\n equals \\n\\u200b\\ne\\n\\u200b\\nn\\n\\u200b\\n:\\n \\n \\n \\nIt is a special case when the derivative of an exponential function equals this exponential function', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 216}),\n",
       " Document(page_content='itself, as its exponent is exactly what we are deriving with respect to, thus its derivative equals \\n\\u200b\\n1\\n\\u200b\\n.\\n \\nWe also know that the range \\n\\u200b\\n1...L\\n\\u200b\\n contains \\n\\u200b\\nk\\n\\u200b\\n (\\n\\u200b\\nk\\n\\u200b\\n is one of the indices from this range) exactly once\\n \\nand then, in this case, the derivative is going to equal \\n\\u200b\\ne\\n\\u200b\\n to the power of the \\n\\u200b\\nz\\n\\u200b\\ni,k\\n\\u200b\\n (as \\n\\u200b\\nj\\n\\u200b\\n equals \\n\\u200b\\nk\\n\\u200b\\n) and \\n\\u200b\\n0\\n \\notherwise (when \\n\\u200b\\nj\\n\\u200b\\n does not equal \\n\\u200b\\nk\\n\\u200b\\n as \\n\\u200b\\nz\\n\\u200b\\ni,l\\n\\u200b\\n won’t contain \\n\\u200b\\nz\\n\\u200b\\ni,k\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 216}),\n",
       " Document(page_content='\\u200b\\nz\\n\\u200b\\ni,k\\n\\u200b\\n and wil l be treated as a constant — The\\n \\nderivative of the constant equals \\n\\u200b\\n0\\n\\u200b\\n):', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 216}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n48\\n \\nThe derivative on the left side of the subtraction operator in the denominator is a slightly different\\n \\ncase:\\n \\n \\n \\nIt does not contain the sum over all of the elements like the derivative we solved moments ago, so\\n \\nit can become either \\n\\u200b\\n0\\n\\u200b\\n if \\n\\u200b\\nj≠k\\n\\u200b\\n or \\n\\u200b\\ne\\n\\u200b\\n to the power of the \\n\\u200b\\nz\\n\\u200b\\ni,j\\n\\u200b\\n if \\n\\u200b\\nj=k\\n\\u200b\\n. That means, starting from this step,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 217}),\n",
       " Document(page_content='we need to calculate the derivatives separately for both cases. Let’s start with \\n\\u200b\\nj=k\\n\\u200b\\n.\\n \\n \\nIn the case of \\n\\u200b\\nj=k\\n\\u200b\\n, the derivative on the left side is going to equal \\n\\u200b\\ne\\n\\u200b\\n to the power of the \\n\\u200b\\nz\\n\\u200b\\ni,j\\n\\u200b\\n \\n\\u200b\\nand the\\n \\nderivative on the right solves to the same value in both cases. Let’s substitute them:\\n \\n \\n \\nThe numerator contains the constant \\n\\u200b\\ne\\n\\u200b\\n to the power of \\n\\u200b\\nz\\n\\u200b\\ni,j\\n\\u200b\\n in both the minuend (the value we are', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 217}),\n",
       " Document(page_content='subtracting from) and subtrahend (the value we are subtracting from the minuend) of the\\n \\nsubtraction operation. Because of this, we can regroup the numerator to contain this value\\n \\nmultiplied by the subtraction of their current multipliers. We can also write the denominator as\\n \\na multiplication of the value instead of using the power of \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n \\n \\nThen let’s split the whole equation into 2 parts:\\n \\n \\n \\nWe moved \\n\\u200b\\ne\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 217}),\n",
       " Document(page_content='We moved \\n\\u200b\\ne\\n\\u200b\\n from the numerator and the sum from the denominator to its own fraction, and the\\n \\ncontent of the parentheses in the numerator, and the other sum from the denominator as another\\n \\nfraction, both joined by the multiplication operation. Now we can further split the “right” fraction\\n \\ninto two separate fractions:\\n \\n \\n \\nIn this case, as it’s a subtraction operation, we separated both values from the numerator, dividing', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 217}),\n",
       " Document(page_content='them both by the denominator and applying the subtraction operation between new fractions. If', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 217}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n49\\n \\nwe look closely, the “left” fraction turns into the Softmax function’s equation, as well as the\\n \\n“right” one, with the middle fraction solving to \\n\\u200b\\n1\\n\\u200b\\n as the numerator and the denominator are the\\n \\nsame values:\\n \\n \\n \\nNote that the “left” Softmax function carries the \\n\\u200b\\nj\\n\\u200b\\n parameter, and t he “right” one \\n\\u200b\\nk\\n\\u200b\\n — both came\\n \\nfrom their numerators, respectively.\\n \\nFull solution:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 218}),\n",
       " Document(page_content='Now we have to go back and solve the derivative in the case of \\n\\u200b\\nj≠k\\n\\u200b\\n. In this case, the “left”\\n \\nderivative of the original equation solves to \\n\\u200b\\n0\\n\\u200b\\n as the whole expression is treated as a constant:\\n \\n \\n \\nThe difference is that now the whole subtrahend solves to \\n\\u200b\\n0\\n\\u200b\\n, leaving us with just the minuend in\\n \\nthe numerator:\\n \\n \\n \\nNow, exactly like before, we can write the denominator as the multiplication of the values instead\\n \\nof using the power of \\n\\u200b\\n2\\n\\u200b\\n:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 218}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n50\\n \\nThat lets us to split this fraction into 2 fractions, using the multiplication operation:\\n \\n \\n \\nNow both fractions represent the Softmax function:\\n \\n \\n \\nNote that the left Softmax function carries the \\n\\u200b\\nj\\n\\u200b\\n parameter, and the  “right” one has \\n\\u200b\\nk\\n\\u200b\\n — both came\\n \\nfrom their numerators, respectively.\\n \\nFull solution:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 219}),\n",
       " Document(page_content='As a summary, the solution of the derivative of the Softmax function with respect to its inputs is:\\n \\n \\n \\nThat’s not the end of the calculation that we can perform here. When left in this form, we’ll have\\n \\n2 separate equations to code and use in different cases, which isn’t very convenient for the speed\\n \\nof calculations. We can, however, further morph the result of the second case of the derivative:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 219}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n51\\n \\nIn the first step, we moved the second Softmax along the minus sign into the brackets so we can\\n \\nadd a zero inside of them and right before this value. That does not change the solution, but now:\\n \\n \\n \\nBoth solutions look very similar, they differ only in a single value. Conveniently, there exists\\n \\nKronecker delta\\n\\u200b\\n function (which we’ll explain soon) whose equation is:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 220}),\n",
       " Document(page_content='We can apply it here, simplifying our equation further to:\\n \\n \\n \\nThat’s the final math solution to the derivative of the Softmax function’s outputs with respect\\n \\nto each of its inputs. To make it a little bit easier to implement in Python using NumPy, let’s\\n \\ntransform the equation for the last time:\\n \\n \\n \\nWe basically multiplied \\n\\u200b\\nS\\n\\u200b\\ni,j\\n\\u200b\\n by both sides of the subtraction operation from the parentheses.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 220}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n52\\n \\n \\nSoftmax activation derivative code\\n \\nimplementation\\n \\nThis lets us code the solution using just two NumPy functions, which we’ll explain now step by\\n \\nstep:\\n \\nLet’s make up a single sample:\\n \\nsoftmax_output \\n\\u200b\\n= \\n\\u200b\\n[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n]\\n \\nAnd shape it as a list of samples:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\n \\nsoftmax_output \\n\\u200b\\n= \\n\\u200b\\nnp.array(softmax_output).reshape(\\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 221}),\n",
       " Document(page_content='1\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\nsoftmax_output)\\n \\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n0.7\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.1\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.2\\n\\u200b\\n]])\\n \\n \\nThe left side of the equation is Softmax’s output multiplied by the Kronecker delta. The\\n \\nKronecker delta equals \\n\\u200b\\n1\\n\\u200b\\n when both inputs are equal, and \\n\\u200b\\n0\\n\\u200b\\n otherwise. If we visualize this as an\\n \\narray, we’ll have an array of zeros with ones on the diagonal — you might remember that we\\n \\nalready have implemented such a solution using the \\n\\u200b\\nnp.eye\\n\\u200b\\n method:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 221}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n53\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\nnp.eye(softmax_output.shape[\\n\\u200b\\n0\\n\\u200b\\n]))\\n \\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\n1.\\n\\u200b\\n]])\\n \\n \\nNow we’ll do the multiplication of both of the values from the equation part:\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\nsoftmax_output \\n\\u200b\\n* \\n\\u200b\\nnp.eye(softmax_output.shape[\\n\\u200b\\n0\\n\\u200b\\n]))\\n \\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0. \\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0.1', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 222}),\n",
       " Document(page_content='\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0. \\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n]])\\n \\n \\nIt turns out that we can gain some speed by replacing this by the \\n\\u200b\\nnp.diagflat\\n\\u200b\\n method call,\\n \\nwhich computes the same solution — the diagflat method creates an array using an input vector as\\n \\nthe diagonal:\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\nnp.diagflat(softmax_output))\\n \\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0. \\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0. \\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0. \\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n]])', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 222}),\n",
       " Document(page_content=', \\n\\u200b\\n0.2\\n\\u200b\\n]])\\n \\n \\nThe other part of the equation is \\n\\u200b\\nS\\n\\u200b\\ni,j\\n\\u200b\\nS\\n\\u200b\\ni,k\\n\\u200b\\n — the multiplication of the  Softmax outputs, iterating over\\n \\nthe \\n\\u200b\\nj\\n\\u200b\\n and \\n\\u200b\\nk\\n\\u200b\\n indices respectively. Since, for each sample (the \\n\\u200b\\ni\\n\\u200b\\n index), we’ll have to multiply the\\n \\nvalues from the Softmax function’s output (in all of the combinations), we can use the dot product\\n \\noperation. For this, we’ll just have to transpose the second argument to get its row vector form (as\\n \\ndescribed in chapter 2):\\n \\nprint\\n\\u200b\\n(', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 222}),\n",
       " Document(page_content='print\\n\\u200b\\n(\\n\\u200b\\nnp.dot(softmax_output, softmax_output.T))\\n \\n \\n \\n>>>\\n \\narray([[\\n\\u200b\\n0.49\\n\\u200b\\n, \\n\\u200b\\n0.07\\n\\u200b\\n, \\n\\u200b\\n0.14\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.07\\n\\u200b\\n, \\n\\u200b\\n0.01\\n\\u200b\\n, \\n\\u200b\\n0.02\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n0.14\\n\\u200b\\n, \\n\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.04\\n\\u200b\\n]])', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 222}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n54\\n \\nFinally, we can perform the subtraction of both arrays (following the equation):\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\nnp.diagflat(softmax_output) \\n\\u200b\\n-\\n \\n      np.dot(softmax_output, softmax_output.T))\\n \\n \\n \\n>>>\\n \\narray([[ \\n\\u200b\\n0.21\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.07\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.14\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n-\\n\\u200b\\n0.07\\n\\u200b\\n,  \\n\\u200b\\n0.09\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.02\\n\\u200b\\n],\\n \\n       [\\n\\u200b\\n-\\n\\u200b\\n0.14\\n\\u200b\\n, \\n\\u200b\\n-\\n\\u200b\\n0.02\\n\\u200b\\n,  \\n\\u200b\\n0.16\\n\\u200b\\n]])', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 223}),\n",
       " Document(page_content='\\u200b\\n0.16\\n\\u200b\\n]])\\n \\n \\nThe matrix result of the equation and the array solution provided by the code is called the\\n \\nJacobian matrix\\n\\u200b\\n. In our case, the Jacobian matrix is an array of partial derivatives in all of the\\n \\ncombinations of both input vectors. Remember, we are calculating the partial derivatives of every\\n \\noutput of the Softmax function with respect to each input separately. We do this because each', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 223}),\n",
       " Document(page_content='input influences each output due to the normalization process, which takes the sum of all the\\n \\nexponentiated inputs. The result of this operation, performed on a batch of samples, is a list of the\\n \\nJacobian matrices, which effectively forms a 3D matrix — you can visualize it as a column whose\\n \\nlevels are Jacobian matrices being the sample-wise gradient of the Softmax function.\\n \\nThis raises a question — if sample-wise gradients are the Jacobian matrices, how do we perform', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 223}),\n",
       " Document(page_content='the chain rule with the gradient back-propagated from the loss function, since it’s a vector for\\n \\neach sample? Also, what do we do with the fact that the previous layer, which is the Dense layer,\\n \\nwill expect the gradients to be a 2D array? Currently, we have a 3D array of the partial derivatives\\n \\n— a list of the Jacobian matrices. The derivative of the Softmax function with respect to any of its', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 223}),\n",
       " Document(page_content='inputs returns a vector of partial derivatives (a row from the Jacobian matrix), as this input\\n \\ninfluences all the outputs, thus also influencing the partial derivative for each of them. We need to\\n \\nsum the values from these vectors so that each of the inputs for each of the samples will return a\\n \\nsingle partial derivative value instead. Because each input influences all of the outputs, the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 223}),\n",
       " Document(page_content='returned vector of the partial derivatives has to be summed up for the final partial derivative with\\n \\nrespect to this input. We can perform this operation on each of the Jacobian matrices directly,\\n \\napplying the chain rule at the same time (applying the gradient from the loss function) using\\n \\nnp.dot()\\n\\u200b\\n — For each sample, it’ll take the row from the Jacobian matrix and multiply it by the\\n \\ncorresponding value from the loss function’s gradient. As a result, the dot product of each of these', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 223}),\n",
       " Document(page_content='vectors and values will return a singular value, forming a vector of the partial derivatives\\n \\nsample-wise and a 2D array (a list of the resulting vectors) batch-wise.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 223}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n55\\n \\nLet’s code the solution:\\n \\n# Softmax activation\\n \\nclass \\n\\u200b\\nActivation_Softmax\\n\\u200b\\n:\\n \\n    \\n\\u200b\\n...\\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Create uninitialized array\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nnp.empty_like(dvalues)\\n \\n \\n        \\n\\u200b\\n# Enumerate outputs and gradients\\n \\n        \\n\\u200b\\nfor \\n\\u200b\\nindex, (single_output, single_dvalues) \\n\\u200b\\nin \\n\\u200b\\n\\\\\\n \\n                \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 224}),\n",
       " Document(page_content='\\u200b\\nenumerate\\n\\u200b\\n(\\n\\u200b\\nzip\\n\\u200b\\n(self.output, dvalues)):\\n \\n            \\n\\u200b\\n# Flatten output array\\n \\n            \\n\\u200b\\nsingle_output \\n\\u200b\\n= \\n\\u200b\\nsingle_output.reshape(\\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n)\\n \\n            \\n\\u200b\\n# Calculate Jacobian matrix of the output and\\n \\n            \\n\\u200b\\njacobian_matrix \\n\\u200b\\n= \\n\\u200b\\nnp.diagflat(single_output) \\n\\u200b\\n- \\n\\u200b\\n\\\\\\n \\n                              np.dot(single_output, single_output.T)\\n \\n            \\n\\u200b\\n# Calculate sample-wise gradient', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 224}),\n",
       " Document(page_content='# and add it to the array of sample gradients\\n \\n            \\n\\u200b\\nself.dinputs[index] \\n\\u200b\\n= \\n\\u200b\\nnp.dot(jacobian_matrix,\\n \\n                                         single_dvalues)\\n \\n \\nFirst, we created an empty array (which will become the resulting gradient array) with the same\\n \\nshape as the gradients that we’re receiving to apply the chain rule. The \\n\\u200b\\nnp.empty_like\\n\\u200b\\n method\\n \\ncreates an empty and uninitialized array. Uninitialized means that we can expect it to contain', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 224}),\n",
       " Document(page_content='meaningless values, but we’ll set all of them shortly anyway, so there’s no need for initialization\\n \\n(for example, with zeros using \\n\\u200b\\nnp.zeros()\\n\\u200b\\n instead). In the next step, we’re going to iterate\\n \\nsample-wise over pairs of the outputs and gradients, calculating the partial derivatives as\\n \\ndescribed earlier and calculating the final product (applying the chain rule) of the Jacobian matrix', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 224}),\n",
       " Document(page_content='and gradient vector (from the passed-in gradient array), storing the resulting vector as a row in the\\n \\ndinput array. We’re going to store each vector in each row while iterating, forming the output\\n \\narray.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 224}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n56\\n \\n \\nCommon Categorical Cross-Entropy loss and\\n \\nSoftmax activation derivative\\n \\nAt the moment, we have calculated the partial derivatives of the Categorical Cross-Entropy loss\\n \\nand Softmax activation functions, and we can finally use them, but there is still one more step that\\n \\nwe can perform to speed the calculations up. Different books and tutorials usually mention the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 225}),\n",
       " Document(page_content='derivative of the loss function with respect to the Softmax inputs, or even weight and biases of the\\n \\noutput layer directly and don’t go into the details of the partial derivatives of these functions\\n \\nseparately. This is partially because the derivatives of both functions combine to solve a simple\\n \\nequation — the whole code implementation is simpler and faster to execute. When we look at our', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 225}),\n",
       " Document(page_content='current code, we perform multiple operations to calculate the gradients and even include a loop in\\n \\nthe backward step of the activation function.\\n \\nLet’s apply the chain rule to calculate the partial derivative of the Categorical Cross-Entropy loss\\n \\nfunction with respect to the Softmax function inputs. First, let’s define this derivative by applying\\n \\nthe chain rule:\\n \\n \\n \\nThis partial derivative equals the partial derivative of the loss function with respect to its inputs,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 225}),\n",
       " Document(page_content='multiplied (using the chain rule) by the partial derivative of the activation function with respect to\\n \\nits inputs. Now we need to systematize semantics — we know that the inputs to the loss function,\\n \\ny-hat\\n\\u200b\\ni,j\\n\\u200b\\n, are the outputs of the activation function, \\n\\u200b\\nS\\n\\u200b\\ni,j\\n\\u200b\\n:\\n \\n \\n \\nThat means that we can update the equation to the form of:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 225}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n57\\n \\nNow we can substitute the equation for the partial derivative of the Categorical Cross-Entropy\\n \\nfunction, but, since we are calculating the partial derivative with respect to the Softmax inputs,\\n \\nwe’ll use the one containing the sum operator over all of the outputs — it will soon become clear\\n \\nwhy. The derivative:\\n \\n \\n \\nAfter substitution to the combined derivative’s equation:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 226}),\n",
       " Document(page_content='Now, as we calculated before, the partial derivative of the Softmax activation, before applying\\n \\nKronecker delta to it:\\n \\n \\n \\nLet’s actually do the substitution of the \\n\\u200b\\nS\\n\\u200b\\ni,j\\n\\u200b\\n with \\n\\u200b\\ny-hat\\n\\u200b\\ni,j\\n\\u200b\\n here as well:\\n \\n \\n \\nThe solution is different depending on if \\n\\u200b\\nj=k\\n\\u200b\\n or \\n\\u200b\\nj≠k\\n\\u200b\\n. To handle for this situation, we have to split\\n \\nthe current partial derivative following these cases — when they both match and when they do\\n \\nnot:\\n \\n \\n \\nFor the \\n\\u200b\\nj≠k\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 226}),\n",
       " Document(page_content='For the \\n\\u200b\\nj≠k\\n\\u200b\\n case, we just updated the sum operator to exclude \\n\\u200b\\nk\\n\\u200b\\n and that’s the only change:\\n \\n \\n \\nFor the \\n\\u200b\\nj=k\\n\\u200b\\n case, we do not need the sum operator as it will sum only one element, of index \\n\\u200b\\nk\\n\\u200b\\n. For', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 226}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n58\\n \\nthe same reason, we also replace \\n\\u200b\\nj\\n\\u200b\\nindices with \\n\\u200b\\nk\\n\\u200b\\n:\\n \\n \\n \\nBack to the main equation:\\n \\n \\n \\nNow we can substitute the partial derivatives of the activation function for both cases with the\\n \\nnewly-defined solutions:\\n \\n \\n \\nWe can cancel out the \\n\\u200b\\ny-hat\\n\\u200b\\ni,k\\n\\u200b\\n from both sides of the subtraction in  the equation — both contain it', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 227}),\n",
       " Document(page_content='as part of the multiplication operations and in their denominators. Then on the “right” side of the\\n \\nequation, we can replace 2 minus signs with the plus one and remove the parentheses:\\n \\n \\n \\nNow let’s multiply the \\n\\u200b\\n-y\\n\\u200b\\ni,k\\n\\u200b\\n with the content of the parentheses on the “left” side of the equation:\\n \\n \\n \\nNow let’s look at the sum operation — it adds up \\n\\u200b\\ny\\n\\u200b\\ni,j\\n\\u200b\\ny-hat\\n\\u200b\\ni,k\\n\\u200b\\n over al l possible values of index\\n \\nj\\n\\u200b\\n except for when it equals \\n\\u200b\\nk\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 227}),\n",
       " Document(page_content='\\u200b\\nk\\n\\u200b\\n. Then, on the left of this part of the equation, we have\\n \\ny\\n\\u200b\\ni,k\\n\\u200b\\ny-hat\\n\\u200b\\ni,k\\n\\u200b\\n, which contains \\n\\u200b\\ny\\n\\u200b\\ni,k\\n\\u200b\\n — the exact element that is excluded  from the sum. We can\\n \\nthen join both expressions:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 227}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n59\\n \\nNow the sum operator iterates over all of the possible values of \\n\\u200b\\nj\\n\\u200b\\n and, since we know that \\n\\u200b\\ny\\n\\u200b\\ni,j\\n\\u200b\\n  for\\n \\neach \\n\\u200b\\ni\\n\\u200b\\n is the one-hot encoded vector of ground-truth values, the sum of  all of its elements equals\\n \\n1\\n\\u200b\\n. In other words, following the earlier explanation in this chapter — this sum will multiply \\n\\u200b\\n0\\n\\u200b\\n by\\n \\nthe \\n\\u200b\\ny-hat\\n\\u200b\\ni,k\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 228}),\n",
       " Document(page_content='\\u200b\\ny-hat\\n\\u200b\\ni,k\\n\\u200b\\n except for a single situation, the true label, where it’ll m ultiply \\n\\u200b\\n1\\n\\u200b\\n by this value. We can\\n \\nthen simplify it further to:\\n \\n \\n \\nFull solution:\\n \\n \\n \\nAs we can see, when we apply the chain rule to both partial derivatives, the whole equation\\n \\nsimplifies significantly to the subtraction of the predicted and ground truth values. It is also\\n \\nmultiple times faster to compute.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 228}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n60\\n \\n \\nCommon Categorical Cross-Entropy loss and\\n \\nSoftmax activation derivative - code\\n \\nimplementation\\n \\nTo code this solution, nothing in the forward pass changes — we still need to perform it on the\\n \\nactivation function to receive the outputs and then on the loss function to calculate the loss value.\\n \\nFor backpropagation, we’ll create the backward step containing the implementation of the new', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 229}),\n",
       " Document(page_content='equation, which calculates the combined gradient of the loss and activation functions. We’ll code\\n \\nthe solution as a separate class, which initializes both the Softmax activation and the Categorical\\n \\nCross-Entropy objects, calling their forward methods respectively during the forward pass. Then\\n \\nthe new backward pass is going to contain the new code:\\n \\n# Softmax classifier - combined Softmax activation\\n \\n# and cross-entropy loss for faster backward step\\n \\nclass \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 229}),\n",
       " Document(page_content=\"class \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy\\n\\u200b\\n():\\n \\n \\n    \\n\\u200b\\n# Creates activation and loss function objects\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        self.activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n        self.loss \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Output layer's activation function\\n \\n        \\n\\u200b\\nself.activation.forward(inputs)\\n \\n        \\n\\u200b\\n# Set the output\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 229}),\n",
       " Document(page_content='# Set the output\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nself.activation.output\\n \\n        \\n\\u200b\\n# Calculate and return loss value\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\nself.loss.calculate(self.output, y_true)\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 229}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n61\\n \\n        \\n\\u200b\\n# If labels are one-hot encoded,\\n \\n        # turn them into discrete values\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n            y_true \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y_true, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n \\n        \\n\\u200b\\n# Copy so we can safely modify\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\ndvalues.copy()\\n \\n        \\n\\u200b\\n# Calculate gradient\\n \\n        \\n\\u200b\\nself.dinputs[\\n\\u200b\\nrange\\n\\u200b\\n(samples), y_true] \\n\\u200b\\n-= \\n\\u200b\\n1\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 230}),\n",
       " Document(page_content='\\u200b\\n1\\n \\n        \\n\\u200b\\n# Normalize gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nself.dinputs \\n\\u200b\\n/ \\n\\u200b\\nsamples\\n \\n \\nTo implement the solution \\n\\u200b\\ny-hat\\n\\u200b\\ni,k\\n\\u200b\\n-y\\n\\u200b\\ni,k\\n\\u200b\\n, instead of performing the subtraction of the full arrays,\\n \\nwe’re taking advantage of the fact that the \\n\\u200b\\ny\\n\\u200b\\n being \\n\\u200b\\ny_true\\n\\u200b\\n in the code consists of one-hot\\n \\nencoded vectors, which means that, for each sample, there is only a singular value of \\n\\u200b\\n1\\n\\u200b\\n in these\\n \\nvectors and the remaining positions are filled with zeros.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 230}),\n",
       " Document(page_content='This means that we can use NumPy to index the prediction array with the sample number and its\\n \\ntrue value index, subtracting \\n\\u200b\\n1\\n\\u200b\\n from these values. This operation requires discrete true labels\\n \\ninstead of one-hot encoded ones, thus the additional code that performs the transformation if\\n \\nneeded — If the number of dimensions in the ground-truth array equals \\n\\u200b\\n2\\n\\u200b\\n, it means that it’s a\\n \\nmatrix consisting of one-hot encoded vectors. We can use \\n\\u200b\\nnp.argmax()\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 230}),\n",
       " Document(page_content='\\u200b\\nnp.argmax()\\n\\u200b\\n, which returns the index\\n \\nof the maximum value (index for \\n\\u200b\\n1\\n\\u200b\\n in this case), but we need to perform this operation\\n \\nsample-wise to get a vector of indices:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\ny_true \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n0\\n\\u200b\\n,\\n\\u200b\\n0\\n\\u200b\\n],[\\n\\u200b\\n0\\n\\u200b\\n,\\n\\u200b\\n0\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n],[\\n\\u200b\\n0\\n\\u200b\\n,\\n\\u200b\\n1\\n\\u200b\\n,\\n\\u200b\\n0\\n\\u200b\\n]])\\n \\nnp.argmax(y_true)\\n \\n \\n \\n>>>\\n \\n0\\n \\n \\n \\nprint\\n\\u200b\\n(\\n\\u200b\\nnp.argmax(y_true, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n))\\n \\n \\n \\n>>>\\n \\n[\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 230}),\n",
       " Document(page_content='2\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n]\\n \\n \\nFor the last step, we normalize the gradient in exactly the same way and for the same reason as\\n \\ndescribed along with the Categorical Cross-Entropy gradient normalization.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 230}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n62\\n \\nLet’s summarize the code for each of the classes that we have updated:\\n \\n# Softmax activation\\n \\nclass \\n\\u200b\\nActivation_Softmax\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Remember input values\\n \\n        \\n\\u200b\\nself.inputs \\n\\u200b\\n= \\n\\u200b\\ninputs\\n \\n \\n        \\n\\u200b\\n# Get unnormalized probabilities\\n \\n        \\n\\u200b\\nexp_values \\n\\u200b\\n= \\n\\u200b\\nnp.exp(inputs \\n\\u200b\\n- \\n\\u200b\\nnp.max(inputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 231}),\n",
       " Document(page_content='axis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n))\\n \\n        \\n\\u200b\\n# Normalize them for each sample\\n \\n        \\n\\u200b\\nprobabilities \\n\\u200b\\n= \\n\\u200b\\nexp_values \\n\\u200b\\n/ \\n\\u200b\\nnp.sum(exp_values, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nprobabilities\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Create uninitialized array\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 231}),\n",
       " Document(page_content='\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nnp.empty_like(dvalues)\\n \\n \\n        \\n\\u200b\\n# Enumerate outputs and gradients\\n \\n        \\n\\u200b\\nfor \\n\\u200b\\nindex, (single_output, single_dvalues) \\n\\u200b\\nin \\n\\u200b\\n\\\\\\n \\n                \\n\\u200b\\nenumerate\\n\\u200b\\n(\\n\\u200b\\nzip\\n\\u200b\\n(self.output, dvalues)):\\n \\n            \\n\\u200b\\n# Flatten output array\\n \\n            \\n\\u200b\\nsingle_output \\n\\u200b\\n= \\n\\u200b\\nsingle_output.reshape(\\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n)\\n \\n            \\n\\u200b\\n# Calculate Jacobian matrix of the output and\\n \\n            \\n\\u200b\\njacobian_matrix \\n\\u200b\\n= \\n\\u200b\\nnp.diagflat(single_output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 231}),\n",
       " Document(page_content='\\u200b\\n- \\n\\u200b\\n\\\\\\n \\n                              np.dot(single_output, single_output.T)\\n \\n            \\n\\u200b\\n# Calculate sample-wise gradient\\n \\n            # and add it to the array of sample gradients\\n \\n            \\n\\u200b\\nself.dinputs[index] \\n\\u200b\\n= \\n\\u200b\\nnp.dot(jacobian_matrix,\\n \\n                                         single_dvalues)\\n \\n \\n \\n# Cross-entropy loss\\n \\nclass \\n\\u200b\\nLoss_CategoricalCrossentropy\\n\\u200b\\n(\\n\\u200b\\nLoss\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ny_pred\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 231}),\n",
       " Document(page_content='\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples in a batch\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(y_pred)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 231}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n63\\n \\n        \\n\\u200b\\n# Clip data to prevent division by 0\\n \\n        # Clip both sides to not drag mean towards any value\\n \\n        \\n\\u200b\\ny_pred_clipped \\n\\u200b\\n= \\n\\u200b\\nnp.clip(y_pred, \\n\\u200b\\n1e-7\\n\\u200b\\n, \\n\\u200b\\n1 \\n\\u200b\\n- \\n\\u200b\\n1e-7\\n\\u200b\\n)\\n \\n \\n        # Probabilities for target values -\\n \\n        # only if categorical labels\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n            correct_confidences \\n\\u200b\\n= \\n\\u200b\\ny_pred_clipped[\\n \\n                \\n\\u200b\\nrange', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 232}),\n",
       " Document(page_content='\\u200b\\nrange\\n\\u200b\\n(samples),\\n \\n                y_true\\n \\n            ]\\n \\n \\n        \\n\\u200b\\n# Mask values - only for one-hot encoded labels\\n \\n        \\n\\u200b\\nelif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n            correct_confidences \\n\\u200b\\n= \\n\\u200b\\nnp.sum(\\n \\n                y_pred_clipped \\n\\u200b\\n* \\n\\u200b\\ny_true,\\n \\n                \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n \\n            \\n\\u200b\\n)\\n \\n \\n        \\n\\u200b\\n# Losses\\n \\n        \\n\\u200b\\nnegative_log_likelihoods \\n\\u200b\\n= -\\n\\u200b\\nnp.log(correct_confidences)\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\nnegative_log_likelihoods\\n \\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 232}),\n",
       " Document(page_content=\"\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues)\\n \\n        \\n\\u200b\\n# Number of labels in every sample\\n \\n        # We'll use the first sample to count them\\n \\n        \\n\\u200b\\nlabels \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues[\\n\\u200b\\n0\\n\\u200b\\n])\\n \\n \\n        \\n\\u200b\\n# If labels are sparse, turn them into one-hot vector\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n            y_true \\n\\u200b\\n= \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 232}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\nnp.eye(labels)[y_true]\\n \\n \\n        \\n\\u200b\\n# Calculate gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= -\\n\\u200b\\ny_true \\n\\u200b\\n/ \\n\\u200b\\ndvalues\\n \\n        \\n\\u200b\\n# Normalize gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nself.dinputs \\n\\u200b\\n/ \\n\\u200b\\nsamples\\n \\n \\n \\n# Softmax classifier - combined Softmax activation\\n \\n# and cross-entropy loss for faster backward step\\n \\nclass \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy\\n\\u200b\\n():\\n \\n \\n    \\n\\u200b\\n# Creates activation and loss function objects\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 232}),\n",
       " Document(page_content='\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        self.activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n        self.loss \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 232}),\n",
       " Document(page_content=\"Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n64\\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Output layer's activation function\\n \\n        \\n\\u200b\\nself.activation.forward(inputs)\\n \\n        \\n\\u200b\\n# Set the output\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nself.activation.output\\n \\n        \\n\\u200b\\n# Calculate and return loss value\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\nself.loss.calculate(self.output, y_true)\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 233}),\n",
       " Document(page_content='\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues)\\n \\n \\n        \\n\\u200b\\n# If labels are one-hot encoded,\\n \\n        # turn them into discrete values\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n            y_true \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y_true, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n \\n        \\n\\u200b\\n# Copy so we can safely modify\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\ndvalues.copy()\\n \\n        \\n\\u200b\\n# Calculate gradient', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 233}),\n",
       " Document(page_content='\\u200b\\nself.dinputs[\\n\\u200b\\nrange\\n\\u200b\\n(samples), y_true] \\n\\u200b\\n-= \\n\\u200b\\n1\\n \\n        \\n\\u200b\\n# Normalize gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nself.dinputs \\n\\u200b\\n/ \\n\\u200b\\nsamples\\n \\n \\nWe can now test if the combined backward step returns the same values compared to when we\\n \\nbackpropagate gradients through both of the functions separately. For this example, let’s make up\\n \\nan output of the Softmax function and some target values. Next, let’s backpropagate them using\\n \\nboth solutions:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 233}),\n",
       " Document(page_content='\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nimport \\n\\u200b\\nnnfs\\n \\n \\nnnfs.init()\\n \\n \\nsoftmax_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.9\\n\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]])\\n \\n \\nclass_targets \\n\\u200b\\n= \\n\\u200b\\nnp.array([\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n])\\n \\n \\nsoftmax_loss \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\nsoftmax_loss.backward(softmax_outputs, class_targets)\\n \\ndvalues1 \\n\\u200b\\n= \\n\\u200b\\nsoftmax_loss.dinputs', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 233}),\n",
       " Document(page_content='activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\nactivation.output \\n\\u200b\\n= \\n\\u200b\\nsoftmax_outputs\\n \\nloss \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()\\n \\nloss.backward(softmax_outputs, class_targets)\\n \\nactivation.backward(loss.dinputs)\\n \\ndvalues2 \\n\\u200b\\n= \\n\\u200b\\nactivation.dinputs', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 233}),\n",
       " Document(page_content=\"Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n65\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'Gradients: combined loss and activation:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(dvalues1)\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'Gradients: separate loss and activation:'\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(dvalues2)\\n \\n \\n \\n>>>\\n \\nGradients: combined loss and activation:\\n \\n[[\\n\\u200b\\n-\\n\\u200b\\n0.1         0.03333333  0.06666667\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0.03333333 \\n\\u200b\\n-\\n\\u200b\\n0.16666667  0.13333333\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0.00666667 \\n\\u200b\\n-\\n\\u200b\\n0.03333333  0.02666667\\n\\u200b\\n]]\\n \\nGradients: separate loss and activation:\\n \\n[[\\n\\u200b\\n-\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 234}),\n",
       " Document(page_content='[[\\n\\u200b\\n-\\n\\u200b\\n0.09999999  0.03333334  0.06666667\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0.03333334 \\n\\u200b\\n-\\n\\u200b\\n0.16666667  0.13333334\\n\\u200b\\n]\\n \\n [ \\n\\u200b\\n0.00666667 \\n\\u200b\\n-\\n\\u200b\\n0.03333333  0.02666667\\n\\u200b\\n]]\\n \\n \\nThe results are the same. The small difference between values in both arrays results from the\\n \\nprecision of floating-point values in raw Python and NumPy. To answer the question of how\\n \\nmany times faster this solution is, we can take advantage of Python’s timeit module, running', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 234}),\n",
       " Document(page_content='both solutions multiple times and combining the execution times. A full description of the timeit\\n \\nmodule and the code used here is outside of the scope of this book, but we include this code\\n \\npurely to show the speed deltas:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nfrom \\n\\u200b\\ntimeit \\n\\u200b\\nimport \\n\\u200b\\ntimeit\\n \\nimport \\n\\u200b\\nnnfs\\n \\n \\nnnfs.init()\\n \\n \\nsoftmax_outputs \\n\\u200b\\n= \\n\\u200b\\nnp.array([[\\n\\u200b\\n0.7\\n\\u200b\\n, \\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.2\\n\\u200b\\n],\\n \\n                            [\\n\\u200b\\n0.1\\n\\u200b\\n, \\n\\u200b\\n0.5\\n\\u200b\\n, \\n\\u200b\\n0.4\\n\\u200b\\n],\\n \\n                            [', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 234}),\n",
       " Document(page_content='\\u200b\\n0.02\\n\\u200b\\n, \\n\\u200b\\n0.9\\n\\u200b\\n, \\n\\u200b\\n0.08\\n\\u200b\\n]])\\n \\n \\nclass_targets \\n\\u200b\\n= \\n\\u200b\\nnp.array([\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n])\\n \\n \\n \\ndef \\n\\u200b\\nf1\\n\\u200b\\n():\\n \\n    softmax_loss \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n    softmax_loss.backward(softmax_outputs, class_targets)\\n \\n    dvalues1 \\n\\u200b\\n= \\n\\u200b\\nsoftmax_loss.dinputs\\n \\n \\n \\ndef \\n\\u200b\\nf2\\n\\u200b\\n():\\n \\n    activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n    activation.output \\n\\u200b\\n= \\n\\u200b\\nsoftmax_outputs\\n \\n    loss \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 234}),\n",
       " Document(page_content='loss.backward(softmax_outputs, class_targets)\\n \\n    activation.backward(loss.dinputs)\\n \\n    dvalues2 \\n\\u200b\\n= \\n\\u200b\\nactivation.dinputs', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 234}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n66\\n \\n \\nt1 \\n\\u200b\\n= \\n\\u200b\\ntimeit(\\n\\u200b\\nlambda\\n\\u200b\\n: f1(), \\n\\u200b\\nnumber\\n\\u200b\\n=\\n\\u200b\\n10000\\n\\u200b\\n)\\n \\nt2 \\n\\u200b\\n= \\n\\u200b\\ntimeit(\\n\\u200b\\nlambda\\n\\u200b\\n: f2(), \\n\\u200b\\nnumber\\n\\u200b\\n=\\n\\u200b\\n10000\\n\\u200b\\n)\\n \\nprint\\n\\u200b\\n(t2\\n\\u200b\\n/\\n\\u200b\\nt1)\\n \\n \\n \\n>>>\\n \\n6.922146504409747\\n \\n \\nCalculating the gradients separately is about 7 times slower. This factor can differ from a machine\\n \\nto a machine, but it clearly shows that it was worth putting in additional effort to calculate and', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 235}),\n",
       " Document(page_content=\"code the optimized solution of the combined loss and activation function derivative.\\n \\nLet’s take the code of the model and initialize the new class of combined accuracy and loss class’\\n \\nobject:\\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\nInstead of the previous:\\n \\n# Create Softmax activation (to be used with Dense layer):\\n \\nactivation2 \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n \\n# Create loss function\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 235}),\n",
       " Document(page_content='loss_function \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()\\n \\n \\nThen replace the forward pass calls over these objects:\\n \\n# Perform a forward pass through activation function\\n \\n# takes the output of second dense layer here\\n \\nactivation2.forward(dense2.output)\\n \\n \\n...\\n \\n \\n# Calculate sample losses from output of activation2 (softmax activation)\\n \\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_function.forward(activation2.output, y)\\n \\n \\nWith the forward pass call on the new object:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 235}),\n",
       " Document(page_content='# Perform a forward pass through the activation/loss function\\n \\n# takes the output of second dense layer here and returns loss\\n \\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 235}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n67\\n \\nAnd finally add the backward step and printing gradients:\\n \\n# Backward pass\\n \\nloss_activation.backward(loss_activation.output, y)\\n \\ndense2.backward(loss_activation.dinputs)\\n \\nactivation1.backward(dense2.dinputs)\\n \\ndense1.backward(activation1.dinputs)\\n \\n \\n# Print gradients\\n \\nprint\\n\\u200b\\n(dense1.dweights)\\n \\nprint\\n\\u200b\\n(dense1.dbiases)\\n \\nprint\\n\\u200b\\n(dense2.dweights)\\n \\nprint\\n\\u200b\\n(dense2.dbiases)\\n \\n \\nFull model code:\\n \\n# Create dataset', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 236}),\n",
       " Document(page_content='# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 3 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 3 input features (as we take output\\n \\n# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 236}),\n",
       " Document(page_content=\"3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\n# Perform a forward pass of our training data through this layer\\n \\ndense1.forward(X)\\n \\n \\n# Perform a forward pass through activation function\\n \\n# takes the output of first dense layer here\\n \\nactivation1.forward(dense1.output)\\n \\n \\n# Perform a forward pass through second Dense layer\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 236}),\n",
       " Document(page_content='# takes outputs of activation function of first layer as inputs\\n \\ndense2.forward(activation1.output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 236}),\n",
       " Document(page_content=\"Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n68\\n \\n# Perform a forward pass through the activation/loss function\\n \\n# takes the output of second dense layer here and returns loss\\n \\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)\\n \\n \\n# Let's see output of the first few samples:\\n \\nprint\\n\\u200b\\n(loss_activation.output[:\\n\\u200b\\n5\\n\\u200b\\n])\\n \\n \\n# Print loss value\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'loss:'\\n\\u200b\\n, loss)\\n \\n \\n# Calculate accuracy from output of activation2 and targets\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 237}),\n",
       " Document(page_content=\"# calculate values along first axis\\n \\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\nif \\n\\u200b\\nlen\\n\\u200b\\n(y.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n    y \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\naccuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n# Print accuracy\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'acc:'\\n\\u200b\\n, accuracy)\\n \\n \\n# Backward pass\\n \\nloss_activation.backward(loss_activation.output, y)\\n \\ndense2.backward(loss_activation.dinputs)\\n \\nactivation1.backward(dense2.dinputs)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 237}),\n",
       " Document(page_content='dense1.backward(activation1.dinputs)\\n \\n \\n# Print gradients\\n \\nprint\\n\\u200b\\n(dense1.dweights)\\n \\nprint\\n\\u200b\\n(dense1.dbiases)\\n \\nprint\\n\\u200b\\n(dense2.dweights)\\n \\nprint\\n\\u200b\\n(dense2.dbiases)\\n \\n \\n \\n>>>\\n \\n[[\\n\\u200b\\n0.33333334 0.33333334 0.33333334\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333316 0.3333332  0.33333364\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333287 0.3333329  0.33333418\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.3333326  0.33333263 0.33333477\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n0.33333233 0.3333324  0.33333528\\n\\u200b\\n]]\\n \\nloss: \\n\\u200b\\n1.0986104\\n \\nacc: \\n\\u200b\\n0.34\\n \\n[[ \\n\\u200b\\n1.5766358e-04  7.8368575e-05  4.7324404e-05\\n\\u200b\\n]\\n \\n [', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 237}),\n",
       " Document(page_content='\\u200b\\n]\\n \\n [ \\n\\u200b\\n1.8161036e-04  1.1045571e-05 \\n\\u200b\\n-\\n\\u200b\\n3.3096316e-05\\n\\u200b\\n]]\\n \\n[[\\n\\u200b\\n-\\n\\u200b\\n3.6055347e-04  9.6611722e-05 \\n\\u200b\\n-\\n\\u200b\\n1.0367142e-04\\n\\u200b\\n]]\\n \\n[[ \\n\\u200b\\n5.4410957e-05  1.0741142e-04 \\n\\u200b\\n-\\n\\u200b\\n1.6182236e-04\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n-\\n\\u200b\\n4.0791339e-05 \\n\\u200b\\n-\\n\\u200b\\n7.1678100e-05  1.1246944e-04\\n\\u200b\\n]\\n \\n [\\n\\u200b\\n-\\n\\u200b\\n5.3011299e-05  8.5817286e-05 \\n\\u200b\\n-\\n\\u200b\\n3.2805994e-05\\n\\u200b\\n]]\\n \\n[[\\n\\u200b\\n-\\n\\u200b\\n1.0732794e-05 \\n\\u200b\\n-\\n\\u200b\\n9.4590941e-06  2.0027626e-05\\n\\u200b\\n]]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 237}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n69\\n \\n \\nFull code up to this point:\\n \\nimport \\n\\u200b\\nnumpy \\n\\u200b\\nas \\n\\u200b\\nnp\\n \\nimport \\n\\u200b\\nnnfs\\n \\nfrom \\n\\u200b\\nnnfs.datasets \\n\\u200b\\nimport \\n\\u200b\\nspiral_data\\n \\n \\nnnfs.init()\\n \\n \\n \\n# Dense layer\\n \\nclass \\n\\u200b\\nLayer_Dense\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Layer initialization\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nn_inputs\\n\\u200b\\n, \\n\\u200b\\nn_neurons\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Initialize weights and biases\\n \\n        \\n\\u200b\\nself.weights \\n\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 238}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\n0.01 \\n\\u200b\\n* \\n\\u200b\\nnp.random.randn(n_inputs, n_neurons)\\n \\n        self.biases \\n\\u200b\\n= \\n\\u200b\\nnp.zeros((\\n\\u200b\\n1\\n\\u200b\\n, n_neurons))\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Remember input values\\n \\n        \\n\\u200b\\nself.inputs \\n\\u200b\\n= \\n\\u200b\\ninputs\\n \\n        \\n\\u200b\\n# Calculate output values from inputs, weights and biases\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nnp.dot(inputs, self.weights) \\n\\u200b\\n+ \\n\\u200b\\nself.biases\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 238}),\n",
       " Document(page_content='\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Gradients on parameters\\n \\n        \\n\\u200b\\nself.dweights \\n\\u200b\\n= \\n\\u200b\\nnp.dot(self.inputs.T, dvalues)\\n \\n        self.dbiases \\n\\u200b\\n= \\n\\u200b\\nnp.sum(dvalues, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n0\\n\\u200b\\n, \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n        \\n\\u200b\\n# Gradient on values\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nnp.dot(dvalues, self.weights.T)\\n \\n \\n \\n# ReLU activation\\n \\nclass \\n\\u200b\\nActivation_ReLU\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 238}),\n",
       " Document(page_content=\"Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n70\\n \\n        \\n\\u200b\\n# Remember input values\\n \\n        \\n\\u200b\\nself.inputs \\n\\u200b\\n= \\n\\u200b\\ninputs\\n \\n        \\n\\u200b\\n# Calculate output values from inputs\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nnp.maximum(\\n\\u200b\\n0\\n\\u200b\\n, inputs)\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Since we need to modify original variable,\\n \\n        # let's make a copy of values first\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\ndvalues.copy()\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 239}),\n",
       " Document(page_content='\\u200b\\ndvalues.copy()\\n \\n \\n        \\n\\u200b\\n# Zero gradient where input values were negative\\n \\n        \\n\\u200b\\nself.dinputs[self.inputs \\n\\u200b\\n<= \\n\\u200b\\n0\\n\\u200b\\n] \\n\\u200b\\n= \\n\\u200b\\n0\\n \\n \\n \\n# Softmax activation\\n \\nclass \\n\\u200b\\nActivation_Softmax\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Remember input values\\n \\n        \\n\\u200b\\nself.inputs \\n\\u200b\\n= \\n\\u200b\\ninputs\\n \\n \\n        \\n\\u200b\\n# Get unnormalized probabilities\\n \\n        \\n\\u200b\\nexp_values \\n\\u200b\\n= \\n\\u200b\\nnp.exp(inputs \\n\\u200b\\n- \\n\\u200b\\nnp.max(inputs, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 239}),\n",
       " Document(page_content='\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n))\\n \\n        \\n\\u200b\\n# Normalize them for each sample\\n \\n        \\n\\u200b\\nprobabilities \\n\\u200b\\n= \\n\\u200b\\nexp_values \\n\\u200b\\n/ \\n\\u200b\\nnp.sum(exp_values, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n,\\n \\n                                            \\n\\u200b\\nkeepdims\\n\\u200b\\n=\\n\\u200b\\nTrue\\n\\u200b\\n)\\n \\n \\n        self.output \\n\\u200b\\n= \\n\\u200b\\nprobabilities\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Create uninitialized array\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 239}),\n",
       " Document(page_content='\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nnp.empty_like(dvalues)\\n \\n \\n        \\n\\u200b\\n# Enumerate outputs and gradients\\n \\n        \\n\\u200b\\nfor \\n\\u200b\\nindex, (single_output, single_dvalues) \\n\\u200b\\nin \\n\\u200b\\n\\\\\\n \\n                \\n\\u200b\\nenumerate\\n\\u200b\\n(\\n\\u200b\\nzip\\n\\u200b\\n(self.output, dvalues)):\\n \\n            \\n\\u200b\\n# Flatten output array\\n \\n            \\n\\u200b\\nsingle_output \\n\\u200b\\n= \\n\\u200b\\nsingle_output.reshape(\\n\\u200b\\n-\\n\\u200b\\n1\\n\\u200b\\n, \\n\\u200b\\n1\\n\\u200b\\n)\\n \\n            \\n\\u200b\\n# Calculate Jacobian matrix of the output and\\n \\n            \\n\\u200b\\njacobian_matrix \\n\\u200b\\n= \\n\\u200b\\nnp.diagflat(single_output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 239}),\n",
       " Document(page_content='\\u200b\\n- \\n\\u200b\\n\\\\\\n \\n                              np.dot(single_output, single_output.T)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 239}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n71\\n \\n            \\n\\u200b\\n# Calculate sample-wise gradient\\n \\n            # and add it to the array of sample gradients\\n \\n            \\n\\u200b\\nself.dinputs[index] \\n\\u200b\\n= \\n\\u200b\\nnp.dot(jacobian_matrix,\\n \\n                                         single_dvalues)\\n \\n \\n \\n# Common loss class\\n \\nclass \\n\\u200b\\nLoss\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Calculates the data and regularization losses\\n \\n    # given model output and ground truth values\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\ncalculate\\n\\u200b\\n(\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 240}),\n",
       " Document(page_content='\\u200b\\ncalculate\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\noutput\\n\\u200b\\n, \\n\\u200b\\ny\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Calculate sample losses\\n \\n        \\n\\u200b\\nsample_losses \\n\\u200b\\n= \\n\\u200b\\nself.forward(output, y)\\n \\n \\n        \\n\\u200b\\n# Calculate mean loss\\n \\n        \\n\\u200b\\ndata_loss \\n\\u200b\\n= \\n\\u200b\\nnp.mean(sample_losses)\\n \\n \\n        \\n\\u200b\\n# Return loss\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\ndata_loss\\n \\n \\n \\n# Cross-entropy loss\\n \\nclass \\n\\u200b\\nLoss_CategoricalCrossentropy\\n\\u200b\\n(\\n\\u200b\\nLoss\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ny_pred\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 240}),\n",
       " Document(page_content='\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples in a batch\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(y_pred)\\n \\n \\n        \\n\\u200b\\n# Clip data to prevent division by 0\\n \\n        # Clip both sides to not drag mean towards any value\\n \\n        \\n\\u200b\\ny_pred_clipped \\n\\u200b\\n= \\n\\u200b\\nnp.clip(y_pred, \\n\\u200b\\n1e-7\\n\\u200b\\n, \\n\\u200b\\n1 \\n\\u200b\\n- \\n\\u200b\\n1e-7\\n\\u200b\\n)\\n \\n \\n        # Probabilities for target values -\\n \\n        # only if categorical labels\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n            correct_confidences \\n\\u200b\\n= \\n\\u200b\\ny_pred_clipped[', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 240}),\n",
       " Document(page_content='\\u200b\\ny_pred_clipped[\\n \\n                \\n\\u200b\\nrange\\n\\u200b\\n(samples),\\n \\n                y_true\\n \\n            ]', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 240}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n72\\n \\n        \\n\\u200b\\n# Mask values - only for one-hot encoded labels\\n \\n        \\n\\u200b\\nelif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n            correct_confidences \\n\\u200b\\n= \\n\\u200b\\nnp.sum(\\n \\n                y_pred_clipped \\n\\u200b\\n* \\n\\u200b\\ny_true,\\n \\n                \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n \\n            \\n\\u200b\\n)\\n \\n \\n        \\n\\u200b\\n# Losses\\n \\n        \\n\\u200b\\nnegative_log_likelihoods \\n\\u200b\\n= -\\n\\u200b\\nnp.log(correct_confidences)\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\\nnegative_log_likelihoods', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 241}),\n",
       " Document(page_content=\"\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues)\\n \\n        \\n\\u200b\\n# Number of labels in every sample\\n \\n        # We'll use the first sample to count them\\n \\n        \\n\\u200b\\nlabels \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues[\\n\\u200b\\n0\\n\\u200b\\n])\\n \\n \\n        \\n\\u200b\\n# If labels are sparse, turn them into one-hot vector\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n1\\n\\u200b\\n:\\n \\n            y_true \\n\\u200b\\n= \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 241}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\nnp.eye(labels)[y_true]\\n \\n \\n        \\n\\u200b\\n# Calculate gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= -\\n\\u200b\\ny_true \\n\\u200b\\n/ \\n\\u200b\\ndvalues\\n \\n        \\n\\u200b\\n# Normalize gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nself.dinputs \\n\\u200b\\n/ \\n\\u200b\\nsamples\\n \\n \\n \\n# Softmax classifier - combined Softmax activation\\n \\n# and cross-entropy loss for faster backward step\\n \\nclass \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy\\n\\u200b\\n():\\n \\n \\n    \\n\\u200b\\n# Creates activation and loss function objects\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 241}),\n",
       " Document(page_content=\"\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        self.activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax()\\n \\n        self.loss \\n\\u200b\\n= \\n\\u200b\\nLoss_CategoricalCrossentropy()\\n \\n \\n    \\n\\u200b\\n# Forward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nforward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ninputs\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n        \\n\\u200b\\n# Output layer's activation function\\n \\n        \\n\\u200b\\nself.activation.forward(inputs)\\n \\n        \\n\\u200b\\n# Set the output\\n \\n        \\n\\u200b\\nself.output \\n\\u200b\\n= \\n\\u200b\\nself.activation.output\\n \\n        \\n\\u200b\\n# Calculate and return loss value\\n \\n        \\n\\u200b\\nreturn \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 241}),\n",
       " Document(page_content='\\u200b\\nreturn \\n\\u200b\\nself.loss.calculate(self.output, y_true)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 241}),\n",
       " Document(page_content='Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n \\n73\\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nbackward\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\ndvalues\\n\\u200b\\n, \\n\\u200b\\ny_true\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# Number of samples\\n \\n        \\n\\u200b\\nsamples \\n\\u200b\\n= \\n\\u200b\\nlen\\n\\u200b\\n(dvalues)\\n \\n \\n        \\n\\u200b\\n# If labels are one-hot encoded,\\n \\n        # turn them into discrete values\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y_true.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n            y_true \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y_true, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 242}),\n",
       " Document(page_content='\\u200b\\n)\\n \\n \\n        \\n\\u200b\\n# Copy so we can safely modify\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\ndvalues.copy()\\n \\n        \\n\\u200b\\n# Calculate gradient\\n \\n        \\n\\u200b\\nself.dinputs[\\n\\u200b\\nrange\\n\\u200b\\n(samples), y_true] \\n\\u200b\\n-= \\n\\u200b\\n1\\n \\n        \\n\\u200b\\n# Normalize gradient\\n \\n        \\n\\u200b\\nself.dinputs \\n\\u200b\\n= \\n\\u200b\\nself.dinputs \\n\\u200b\\n/ \\n\\u200b\\nsamples\\n \\n \\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 3 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 242}),\n",
       " Document(page_content=\"Layer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 3 input features (as we take output\\n \\n# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n3\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 242}),\n",
       " Document(page_content='# Perform a forward pass of our training data through this layer\\n \\ndense1.forward(X)\\n \\n \\n# Perform a forward pass through activation function\\n \\n# takes the output of first dense layer here\\n \\nactivation1.forward(dense1.output)\\n \\n \\n# Perform a forward pass through second Dense layer\\n \\n# takes outputs of activation function of first layer as inputs\\n \\ndense2.forward(activation1.output)\\n \\n \\n# Perform a forward pass through the activation/loss function', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 242}),\n",
       " Document(page_content='# takes the output of second dense layer here and returns loss\\n \\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 242}),\n",
       " Document(page_content=\"Chapter 9 - Backpropagation - Neural Networks from Scratch in Python\\n74 \\n# Let's see output of the first few samples:  \\nprint\\u200b(loss_activation.output[:\\u200b5\\u200b])  \\n# Print loss value  \\nprint\\u200b(\\u200b'loss:'\\u200b, loss)  \\n# Calculate accuracy from output of activation2 and targets  \\n# calculate values along first axis  \\npredictions \\u200b= \\u200bnp.argmax(loss_activation.output, \\u200baxis \\u200b=\\u200b1\\u200b) \\nif \\u200blen\\u200b(y.shape) \\u200b== \\u200b2\\u200b:  \\n   y \\u200b= \\u200bnp.argmax(y, \\u200baxis \\u200b=\\u200b1\\u200b) \\naccuracy \\u200b= \\u200bnp.mean(predictions\\u200b==\\u200by)  \\n# Print accuracy\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 243}),\n",
       " Document(page_content=\"# Print accuracy  \\nprint\\u200b(\\u200b'acc:'\\u200b, accuracy)  \\n# Backward pass  \\nloss_activation.backward(loss_activation.output, y)  \\ndense2.backward(loss_activation.dinputs)  \\nactivation1.backward(dense2.dinputs)  \\ndense1.backward(activation1.dinputs)  \\n# Print gradients  \\nprint\\u200b(dense1.dweights)  \\nprint\\u200b(dense1.dbiases)  \\nprint\\u200b(dense2.dweights)  \\nprint\\u200b(dense2.dbiases)  \\nAt this point, thanks to gradients and backpropagation using the chain rule, we’re able to adjust\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 243}),\n",
       " Document(page_content='the weights and biases with the goal of lowering loss, but we’d be doing it in a very rudimentary  \\nway. This process of adjusting weights and biases using gradients to decrease loss is the job of the  \\noptimizer, which is the subject of the next chapter.  \\nSupplementary Material: \\u200bhttps://nnfs.io/ch9  \\nChapter code, further resources, and errata for this chapter.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 243}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n6\\n \\n \\n \\n \\n \\nChapter 10\\n \\nOptimizers\\n \\nOnce we have calculated the gradient, we can use this information to adjust weights and biases to\\n \\ndecrease the measure of loss. In a previous toy example, we showed how we could successfully\\n \\ndecrease a neuron’s activation function’s (ReLU) output in this manner. Recall that we subtracted\\n \\na fraction of the gradient for each weight and bias parameter. While very rudimentary, this is still', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 244}),\n",
       " Document(page_content='a commonly used optimizer called \\n\\u200b\\nStochastic Gradient Descent (SGD)\\n\\u200b\\n. As you will soon\\n \\ndiscover, most optimizers are just variants of SGD.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 244}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n7\\n \\n \\nStochastic Gradient Descent (SGD)\\n \\nThere are some naming conventions with this optimizer that can be confusing, so let’s walk\\n \\nthrough those first. You might hear the following names:\\n \\n-\\nStochastic Gradient Descent, SGD\\n \\n-\\nVanilla Gradient Descent, Gradient Descent, GD, or Batch Gradient Descent, BGD\\n \\n-\\nMini-batch Gradient Descent, MBGD\\n \\nThe first name, \\n\\u200b\\nStochastic Gradient Descent\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 245}),\n",
       " Document(page_content='\\u200b\\n, historically refers to an optimizer that fits a single\\n \\nsample at a time. The second optimizer, \\n\\u200b\\nBatch Gradient Descent\\n\\u200b\\n, is an optimizer used to fit a\\n \\nwhole dataset at once. The last optimizer, \\n\\u200b\\nMini-batch Gradient Descent\\n\\u200b\\n, is used to fit slices of a\\n \\ndataset, which we’d call batches in our context. The naming convention can be confusing here for\\n \\nmultiple reasons.\\n \\nFirst, in the context of deep learning and this book, we call slices of data \\n\\u200b\\nbatches\\n\\u200b\\n, where,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 245}),\n",
       " Document(page_content='\\u200b\\n, where,\\n \\nhistorically, the term to refer to slices of data in the context of Stochastic Gradient Descent was\\n \\nmini-batches\\n\\u200b\\n. In our context, it does not matter if the batch contains a single sample, a slice of\\n \\nthe dataset, or the full dataset — as a batch of the data. Additionally, with the current code, we are\\n \\nfitting the full dataset; following this naming convention, we would use \\n\\u200b\\nBatch Gradient Descent\\n\\u200b\\n.\\n \\nIn a future chapter, we’ll introduce data slices, or \\n\\u200b\\nbatches\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 245}),\n",
       " Document(page_content='\\u200b\\nbatches\\n\\u200b\\n, so we should start by using the\\n \\nMini-batch Gradient Descent\\n\\u200b\\n optimizer. That said, current naming trends and conventions with\\n \\nStochastic Gradient Descent in use with deep learning today have merged and normalized all of\\n \\nthese variants, to the point where we think of the \\n\\u200b\\nStochastic Gradient Descent\\n\\u200b\\n optimizer as one\\n \\nthat assumes a batch of data, whether that batch happens to be a single sample, every sample in a', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 245}),\n",
       " Document(page_content='dataset, or some subset of the full dataset at a time.\\n \\nIn the case of Stochastic Gradient Descent, we choose a learning rate, such as \\n\\u200b\\n1.0\\n\\u200b\\n. We then\\n \\nsubtract the \\n\\u200b\\nlearning_rate · parameter_gradients\\n\\u200b\\n from the actual parameter values. If our learning\\n \\nrate is 1, then we’re subtracting the exact amount of gradient from our parameters. We’re going to\\n \\nstart with 1 to see the results, but we’ll be diving more into the learning rate shortly. Let’s create', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 245}),\n",
       " Document(page_content='the SGD optimizer class code. The initialization method will take hyper-parameters, starting with\\n \\nthe learning rate, for now, storing them in the class’ properties. The \\n\\u200b\\nupdate_params\\n\\u200b\\n method,\\n \\ngiven a layer object, performs the most basic optimization, the same way that we performed it in\\n \\nthe previous chapter — it multiplies the gradients stored in the layers by the negated learning rate', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 245}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n8\\n \\nand adds the result to the layer’s parameters. It seems that, in the previous chapter, we performed\\n \\nSGD optimization without knowing it. The full class so far:\\n \\nclass \\n\\u200b\\nOptimizer_SGD\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Initialize optimizer - set settings,\\n \\n    # learning rate of 1. is default for this optimizer\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlearning_rate\\n\\u200b\\n=\\n\\u200b\\n1.0\\n\\u200b\\n):\\n \\n        self.learning_rate \\n\\u200b\\n= \\n\\u200b\\nlearning_rate', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 246}),\n",
       " Document(page_content='\\u200b\\nlearning_rate\\n \\n \\n    \\n\\u200b\\n# Update parameters\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nupdate_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlayer\\n\\u200b\\n):\\n \\n        layer.weights \\n\\u200b\\n+= -\\n\\u200b\\nself.learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dweights\\n \\n        layer.biases \\n\\u200b\\n+= -\\n\\u200b\\nself.learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dbiases\\n \\n \\nTo use this, we need to create an optimizer object:\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_SGD()\\n \\n \\nThen update our network layer’s parameters after calculating the gradient using:\\n \\noptimizer.update_params(dense1)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 246}),\n",
       " Document(page_content='optimizer.update_params(dense2)\\n \\n \\nRecall that the layer object contains its parameters (weights and biases) and also, at this stage, the\\n \\ngradient that is calculated during backpropagation. We store these in the layer’s properties so that\\n \\nthe optimizer can make use of them. In our main neural network code, we’d bring the\\n \\noptimization in after backpropagation. Let’s make a 1x64 densely-connected neural network (1\\n \\nhidden layer with 64 neurons) and use the same dataset as before:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 246}),\n",
       " Document(page_content='# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 64 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 64 input features (as we take output\\n \\n \\n# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 246}),\n",
       " Document(page_content=\"\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 246}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n9\\n \\nThe next step is to create the optimizer’s object:\\n \\n# Create optimizer\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_SGD()\\n \\n \\nThen perform a \\n\\u200b\\nforward pass\\n\\u200b\\n of our sample data:\\n \\n# Perform a forward pass of our training data through this layer\\n \\ndense1.forward(X)\\n \\n \\n# Perform a forward pass through activation function\\n \\n# takes the output of first dense layer here\\n \\nactivation1.forward(dense1.output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 247}),\n",
       " Document(page_content=\"# Perform a forward pass through second Dense layer\\n \\n# takes outputs of activation function of first layer as inputs\\n \\ndense2.forward(activation1.output)\\n \\n \\n# Perform a forward pass through the activation/loss function\\n \\n# takes the output of second dense layer here and returns loss\\n \\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)\\n \\n \\n# Let's print loss value\\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'loss:'\\n\\u200b\\n, loss)\\n \\n \\n# Calculate accuracy from output of activation2 and targets\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 247}),\n",
       " Document(page_content=\"# calculate values along first axis\\n \\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\nif \\n\\u200b\\nlen\\n\\u200b\\n(y.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n    y \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\naccuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\nprint\\n\\u200b\\n(\\n\\u200b\\n'acc:'\\n\\u200b\\n, accuracy)\\n \\n \\nNext, we do our \\n\\u200b\\nbackward pass\\n\\u200b\\n, which is also called \\n\\u200b\\nbackpropagation\\n\\u200b\\n:\\n \\n# Backward pass\\n \\nloss_activation.backward(loss_activation.output, y)\\n \\ndense2.backward(loss_activation.dinputs)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 247}),\n",
       " Document(page_content='activation1.backward(dense2.dinputs)\\n \\ndense1.backward(activation1.dinputs)\\n \\n \\nThen we finally use our optimizer to update weights and biases:\\n \\n# Update weights and biases\\n \\noptimizer.update_params(dense1)\\n \\noptimizer.update_params(dense2)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 247}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n10\\n \\nThis is everything we need to train our model! But why would we only perform this optimization\\n \\nonce, when we can perform it lots of times by leveraging Python’s looping capabilities? We will\\n \\nrepeatedly perform a forward pass, backward pass, and optimization until we reach some stopping\\n \\npoint. Each full pass through all of the training data is called an \\n\\u200b\\nepoch\\n\\u200b\\n. In most deep learning', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 248}),\n",
       " Document(page_content='tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to\\n \\nhave a perfect model with ideal weights and biases after only one epoch. To add multiple epochs\\n \\nof training into our code, we will initialize our model and run a loop around all the code\\n \\nperforming the forward pass, backward pass, and optimization calculations:\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 248}),\n",
       " Document(page_content=\"\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 64 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 64 input features (as we take output\\n \\n \\n# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 248}),\n",
       " Document(page_content='loss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\n# Create optimizer\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_SGD()\\n \\n \\n# Train in loop\\n \\nfor \\n\\u200b\\nepoch \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10001\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of our training data through this layer\\n \\n    \\n\\u200b\\ndense1.forward(X)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\\n \\n    # takes the output of first dense layer here\\n \\n    \\n\\u200b\\nactivation1.forward(dense1.output)\\n \\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 248}),\n",
       " Document(page_content='\\u200b\\n# Perform a forward pass through second Dense layer\\n \\n    # takes outputs of activation function of first layer as inputs\\n \\n    \\n\\u200b\\ndense2.forward(activation1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through the activation/loss function\\n \\n    # takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 248}),\n",
       " Document(page_content=\"Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n11\\n \\n    \\n\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis\\n \\n    \\n\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n        y \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n    \\n\\u200b\\nif not \\n\\u200b\\nepoch \\n\\u200b\\n% \\n\\u200b\\n100\\n\\u200b\\n:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\nf\\n\\u200b\\n'epoch: \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 249}),\n",
       " Document(page_content=\"(\\n\\u200b\\nf\\n\\u200b\\n'epoch: \\n\\u200b\\n{epoch}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'acc: \\n\\u200b\\n{accuracy\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'loss: \\n\\u200b\\n{loss\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n'\\n\\u200b\\n)\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\nloss_activation.backward(loss_activation.output, y)\\n \\n    dense2.backward(loss_activation.dinputs)\\n \\n    activation1.backward(dense2.dinputs)\\n \\n    dense1.backward(activation1.dinputs)\\n \\n \\n    \\n\\u200b\\n# Update weights and biases\\n \\n    \\n\\u200b\\noptimizer.update_params(dense1)\\n \\n    optimizer.update_params(dense2)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 249}),\n",
       " Document(page_content='This gives us an update of where we are (epochs), the model’s accuracy, and loss every 100\\n \\nepochs. Initially, we can see consistent improvement:\\n \\nepoch: \\n\\u200b\\n0\\n\\u200b\\n, acc: \\n\\u200b\\n0.360\\n\\u200b\\n, loss: \\n\\u200b\\n1.099\\n \\nepoch: \\n\\u200b\\n100\\n\\u200b\\n, acc: \\n\\u200b\\n0.400\\n\\u200b\\n, loss: \\n\\u200b\\n1.087\\n \\nepoch: \\n\\u200b\\n200\\n\\u200b\\n, acc: \\n\\u200b\\n0.417\\n\\u200b\\n, loss: \\n\\u200b\\n1.077\\n \\n...\\n \\nepoch: \\n\\u200b\\n1000\\n\\u200b\\n, acc: \\n\\u200b\\n0.407\\n\\u200b\\n, loss: \\n\\u200b\\n1.058\\n \\n...\\n \\nepoch: \\n\\u200b\\n2000\\n\\u200b\\n, acc: \\n\\u200b\\n0.403\\n\\u200b\\n, loss: \\n\\u200b\\n1.038\\n \\nepoch: \\n\\u200b\\n2100\\n\\u200b\\n, acc: \\n\\u200b\\n0.447\\n\\u200b\\n, loss: \\n\\u200b\\n1.022\\n \\nepoch: \\n\\u200b\\n2200', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 249}),\n",
       " Document(page_content='epoch: \\n\\u200b\\n2200\\n\\u200b\\n, acc: \\n\\u200b\\n0.467\\n\\u200b\\n, loss: \\n\\u200b\\n1.023\\n \\nepoch: \\n\\u200b\\n2300\\n\\u200b\\n, acc: \\n\\u200b\\n0.437\\n\\u200b\\n, loss: \\n\\u200b\\n1.005\\n \\nepoch: \\n\\u200b\\n2400\\n\\u200b\\n, acc: \\n\\u200b\\n0.497\\n\\u200b\\n, loss: \\n\\u200b\\n0.993\\n \\nepoch: \\n\\u200b\\n2500\\n\\u200b\\n, acc: \\n\\u200b\\n0.513\\n\\u200b\\n, loss: \\n\\u200b\\n0.981\\n \\n...\\n \\nepoch: \\n\\u200b\\n9500\\n\\u200b\\n, acc: \\n\\u200b\\n0.590\\n\\u200b\\n, loss: \\n\\u200b\\n0.865\\n \\nepoch: \\n\\u200b\\n9600\\n\\u200b\\n, acc: \\n\\u200b\\n0.627\\n\\u200b\\n, loss: \\n\\u200b\\n0.863\\n \\nepoch: \\n\\u200b\\n9700\\n\\u200b\\n, acc: \\n\\u200b\\n0.630\\n\\u200b\\n, loss: \\n\\u200b\\n0.830\\n \\nepoch: \\n\\u200b\\n9800\\n\\u200b\\n, acc: \\n\\u200b\\n0.663\\n\\u200b\\n, loss: \\n\\u200b\\n0.844\\n \\nepoch: \\n\\u200b\\n9900\\n\\u200b\\n, acc: \\n\\u200b\\n0.627\\n\\u200b\\n, loss: \\n\\u200b\\n0.820', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 249}),\n",
       " Document(page_content=', loss: \\n\\u200b\\n0.820\\n \\nepoch: \\n\\u200b\\n10000\\n\\u200b\\n, acc: \\n\\u200b\\n0.633\\n\\u200b\\n, loss: \\n\\u200b\\n0.848\\n \\n \\nAdditionally, we’ve prepared animations to help visualize the training process and to convey the\\n \\nimpact of various optimizers and their hyperparameters. The left part of the animation canvas', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 249}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n12\\n \\ncontains dots, where color represents each of the 3 classes of the data, the coordinates are\\n \\nfeatures, and the background colors show the model prediction areas. Ideally, the points’ colors\\n \\nand the background should match if the model classifies correctly. The surrounding area should\\n \\nalso follow the data’s “trend” — which is what we’d call generalization — the ability of the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 250}),\n",
       " Document(page_content='model to correctly predict unseen data. The colorful squares on the right show weights and biases\\n \\n— red for positive and blue for negative values. The matching areas right below the Dense 1 bar\\n \\nand next to the Dense 2 bar show the updates that the optimizer performs to the layers. The\\n \\nupdates might look overly strong compared to the weights and biases, but that’s because we’ve\\n \\nvisually normalized them to the maximum value, or else they would be almost invisible since the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 250}),\n",
       " Document(page_content='updates are quite small at a time. The other 3 graphs show the loss, accuracy, and current learning\\n \\nrate values in conjunction with the training time, epochs in this case.\\n \\n \\nFig 10.01:\\n\\u200b\\n Model training with Stochastic Gradient Descent optimizer.\\n \\nEpilepsy Warning, there are quick flashing colors in the animation:\\n \\n \\nAnim 10.01:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/pup', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 250}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n13\\n \\nOur neural network mostly stays stuck at around a loss of 1 and later 0.85-0.9, and an accuracy\\n \\naround 0.60. The animation also has a “flashy wiggle” effect, which most likely means we chose\\n \\ntoo high of a learning rate. Given that loss didn’t decrease much, we can assume that this learning\\n \\nrate, being too high, also caused the model to get stuck in a \\n\\u200b\\nlocal minimum\\n\\u200b\\n, which we’ll learn', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 251}),\n",
       " Document(page_content='more about soon. Iterating over more epochs doesn’t seem helpful at this point, which tells us that\\n \\nwe’re likely stuck with our optimization. Does this mean that this is the most we can get from our\\n \\noptimizer on this dataset?\\n \\nRecall that we’re adjusting our weights and biases by applying some fraction, in this case, \\n\\u200b\\n1.0\\n\\u200b\\n, to\\n \\nthe gradient and subtracting this from the weights and biases. This fraction is called the \\n\\u200b\\nlearning\\n \\nrate\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 251}),\n",
       " Document(page_content='\\u200b\\nlearning\\n \\nrate\\n\\u200b\\n (LR) and is the primary adjustable parameter for the optimizer as it decreases loss. To gain\\n \\nan intuition for adjusting, planning, or initially setting the learning rate, we should first understand\\n \\nhow the learning rate affects the optimizer and output of the loss function.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 251}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n14\\n \\n \\nLearning Rate\\n \\nSo far, we have a gradient of a model and the loss function with respect to all of the parameters,\\n \\nand we want to apply a fraction of this gradient to the parameters in order to descend the loss\\n \\nvalue.\\n \\n \\nIn most cases, we won’t apply the negative gradient as is, as the direction of the function’s\\n \\nsteepest descent will be continuously changing, and these values will usually be too big for', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 252}),\n",
       " Document(page_content='meaningful model improvements to occur. Instead, we want to perform small steps — calculating\\n \\nthe gradient, updating parameters by a negative fraction of this gradient, and repeating this in a\\n \\nloop. Small steps ensure that we are following the direction of the steepest descent, but these steps\\n \\ncan also be too small, causing learning stagnation — we’ll explain this shortly.\\n \\n \\nLet’s forget, for a while, that we are performing gradient descent of an n-dimensional function', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 252}),\n",
       " Document(page_content='(our loss function), where n is the number parameters (weights and biases) that the model\\n \\ncontains, and assume that we have just one dimension to the loss function (a singular input). Our\\n \\ngoal for the following images and animations is to visualize some concepts and gain an intuition;\\n \\nthus, we will not use or present certain optimizer settings, and instead will be considering things\\n \\nin more general terms. That said, we’ve used a real SGD optimizer on a real function to prepare', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 252}),\n",
       " Document(page_content='all of the following examples. Here’s the function where we want to determine what input to it\\n \\nwill result in the lowest possible output:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 252}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n15\\n \\n \\nFig 10.02:\\n\\u200b\\n Example function to minimize the output.\\n \\nWe can see the \\n\\u200b\\nglobal minimum\\n\\u200b\\n of this function, which is the lowest possible \\n\\u200b\\ny\\n\\u200b\\n value that this\\n \\nfunction can output. This is the goal — to minimize the function’s output to find the global\\n \\nminimum. The values of the axes are not important in this case. The goal is only to show the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 253}),\n",
       " Document(page_content='function and the learning rate concept. Also, remember that this one-dimensional function\\n \\nexample is being used merely to aid in visualization. It would be easy to solve this function with\\n \\nsimpler math than what is required to solve the much larger n-dimensional loss function for neural\\n \\nnetworks, where n (which is the number of weights and biases) can be in the millions or even\\n \\nbillions (or more). When we have millions of, or more, dimensions, gradient descent is the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 253}),\n",
       " Document(page_content='best-known way to search for a global minimum.\\n \\nWe’ll start descending from the left side of this graph. With an example learning rate:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 253}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n16\\n \\n \\nFig 10.03:\\n\\u200b\\n Stuck in the first local minimum.\\n \\n \\nAnim 10.03:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/and\\n \\nThe learning rate turned out to be too small. Small updates to the parameters caused stagnation in\\n \\nthe model’s learning — the model got stuck in a local minimum. The \\n\\u200b\\nlocal minimum\\n\\u200b\\n is a\\n \\nminimum that is near where we look but isn’t necessarily the global minimum, which is the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 254}),\n",
       " Document(page_content='absolute lowest point for a function. With our example here, as well as with optimizing full neural\\n \\nnetworks, we do not know where the global minimum is. How do we know if we’ve reached the\\n \\nglobal minimum or at least gotten close? The loss function measures how far the model is with its\\n \\npredictions to the real target values, so, as long as the loss value is not \\n\\u200b\\n0\\n\\u200b\\n or very close to \\n\\u200b\\n0\\n\\u200b\\n, and the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 254}),\n",
       " Document(page_content='\\u200b\\n0\\n\\u200b\\n, and the\\n \\nmodel stopped learning, we’re at some local minimum.  In reality, we almost never approach a\\n \\nloss of \\n\\u200b\\n0\\n\\u200b\\n for various reasons. One reason for this may be imperfect neural network\\n \\nhyperparameters. Another reason for this may be insufficient data. If you did reach a loss of 0\\n \\nwith a neural network, you should find it suspicious, for reasons we’ll get into later in this book.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 254}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n17\\n \\nWe can try to modify the learning rate:\\n \\n \\nFig 10.04:\\n\\u200b\\n Stuck in the second local minimum.\\n \\n \\nAnim 10.04:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/xor\\n \\nThis time, the model escaped this local minimum but got stuck at another one. Let’s see one more\\n \\nexample after another learning rate change:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 255}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n18\\n \\n \\nFig 10.05:\\n\\u200b\\n Stuck in the third local minimum, near the global minimum.\\n \\n \\nAnim 10.05: \\n\\u200b\\nhttps://nnfs.io/tho\\n \\nThis time the model got stuck at a local minimum near the global minimum. The model was able\\n \\nto escape the “deeper” local minimums, so it might be counter-intuitive why it is stuck here.\\n \\nRemember, the model follows the direction of steepest descent of the loss function, no matter how', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 256}),\n",
       " Document(page_content='large or slight the descent is. For this reason, we’ll introduce momentum and the other techniques\\n \\nto prevent such situations.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 256}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n19\\n \\nMomentum, in an optimizer, adds to the gradient what, in the physical world, we could call inertia\\n \\n— for example, we can throw a ball uphill and, with a small enough hill or big enough applied\\n \\nforce, the ball can roll-over to the other side of the hill. Let’s see how this might look with the\\n \\nmodel in training:\\n \\n \\nFig 10.06:\\n\\u200b\\n Reached the global minimum, too low learning rate.\\n \\n \\nAnim 10.06: \\n\\u200b\\nhttps://nnfs.io/pog', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 257}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n20\\n \\nWe used a very small learning rate here with a large momentum. The color change from green,\\n \\nthrough orange to red presents the advancement of the gradient descent process, the steps. We can\\n \\nsee that the model achieved the goal and found the global minimum, but this took many steps.\\n \\nCan this be done better?\\n \\n \\nFig 10.07:\\n\\u200b\\n Reached the global minimum, better learning rate.\\n \\n \\nAnim 10.07: \\n\\u200b\\nhttps://nnfs.io/jog', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 258}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n21\\n \\nAnd even further:\\n \\n \\nFig 10.08:\\n\\u200b\\n Reached the global minimum, significantly better learning rate.\\n \\n \\nAnim 10.08:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/mog', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 259}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n22\\n \\nWith these examples, we were able to find the global minimum in about 200, 100, and 50 steps,\\n \\nrespectively, by modifying the learning rate and the momentum. It’s possible to significantly\\n \\nshorten the training time by adjusting the parameters of the optimizer. However, we have to be\\n \\ncareful with these hyper-parameter adjustments, as this won’t necessarily always help the model:\\n \\n \\nFig 10.09:\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 260}),\n",
       " Document(page_content='Fig 10.09:\\n\\u200b\\n Unstable model, learning rate too big.\\n \\n \\nAnim 10.09: \\n\\u200b\\nhttps://nnfs.io/log\\n \\nWith the learning rate set too high, the model might not be able to find the global minimum.\\n \\nEven, at some point, if it does, further adjustments could cause it to jump out of this minimum.\\n \\nWe’ll see this behavior later in this chapter — try to take a close look at results and see if you can\\n \\nfind it, as well as the other issues we’ve described, from the different optimizers as we work', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 260}),\n",
       " Document(page_content='through them.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 260}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n23\\n \\nIn this case, the model was “jumping” around some minimum and what this might mean is that\\n \\nwe should try to lower the learning rate, raise the momentum, or possibly apply a learning rate\\n \\ndecay (lowering the learning rate during training), which we’ll describe in this chapter. If we set\\n \\nthe learning rate far too high:\\n \\n \\nFig 10.10:\\n\\u200b\\n Unstable model, learning rate significantly too big.\\n \\n \\nAnim 10.10: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 261}),\n",
       " Document(page_content='Anim 10.10: \\n\\u200b\\nhttps://nnfs.io/sog', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 261}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n24\\n \\nIn this situation, the model starts “jumping” around, and moves in what we might observe as\\n \\nrandom directions. This is an example of “overshooting,” with every step — the direction of a\\n \\nchange is correct, but the amount of the gradient applied is too large. In an extreme situation, we\\n \\ncould cause a \\n\\u200b\\ngradient explosion\\n\\u200b\\n:\\n \\n \\nFig 10.11:\\n\\u200b\\n Broken model, learning rate critically too big.\\n \\n \\nAnim 10.11:\\n\\u200b\\n \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 262}),\n",
       " Document(page_content='Anim 10.11:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/bog\\n \\nA gradient explosion is a situation where the parameter updates cause the function’s output to rise\\n \\ninstead of fall, and, with each step, the loss value and gradient become larger. At some point, the\\n \\nfloating-point variable limitation causes an overflow as it cannot hold values of this size anymore,\\n \\nand the model is no longer able to train. It’s crucial to recognize this situation forming during', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 262}),\n",
       " Document(page_content='training, especially for large models, where the training can take days, weeks, or more. It is\\n \\npossible to tune the model’s hyper-parameters in time to save the model and to continue training.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 262}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n25\\n \\nWhen we choose the learning rate and the other hyper-parameters correctly, the learning process\\n \\ncan be relatively quick:\\n \\n \\nFig 10.12:\\n\\u200b\\n Model learned, good learning rate, can be better.\\n \\n \\nAnim 10.12: \\n\\u200b\\nhttps://nnfs.io/cog', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 263}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n26\\n \\nThis time it took significantly less time for the model to find the global minimum, but it can\\n \\nalways be better:\\n \\n \\nFig 10.13:\\n\\u200b\\n An efficient learning example.\\n \\n \\nAnim 10.13: \\n\\u200b\\nhttps://nnfs.io/rog\\n \\nThis time the model needed just a few steps to find the global minimum. The challenge is to\\n \\nchoose the hyper-parameters correctly, and it is not always an easy task. It is usually best to', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 264}),\n",
       " Document(page_content='start with the optimizer defaults, perform a few steps, and observe the training process when\\n \\ntuning different settings. It is not always possible to see meaningful results in a short-enough\\n \\nperiod of time, and, in this case, it’s good to have the ability to update the optimizer’s settings\\n \\nduring training. How you choose the learning rate, and other hyper-parameters, depends on the\\n \\nmodel, data, including the amount of data, the parameter initialization method, etc. There is no', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 264}),\n",
       " Document(page_content='single, best way to set hyper-parameters, but experience usually helps. As we mentioned, one', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 264}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n27\\n \\nof the challenges during the training of a neural network model is to choose the right settings.\\n \\nThe', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 265}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n28\\n \\ndifference can be anything from a model not learning at all to learning very well.\\n \\nFor a summary of learning rates — if we plot the loss along an axis of steps:\\n \\n \\nFig 10.14:\\n\\u200b\\n Graphs of the loss in a function of steps, different rates\\n \\nWe can see various examples of relative learning rates and what loss will ideally look like as a\\n \\ngraph over time (steps) of training.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 266}),\n",
       " Document(page_content='Knowing what the learning rate should be to get the most out of your training process isn’t\\n \\npossible, but a good rule is that your initial training will benefit from a larger learning rate to take\\n \\ninitial steps faster. If you start with steps that are too small, you might get stuck in a local\\n \\nminimum and be unable to leave it due to not making large enough updates to the parameters.\\n \\nFor example, what if we make the learning rate 0.85 rather than 1.0 with the SGD optimizer?', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 266}),\n",
       " Document(page_content='# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 64 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 266}),\n",
       " Document(page_content=\"Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n29\\n \\n# Create second Dense layer with 64 input features (as we take output\\n \\n# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\n# Create optimizer\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_SGD(\\n\\u200b\\nlearning_rate\\n\\u200b\\n=\\n\\u200b\\n.85\\n\\u200b\\n)\\n \\n \\n# Train in loop\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 267}),\n",
       " Document(page_content='# Train in loop\\n \\nfor \\n\\u200b\\nepoch \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10001\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of our training data through this layer\\n \\n    \\n\\u200b\\ndense1.forward(X)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\\n \\n    # takes the output of first dense layer here\\n \\n    \\n\\u200b\\nactivation1.forward(dense1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through second Dense layer\\n \\n    # takes outputs of activation function of first layer as inputs\\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 267}),\n",
       " Document(page_content='\\u200b\\ndense2.forward(activation1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through the activation/loss function\\n \\n    # takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)\\n \\n \\n    \\n\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis\\n \\n    \\n\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 267}),\n",
       " Document(page_content=\"\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n        y \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n    \\n\\u200b\\nif not \\n\\u200b\\nepoch \\n\\u200b\\n% \\n\\u200b\\n100\\n\\u200b\\n:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\nf\\n\\u200b\\n'epoch: \\n\\u200b\\n{epoch}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'acc: \\n\\u200b\\n{accuracy\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'loss: \\n\\u200b\\n{loss\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n'\\n\\u200b\\n)\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\nloss_activation.backward(loss_activation.output, y)\\n \\n    dense2.backward(loss_activation.dinputs)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 267}),\n",
       " Document(page_content='activation1.backward(dense2.dinputs)\\n \\n    dense1.backward(activation1.dinputs)\\n \\n \\n    \\n\\u200b\\n# Update weights and biases\\n \\n    \\n\\u200b\\noptimizer.update_params(dense1)\\n \\n    optimizer.update_params(dense2)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 267}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n30\\n \\n>>>\\n \\nepoch: \\n\\u200b\\n0\\n\\u200b\\n, acc: \\n\\u200b\\n0.360\\n\\u200b\\n, loss: \\n\\u200b\\n1.099\\n \\nepoch: \\n\\u200b\\n100\\n\\u200b\\n, acc: \\n\\u200b\\n0.403\\n\\u200b\\n, loss: \\n\\u200b\\n1.091\\n \\n...\\n \\nepoch: \\n\\u200b\\n2000\\n\\u200b\\n, acc: \\n\\u200b\\n0.437\\n\\u200b\\n, loss: \\n\\u200b\\n1.053\\n \\nepoch: \\n\\u200b\\n2100\\n\\u200b\\n, acc: \\n\\u200b\\n0.443\\n\\u200b\\n, loss: \\n\\u200b\\n1.026\\n \\nepoch: \\n\\u200b\\n2200\\n\\u200b\\n, acc: \\n\\u200b\\n0.377\\n\\u200b\\n, loss: \\n\\u200b\\n1.050\\n \\nepoch: \\n\\u200b\\n2300\\n\\u200b\\n, acc: \\n\\u200b\\n0.433\\n\\u200b\\n, loss: \\n\\u200b\\n1.016\\n \\nepoch: \\n\\u200b\\n2400\\n\\u200b\\n, acc: \\n\\u200b\\n0.460\\n\\u200b\\n, loss: \\n\\u200b\\n1.000\\n \\nepoch: \\n\\u200b\\n2500\\n\\u200b\\n, acc: \\n\\u200b\\n0.493\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 268}),\n",
       " Document(page_content='\\u200b\\n, acc: \\n\\u200b\\n0.493\\n\\u200b\\n, loss: \\n\\u200b\\n1.010\\n \\nepoch: \\n\\u200b\\n2600\\n\\u200b\\n, acc: \\n\\u200b\\n0.527\\n\\u200b\\n, loss: \\n\\u200b\\n0.998\\n \\nepoch: \\n\\u200b\\n2700\\n\\u200b\\n, acc: \\n\\u200b\\n0.523\\n\\u200b\\n, loss: \\n\\u200b\\n0.977\\n \\n...\\n \\nepoch: \\n\\u200b\\n7100\\n\\u200b\\n, acc: \\n\\u200b\\n0.577\\n\\u200b\\n, loss: \\n\\u200b\\n0.941\\n \\nepoch: \\n\\u200b\\n7200\\n\\u200b\\n, acc: \\n\\u200b\\n0.550\\n\\u200b\\n, loss: \\n\\u200b\\n0.921\\n \\nepoch: \\n\\u200b\\n7300\\n\\u200b\\n, acc: \\n\\u200b\\n0.593\\n\\u200b\\n, loss: \\n\\u200b\\n0.943\\n \\nepoch: \\n\\u200b\\n7400\\n\\u200b\\n, acc: \\n\\u200b\\n0.593\\n\\u200b\\n, loss: \\n\\u200b\\n0.940\\n \\nepoch: \\n\\u200b\\n7500\\n\\u200b\\n, acc: \\n\\u200b\\n0.557\\n\\u200b\\n, loss: \\n\\u200b\\n0.907\\n \\nepoch: \\n\\u200b\\n7600\\n\\u200b\\n, acc: \\n\\u200b\\n0.590\\n\\u200b\\n, loss: \\n\\u200b\\n0.949\\n \\nepoch: \\n\\u200b\\n7700\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 268}),\n",
       " Document(page_content='epoch: \\n\\u200b\\n7700\\n\\u200b\\n, acc: \\n\\u200b\\n0.590\\n\\u200b\\n, loss: \\n\\u200b\\n0.935\\n \\n...\\n \\nepoch: \\n\\u200b\\n9100\\n\\u200b\\n, acc: \\n\\u200b\\n0.597\\n\\u200b\\n, loss: \\n\\u200b\\n0.860\\n \\nepoch: \\n\\u200b\\n9200\\n\\u200b\\n, acc: \\n\\u200b\\n0.630\\n\\u200b\\n, loss: \\n\\u200b\\n0.842\\n \\n...\\n \\nepoch: \\n\\u200b\\n10000\\n\\u200b\\n, acc: \\n\\u200b\\n0.657\\n\\u200b\\n, loss: \\n\\u200b\\n0.816\\n \\n \\n \\nFig 10.15:\\n\\u200b\\n Model training with SGD optimizer and lowered learning rate.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 268}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n31\\n \\nEpilepsy Warning (quick flashing colors).\\n \\n \\nAnim 10.15:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/cup\\n \\nAs you can see, the neural network did slightly better in terms of accuracy, and it achieved a\\n \\nlower loss; lower loss is not always associated with higher accuracy. Remember, even if we desire\\n \\nthe best accuracy out of our model, the optimizer’s task is to decrease loss, not raise accuracy', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 269}),\n",
       " Document(page_content='directly. Loss is the mean value of all of the sample losses, and some of them could drop\\n \\nsignificantly, while others might rise just slightly, changing the prediction for them from a correct\\n \\nto an incorrect class at the same time. This would cause a lower mean loss in general, but also\\n \\nmore incorrectly predicted samples, which will, at the same time, lower the accuracy. A likely\\n \\nreason for this model’s lower accuracy is that it found another local minimum by chance — the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 269}),\n",
       " Document(page_content='descent path has changed, due to smaller steps. In a direct comparison of these two models in\\n \\ntraining, different learning rates did not show that the lower this learning rate value is, the better.\\n \\nIn most cases, we want to start with a larger learning rate and decrease the learning rate over\\n \\ntime/steps.\\n \\n \\nA commonly-used solution to keep initial updates large and explore various learning rates during\\n \\ntraining is to implement a \\n\\u200b\\nlearning rate decay\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 269}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n32\\n \\n \\nLearning Rate Decay\\n \\nThe idea of a \\n\\u200b\\nlearning rate decay\\n\\u200b\\n is to start with a large learning rate, say 1.0 in our case, and\\n \\nthen decrease it during training. There are a few methods for doing this. One is to decrease the\\n \\nlearning rate in response to the loss across epochs — for example, if the loss begins to level out/\\n \\nplateau or starts “jumping” over large deltas. You can either program this behavior-monitoring', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 270}),\n",
       " Document(page_content='logically or simply track your loss over time and manually decrease the learning rate when you\\n \\ndeem it appropriate. Another option, which we will implement, is to program a \\n\\u200b\\nDecay Rate\\n\\u200b\\n,\\n \\nwhich steadily decays the learning rate per batch or epoch.\\n \\nLet’s plan to decay per step. This can also be referred to as \\n\\u200b\\n1/t decaying\\n\\u200b\\n or \\n\\u200b\\nexponential\\n \\ndecaying\\n\\u200b\\n. Basically, we’re going to update the learning rate each step by the reciprocal of the', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 270}),\n",
       " Document(page_content='step count fraction. This fraction is a new hyper-parameter that we’ll add to the optimizer, called\\n \\nthe \\n\\u200b\\nlearning rate decay\\n\\u200b\\n. How this decaying works is it takes the step and the decaying ratio and\\n \\nmultiplies them. The further in training, the bigger the step is, and the bigger result of this\\n \\nmultiplication is. We then take its reciprocal (the further in training, the lower the value) and\\n \\nmultiply the initial learning rate by it. The added \\n\\u200b\\n1\\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 270}),\n",
       " Document(page_content='\\u200b\\n1\\n\\u200b\\n makes sure that the resulting algorithm never\\n \\nraises the learning rate. For example, for the first step, we might divide 1 by the learning rate,\\n \\n0.001\\n\\u200b\\n for example, which will result in a current learning rate of \\n\\u200b\\n1000\\n\\u200b\\n. That’s definitely not what\\n \\nwe wanted. 1 divided by the 1+fraction ensures that the result, a fraction of the starting learning\\n \\nrate, will always be less than or equal to 1, decreasing over time. That’s the desired result —', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 270}),\n",
       " Document(page_content='start with the current learning rate and make it smaller with time. The code for determining the\\n \\ncurrent decay rate:\\n \\nstarting_learning_rate \\n\\u200b\\n= \\n\\u200b\\n1.\\n \\nlearning_rate_decay \\n\\u200b\\n= \\n\\u200b\\n0.1\\n \\nstep \\n\\u200b\\n= \\n\\u200b\\n1\\n \\n \\nlearning_rate \\n\\u200b\\n= \\n\\u200b\\nstarting_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                (\\n\\u200b\\n1. \\n\\u200b\\n/ \\n\\u200b\\n(\\n\\u200b\\n1 \\n\\u200b\\n+ \\n\\u200b\\nlearning_rate_decay \\n\\u200b\\n* \\n\\u200b\\nstep))\\n \\nprint\\n\\u200b\\n(learning_rate)\\n \\n \\n \\n>>>\\n \\n0.9090909090909091', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 270}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n33\\n \\nIn practice, 0.1 would be considered a fairly aggressive decay rate, but this should give you a\\n \\nsense of the concept. If we are on step 20:\\n \\nstarting_learning_rate \\n\\u200b\\n= \\n\\u200b\\n1.\\n \\nlearning_rate_decay \\n\\u200b\\n= \\n\\u200b\\n0.1\\n \\nstep \\n\\u200b\\n= \\n\\u200b\\n20\\n \\n \\nlearning_rate \\n\\u200b\\n= \\n\\u200b\\nstarting_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                (\\n\\u200b\\n1. \\n\\u200b\\n/ \\n\\u200b\\n(\\n\\u200b\\n1 \\n\\u200b\\n+ \\n\\u200b\\nlearning_rate_decay \\n\\u200b\\n* \\n\\u200b\\nstep))\\n \\nprint\\n\\u200b\\n(learning_rate)\\n \\n \\n \\n>>>\\n \\n0.3333333333333333', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 271}),\n",
       " Document(page_content='0.3333333333333333\\n \\n \\nWe can also simulate this in a loop, which is more comparable to how we will be applying\\n \\nlearning rate decay:\\n \\nstarting_learning_rate \\n\\u200b\\n= \\n\\u200b\\n1.\\n \\nlearning_rate_decay \\n\\u200b\\n= \\n\\u200b\\n0.1\\n \\n \\nfor \\n\\u200b\\nstep \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n20\\n\\u200b\\n):\\n \\n    learning_rate \\n\\u200b\\n= \\n\\u200b\\nstarting_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                    (\\n\\u200b\\n1. \\n\\u200b\\n/ \\n\\u200b\\n(\\n\\u200b\\n1 \\n\\u200b\\n+ \\n\\u200b\\nlearning_rate_decay \\n\\u200b\\n* \\n\\u200b\\nstep))\\n \\n    \\n\\u200b\\nprint\\n\\u200b\\n(learning_rate)\\n \\n \\n \\n>>>\\n \\n1.0\\n \\n0.9090909090909091\\n \\n0.8333333333333334', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 271}),\n",
       " Document(page_content='0.7692307692307692\\n \\n0.7142857142857143\\n \\n0.6666666666666666\\n \\n0.625\\n \\n0.588235294117647\\n \\n0.5555555555555556\\n \\n0.5263157894736842\\n \\n0.5\\n \\n0.47619047619047616\\n \\n0.45454545454545453\\n \\n0.4347826086956522\\n \\n0.41666666666666663\\n \\n0.4\\n \\n0.3846153846153846\\n \\n0.37037037037037035\\n \\n0.35714285714285715\\n \\n0.3448275862068965', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 271}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n34\\n \\nThis learning rate decay scheme lowers the learning rate each step using the mentioned formula.\\n \\nInitially, the learning rate drops fast, but the change in the learning rate lowers each step, letting\\n \\nthe model sit as close as possible to the minimum. The model needs small updates near the end of\\n \\ntraining to be able to get as close to this point as possible. We can now update our SGD optimizer', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 272}),\n",
       " Document(page_content='class to allow for the learning rate decay:\\n \\n# SGD optimizer\\n \\nclass \\n\\u200b\\nOptimizer_SGD\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Initialize optimizer - set settings,\\n \\n    # learning rate of 1. is default for this optimizer\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlearning_rate\\n\\u200b\\n=\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\ndecay\\n\\u200b\\n=\\n\\u200b\\n0.\\n\\u200b\\n):\\n \\n        self.learning_rate \\n\\u200b\\n= \\n\\u200b\\nlearning_rate\\n \\n        self.current_learning_rate \\n\\u200b\\n= \\n\\u200b\\nlearning_rate\\n \\n        self.decay \\n\\u200b\\n= \\n\\u200b\\ndecay\\n \\n        self.iterations \\n\\u200b\\n= \\n\\u200b\\n0\\n \\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 272}),\n",
       " Document(page_content='\\u200b\\n= \\n\\u200b\\n0\\n \\n \\n    \\n\\u200b\\n# Call once before any parameter updates\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\npre_update_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nself.decay:\\n \\n            self.current_learning_rate \\n\\u200b\\n= \\n\\u200b\\nself.learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                (\\n\\u200b\\n1. \\n\\u200b\\n/ \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\n+ \\n\\u200b\\nself.decay \\n\\u200b\\n* \\n\\u200b\\nself.iterations))\\n \\n \\n    \\n\\u200b\\n# Update parameters\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nupdate_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlayer\\n\\u200b\\n):\\n \\n        layer.weights \\n\\u200b\\n+= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dweights', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 272}),\n",
       " Document(page_content='\\u200b\\nlayer.dweights\\n \\n        layer.biases \\n\\u200b\\n+= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dbiases\\n \\n \\n    \\n\\u200b\\n# Call once after any parameter updates\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\npost_update_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        self.iterations \\n\\u200b\\n+= \\n\\u200b\\n1\\n \\n \\nWe’ve updated a few things in the SGD class. First, in the \\n\\u200b\\n__init__\\n\\u200b\\n \\n\\u200b\\nmethod, we added\\n \\nhandling for the current learning rate, and \\n\\u200b\\nself.learning_rate\\n\\u200b\\n is now the initial learning', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 272}),\n",
       " Document(page_content='rate. We also added attributes to track the decay rate and the number of iterations that the\\n \\noptimizer has gone through. Next, we added a new method called \\n\\u200b\\npre_update_params\\n\\u200b\\n. This\\n \\nmethod, if we have a decay rate other than 0, will update our \\n\\u200b\\nself.current_learning_rate\\n \\nusing the prior formula. The \\n\\u200b\\nupdate_params\\n\\u200b\\n \\n\\u200b\\nmethod remains unchanged, but we do have a\\n \\nnew \\n\\u200b\\npost_update_params\\n\\u200b\\n method that will add to our \\n\\u200b\\nself.iterations\\n\\u200b\\n \\n\\u200b\\ntracking. With our', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 272}),\n",
       " Document(page_content='updated SGD optimizer class, we’ve added printing the current learning rate, and added pre and\\n \\npost optimizer method calls. Let’s use a decay rate of 1e-2 (0.01) and train our model again:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 272}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n35\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 64 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 64 input features (as we take output', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 273}),\n",
       " Document(page_content=\"# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\n# Create optimizer\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_SGD(\\n\\u200b\\ndecay\\n\\u200b\\n=\\n\\u200b\\n1e-2\\n\\u200b\\n)\\n \\n \\n# Train in loop\\n \\nfor \\n\\u200b\\nepoch \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10001\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of our training data through this layer\\n \\n    \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 273}),\n",
       " Document(page_content='\\u200b\\ndense1.forward(X)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\\n \\n    # takes the output of first dense layer here\\n \\n    \\n\\u200b\\nactivation1.forward(dense1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through second Dense layer\\n \\n    # takes outputs of activation function of first layer as inputs\\n \\n    \\n\\u200b\\ndense2.forward(activation1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through the activation/loss function', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 273}),\n",
       " Document(page_content='# takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)\\n \\n \\n    \\n\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis\\n \\n    \\n\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n        y \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n    \\n\\u200b\\nif not', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 273}),\n",
       " Document(page_content=\"\\u200b\\nif not \\n\\u200b\\nepoch \\n\\u200b\\n% \\n\\u200b\\n100\\n\\u200b\\n:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\nf\\n\\u200b\\n'epoch: \\n\\u200b\\n{epoch}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'acc: \\n\\u200b\\n{accuracy\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'loss: \\n\\u200b\\n{loss\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'lr: \\n\\u200b\\n{optimizer.current_learning_rate}\\n\\u200b\\n'\\n\\u200b\\n)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 273}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n36\\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\nloss_activation.backward(loss_activation.output, y)\\n \\n    dense2.backward(loss_activation.dinputs)\\n \\n    activation1.backward(dense2.dinputs)\\n \\n    dense1.backward(activation1.dinputs)\\n \\n \\n    \\n\\u200b\\n# Update weights and biases\\n \\n    \\n\\u200b\\noptimizer.pre_update_params()\\n \\n    optimizer.update_params(dense1)\\n \\n    optimizer.update_params(dense2)\\n \\n    optimizer.post_update_params()\\n \\n \\n \\n>>>\\n \\nepoch:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 274}),\n",
       " Document(page_content='>>>\\n \\nepoch: \\n\\u200b\\n0\\n\\u200b\\n, acc: \\n\\u200b\\n0.360\\n\\u200b\\n, loss: \\n\\u200b\\n1.099\\n\\u200b\\n, lr: \\n\\u200b\\n1.0\\n \\nepoch: \\n\\u200b\\n100\\n\\u200b\\n, acc: \\n\\u200b\\n0.403\\n\\u200b\\n, loss: \\n\\u200b\\n1.095\\n\\u200b\\n, lr: \\n\\u200b\\n0.5025125628140703\\n \\nepoch: \\n\\u200b\\n200\\n\\u200b\\n, acc: \\n\\u200b\\n0.397\\n\\u200b\\n, loss: \\n\\u200b\\n1.084\\n\\u200b\\n, lr: \\n\\u200b\\n0.33444816053511706\\n \\nepoch: \\n\\u200b\\n300\\n\\u200b\\n, acc: \\n\\u200b\\n0.400\\n\\u200b\\n, loss: \\n\\u200b\\n1.080\\n\\u200b\\n, lr: \\n\\u200b\\n0.2506265664160401\\n \\nepoch: \\n\\u200b\\n400\\n\\u200b\\n, acc: \\n\\u200b\\n0.407\\n\\u200b\\n, loss: \\n\\u200b\\n1.078\\n\\u200b\\n, lr: \\n\\u200b\\n0.2004008016032064\\n \\nepoch: \\n\\u200b\\n500\\n\\u200b\\n, acc: \\n\\u200b\\n0.420\\n\\u200b\\n, loss: \\n\\u200b\\n1.078\\n\\u200b\\n, lr: \\n\\u200b\\n0.1669449081803005', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 274}),\n",
       " Document(page_content='epoch: \\n\\u200b\\n600\\n\\u200b\\n, acc: \\n\\u200b\\n0.420\\n\\u200b\\n, loss: \\n\\u200b\\n1.077\\n\\u200b\\n, lr: \\n\\u200b\\n0.14306151645207438\\n \\nepoch: \\n\\u200b\\n700\\n\\u200b\\n, acc: \\n\\u200b\\n0.417\\n\\u200b\\n, loss: \\n\\u200b\\n1.077\\n\\u200b\\n, lr: \\n\\u200b\\n0.1251564455569462\\n \\nepoch: \\n\\u200b\\n800\\n\\u200b\\n, acc: \\n\\u200b\\n0.413\\n\\u200b\\n, loss: \\n\\u200b\\n1.077\\n\\u200b\\n, lr: \\n\\u200b\\n0.11123470522803114\\n \\nepoch: \\n\\u200b\\n900\\n\\u200b\\n, acc: \\n\\u200b\\n0.410\\n\\u200b\\n, loss: \\n\\u200b\\n1.077\\n\\u200b\\n, lr: \\n\\u200b\\n0.10010010010010009\\n \\nepoch: \\n\\u200b\\n1000\\n\\u200b\\n, acc: \\n\\u200b\\n0.417\\n\\u200b\\n, loss: \\n\\u200b\\n1.077\\n\\u200b\\n, lr: \\n\\u200b\\n0.09099181073703366\\n \\n...\\n \\nepoch: \\n\\u200b\\n2000\\n\\u200b\\n, acc: \\n\\u200b\\n0.420\\n\\u200b\\n, loss: \\n\\u200b\\n1.076\\n\\u200b\\n, lr: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 274}),\n",
       " Document(page_content='\\u200b\\n1.076\\n\\u200b\\n, lr: \\n\\u200b\\n0.047641734159123386\\n \\n...\\n \\nepoch: \\n\\u200b\\n3000\\n\\u200b\\n, acc: \\n\\u200b\\n0.413\\n\\u200b\\n, loss: \\n\\u200b\\n1.075\\n\\u200b\\n, lr: \\n\\u200b\\n0.03226847370119393\\n \\n...\\n \\nepoch: \\n\\u200b\\n4000\\n\\u200b\\n, acc: \\n\\u200b\\n0.407\\n\\u200b\\n, loss: \\n\\u200b\\n1.075\\n\\u200b\\n, lr: \\n\\u200b\\n0.02439619419370578\\n \\n...\\n \\nepoch: \\n\\u200b\\n5000\\n\\u200b\\n, acc: \\n\\u200b\\n0.403\\n\\u200b\\n, loss: \\n\\u200b\\n1.074\\n\\u200b\\n, lr: \\n\\u200b\\n0.019611688566385566\\n \\n...\\n \\nepoch: \\n\\u200b\\n7000\\n\\u200b\\n, acc: \\n\\u200b\\n0.400\\n\\u200b\\n, loss: \\n\\u200b\\n1.073\\n\\u200b\\n, lr: \\n\\u200b\\n0.014086491055078181\\n \\n...\\n \\nepoch: \\n\\u200b\\n10000\\n\\u200b\\n, acc: \\n\\u200b\\n0.397\\n\\u200b\\n, loss: \\n\\u200b\\n1.072\\n\\u200b\\n, lr: \\n\\u200b\\n0.009901970492127933', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 274}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n37\\n \\n \\nFig 10.16:\\n\\u200b\\n Model training with SGD optimizer and and learning rate decay set too high.\\n \\nEpilepsy Warning (quick flashing colors)\\n \\n \\nAnim 10.16:\\n\\u200b\\n \\n\\u200b\\nhttps://nnfs.io/zuk\\n \\nThis model definitely got stuck, and the reason is almost certainly because the learning rate\\n \\ndecayed far too quickly and became too small, trapping the model in some local minimum. This is', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 275}),\n",
       " Document(page_content='most likely why, rather than wiggling, our accuracy and loss stopped changing \\n\\u200b\\nat all\\n\\u200b\\n.\\n \\nWe can, instead, try to decay a bit slower by making our decay a smaller number. For example,\\n \\nlet’s go with 1e-3 (0.001):', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 275}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n38\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 64 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 64 input features (as we take output', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 276}),\n",
       " Document(page_content=\"# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\n# Create optimizer\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_SGD(\\n\\u200b\\ndecay\\n\\u200b\\n=\\n\\u200b\\n1e-3\\n\\u200b\\n)\\n \\n \\n# Train in loop\\n \\nfor \\n\\u200b\\nepoch \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10001\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of our training data through this layer\\n \\n    \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 276}),\n",
       " Document(page_content='\\u200b\\ndense1.forward(X)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\\n \\n    # takes the output of first dense layer here\\n \\n    \\n\\u200b\\nactivation1.forward(dense1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through second Dense layer\\n \\n    # takes outputs of activation function of first layer as inputs\\n \\n    \\n\\u200b\\ndense2.forward(activation1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through the activation/loss function', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 276}),\n",
       " Document(page_content='# takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)\\n \\n \\n    \\n\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis\\n \\n    \\n\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n        y \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n    \\n\\u200b\\nif not', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 276}),\n",
       " Document(page_content=\"\\u200b\\nif not \\n\\u200b\\nepoch \\n\\u200b\\n% \\n\\u200b\\n100\\n\\u200b\\n:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\nf\\n\\u200b\\n'epoch: \\n\\u200b\\n{epoch}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'acc: \\n\\u200b\\n{accuracy\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'loss: \\n\\u200b\\n{loss\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'lr: \\n\\u200b\\n{optimizer.current_learning_rate}\\n\\u200b\\n'\\n\\u200b\\n)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 276}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n39\\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\nloss_activation.backward(loss_activation.output, y)\\n \\n    dense2.backward(loss_activation.dinputs)\\n \\n    activation1.backward(dense2.dinputs)\\n \\n    dense1.backward(activation1.dinputs)\\n \\n \\n    \\n\\u200b\\n# Update weights and biases\\n \\n    \\n\\u200b\\noptimizer.pre_update_params()\\n \\n    optimizer.update_params(dense1)\\n \\n    optimizer.update_params(dense2)\\n \\n    optimizer.post_update_params()\\n \\n \\n \\n>>>\\n \\nepoch:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 277}),\n",
       " Document(page_content='>>>\\n \\nepoch: \\n\\u200b\\n0\\n\\u200b\\n, acc: \\n\\u200b\\n0.360\\n\\u200b\\n, loss: \\n\\u200b\\n1.099\\n\\u200b\\n, lr: \\n\\u200b\\n1.0\\n \\nepoch: \\n\\u200b\\n100\\n\\u200b\\n, acc: \\n\\u200b\\n0.400\\n\\u200b\\n, loss: \\n\\u200b\\n1.088\\n\\u200b\\n, lr: \\n\\u200b\\n0.9099181073703367\\n \\nepoch: \\n\\u200b\\n200\\n\\u200b\\n, acc: \\n\\u200b\\n0.423\\n\\u200b\\n, loss: \\n\\u200b\\n1.078\\n\\u200b\\n, lr: \\n\\u200b\\n0.8340283569641367\\n \\n...\\n \\nepoch: \\n\\u200b\\n1700\\n\\u200b\\n, acc: \\n\\u200b\\n0.450\\n\\u200b\\n, loss: \\n\\u200b\\n1.025\\n\\u200b\\n, lr: \\n\\u200b\\n0.3705075954057058\\n \\nepoch: \\n\\u200b\\n1800\\n\\u200b\\n, acc: \\n\\u200b\\n0.470\\n\\u200b\\n, loss: \\n\\u200b\\n1.017\\n\\u200b\\n, lr: \\n\\u200b\\n0.35727045373347627\\n \\nepoch: \\n\\u200b\\n1900\\n\\u200b\\n, acc: \\n\\u200b\\n0.460\\n\\u200b\\n, loss: \\n\\u200b\\n1.008\\n\\u200b\\n, lr: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 277}),\n",
       " Document(page_content='\\u200b\\n1.008\\n\\u200b\\n, lr: \\n\\u200b\\n0.3449465332873405\\n \\nepoch: \\n\\u200b\\n2000\\n\\u200b\\n, acc: \\n\\u200b\\n0.463\\n\\u200b\\n, loss: \\n\\u200b\\n1.000\\n\\u200b\\n, lr: \\n\\u200b\\n0.33344448149383127\\n \\nepoch: \\n\\u200b\\n2100\\n\\u200b\\n, acc: \\n\\u200b\\n0.490\\n\\u200b\\n, loss: \\n\\u200b\\n1.005\\n\\u200b\\n, lr: \\n\\u200b\\n0.32268473701193934\\n \\n...\\n \\nepoch: \\n\\u200b\\n3200\\n\\u200b\\n, acc: \\n\\u200b\\n0.493\\n\\u200b\\n, loss: \\n\\u200b\\n0.983\\n\\u200b\\n, lr: \\n\\u200b\\n0.23815194093831865\\n \\n...\\n \\nepoch: \\n\\u200b\\n5000\\n\\u200b\\n, acc: \\n\\u200b\\n0.577\\n\\u200b\\n, loss: \\n\\u200b\\n0.900\\n\\u200b\\n, lr: \\n\\u200b\\n0.16669444907484582\\n \\n...\\n \\nepoch: \\n\\u200b\\n6000\\n\\u200b\\n, acc: \\n\\u200b\\n0.633\\n\\u200b\\n, loss: \\n\\u200b\\n0.860\\n\\u200b\\n, lr: \\n\\u200b\\n0.1428775539362766\\n \\n...\\n \\nepoch: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 277}),\n",
       " Document(page_content='...\\n \\nepoch: \\n\\u200b\\n8000\\n\\u200b\\n, acc: \\n\\u200b\\n0.647\\n\\u200b\\n, loss: \\n\\u200b\\n0.799\\n\\u200b\\n, lr: \\n\\u200b\\n0.11112345816201799\\n \\n...\\n \\nepoch: \\n\\u200b\\n9800\\n\\u200b\\n, acc: \\n\\u200b\\n0.663\\n\\u200b\\n, loss: \\n\\u200b\\n0.773\\n\\u200b\\n, lr: \\n\\u200b\\n0.09260116677470137\\n \\nepoch: \\n\\u200b\\n9900\\n\\u200b\\n, acc: \\n\\u200b\\n0.663\\n\\u200b\\n, loss: \\n\\u200b\\n0.772\\n\\u200b\\n, lr: \\n\\u200b\\n0.09175153683824203\\n \\nepoch: \\n\\u200b\\n10000\\n\\u200b\\n, acc: \\n\\u200b\\n0.667\\n\\u200b\\n, loss: \\n\\u200b\\n0.771\\n\\u200b\\n, lr: \\n\\u200b\\n0.09091735612328393', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 277}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n40\\n \\n \\nFig 10.17:\\n\\u200b\\n Model training with SGD optimizer and more proper learning rate decay.\\n \\nEpilepsy Warning (quick flashing colors)\\n \\n \\nAnim 10.17: \\n\\u200b\\nhttps://nnfs.io/muk\\n \\nIn this case, we’ve achieved our lowest loss and highest accuracy thus far, but it still should be\\n \\npossible to find parameters that will give us even better results. For example, you may suspect', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 278}),\n",
       " Document(page_content='that the initial learning rate is too high. It can make for a great exercise to attempt to find better\\n \\nsettings. Feel free to try!\\n \\nStochastic Gradient Descent with learning rate decay can do fairly well but is still a fairly\\n \\nbasic optimization method that only follows a gradient without any additional logic that could\\n \\npotentially help the model find the \\n\\u200b\\nglobal\\n\\u200b\\n \\n\\u200b\\nminimum\\n\\u200b\\n to the loss function. One option for\\n \\nimproving the SGD optimizer is to introduce \\n\\u200b\\nmomentum\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 278}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n41\\n \\n \\nStochastic Gradient Descent with Momentum\\n \\nMomentum creates a rolling average of gradients over some number of updates and uses this\\n \\naverage with the unique gradient at each step. Another way of understanding this is to imagine a\\n \\nball going down a hill — even if it finds a small hole or hill, momentum will let it go straight\\n \\nthrough it towards a lower minimum — the bottom of this hill. This can help in cases where', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 279}),\n",
       " Document(page_content='you’re stuck in some local minimum (a hole), bouncing back and forth. With momentum, a model\\n \\nis more likely to pass through local minimums, further decreasing loss. Simply put, momentum\\n \\nmay still point towards the global gradient descent direction.\\n \\nRecall this situation from the beginning of this chapter:\\n \\n \\nWith regular updates, the SGD optimizer might determine that the next best step is one that', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 279}),\n",
       " Document(page_content='keeps the model in a local minimum. Remember that the gradient points toward the current\\n \\nsteepest loss ascent for that step — taking the negative of the gradient vector flips it toward the\\n \\ncurrent steepest descent, which may not necessarily follow descent towards the global minimum\\n \\n— the current steepest descent may point towards a local minimum. So this step may decrease\\n \\nloss for that update but might not get us out of the local minimum. We might wind up with a\\n \\ngradient', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 279}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n42', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 280}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n43\\n \\nthat points in one direction and then the opposite direction in the next update; the gradient could\\n \\ncontinue to bounce back and forth around a local minimum like this, keeping the optimization\\n \\nof the loss stuck. Instead, momentum uses the previous update’s direction to influence the next\\n \\nupdate’s direction, minimizing the chances of bouncing around and getting stuck.\\n \\nRecall another example shown in this chapter:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 281}),\n",
       " Document(page_content='We utilize momentum by setting a parameter between 0 and 1, representing the fraction of the\\n \\nprevious parameter update to retain, and subtracting (adding the negative) our actual gradient,\\n \\nmultiplied by the learning rate (like before), from it. The update contains a portion of the gradient\\n \\nfrom preceding steps as our momentum (direction of previous changes) and only a portion of the\\n \\ncurrent gradient; together, these portions form the actual change to our parameters and the bigger', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 281}),\n",
       " Document(page_content='the role that momentum takes in the update, the slower the update can change the direction. When\\n \\nwe set the momentum fraction too high, the model might stop learning at all since the direction of\\n \\nthe updates won’t be able to follow the global gradient descent. The code for this is as follows:\\n \\nweight_updates \\n\\u200b\\n= \\n\\u200b\\nself.momentum \\n\\u200b\\n* \\n\\u200b\\nlayer.weight_momentums \\n\\u200b\\n- \\n\\u200b\\n\\\\\\n \\n                 self.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dweights\\n \\n \\nThe hyperparameter, \\n\\u200b\\nself.momentum\\n\\u200b\\n,\\n\\u200b\\n \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 281}),\n",
       " Document(page_content='\\u200b\\n,\\n\\u200b\\n \\n\\u200b\\nis chosen at the start and the\\n \\nlayer.weight_momentums\\n\\u200b\\n \\n\\u200b\\nstart as all zeros but are altered during training as:\\n \\nlayer.weight_momentums \\n\\u200b\\n= \\n\\u200b\\nweight_updates', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 281}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n44\\n \\nThis means that the momentum is always the previous update to the parameters. We will perform\\n \\nthe same operations as the above with the biases. We can then update our SGD optimizer class’\\n \\nupdate_params\\n\\u200b\\n method with the momentum calculation, applying with the parameters, and\\n \\nretaining them for the next steps as an alternative chain of operations to the current code. The', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 282}),\n",
       " Document(page_content=\"difference is that we only calculate the updates and we add these updates with the common code:\\n \\n    # Update parameters\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nupdate_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlayer\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# If we use momentum\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nself.momentum:\\n \\n \\n            \\n\\u200b\\n# If layer does not contain momentum arrays, create them\\n \\n            # filled with zeros\\n \\n            \\n\\u200b\\nif not \\n\\u200b\\nhasattr\\n\\u200b\\n(layer, \\n\\u200b\\n'weight_momentums'\\n\\u200b\\n):\\n \\n                layer.weight_momentums \\n\\u200b\\n= \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 282}),\n",
       " Document(page_content=\"\\u200b\\n= \\n\\u200b\\nnp.zeros_like(layer.weights)\\n \\n                \\n\\u200b\\n# If there is no momentum array for weights\\n \\n                # The array doesn't exist for biases yet either.\\n \\n                \\n\\u200b\\nlayer.bias_momentums \\n\\u200b\\n= \\n\\u200b\\nnp.zeros_like(layer.biases)\\n \\n \\n            \\n\\u200b\\n# Build weight updates with momentum - take previous\\n \\n            # updates multiplied by retain factor and update with\\n \\n            # current gradients\\n \\n            \\n\\u200b\\nweight_updates \\n\\u200b\\n= \\n\\u200b\\n\\\\\\n \\n                self.momentum \\n\\u200b\\n*\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 282}),\n",
       " Document(page_content='\\u200b\\n* \\n\\u200b\\nlayer.weight_momentums \\n\\u200b\\n- \\n\\u200b\\n\\\\\\n \\n                self.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dweights\\n \\n            layer.weight_momentums \\n\\u200b\\n= \\n\\u200b\\nweight_updates\\n \\n \\n            \\n\\u200b\\n# Build bias updates\\n \\n            \\n\\u200b\\nbias_updates \\n\\u200b\\n= \\n\\u200b\\n\\\\\\n \\n                self.momentum \\n\\u200b\\n* \\n\\u200b\\nlayer.bias_momentums \\n\\u200b\\n- \\n\\u200b\\n\\\\\\n \\n                self.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dbiases\\n \\n            layer.bias_momentums \\n\\u200b\\n= \\n\\u200b\\nbias_updates\\n \\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 282}),\n",
       " Document(page_content='\\u200b\\n# Vanilla SGD updates (as before momentum update)\\n \\n        \\n\\u200b\\nelse\\n\\u200b\\n:\\n \\n            weight_updates \\n\\u200b\\n= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                             layer.dweights\\n \\n            bias_updates \\n\\u200b\\n= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                           layer.dbiases\\n \\n \\n        \\n\\u200b\\n# Update weights and biases using either\\n \\n        # vanilla or momentum updates\\n \\n        \\n\\u200b\\nlayer.weights \\n\\u200b\\n+= \\n\\u200b\\nweight_updates\\n \\n        layer.biases \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 282}),\n",
       " Document(page_content='\\u200b\\n+= \\n\\u200b\\nbias_updates', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 282}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n45\\n \\nMaking our full SGD optimizer class:\\n \\n# SGD optimizer\\n \\nclass \\n\\u200b\\nOptimizer_SGD\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Initialize optimizer - set settings,\\n \\n \\n    # learning rate of 1. is default for this optimizer\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlearning_rate\\n\\u200b\\n=\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\ndecay\\n\\u200b\\n=\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\nmomentum\\n\\u200b\\n=\\n\\u200b\\n0.\\n\\u200b\\n):\\n \\n        self.learning_rate \\n\\u200b\\n= \\n\\u200b\\nlearning_rate\\n \\n        self.current_learning_rate \\n\\u200b\\n= \\n\\u200b\\nlearning_rate', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 283}),\n",
       " Document(page_content='\\u200b\\nlearning_rate\\n \\n        self.decay \\n\\u200b\\n= \\n\\u200b\\ndecay\\n \\n        self.iterations \\n\\u200b\\n= \\n\\u200b\\n0\\n \\n        \\n\\u200b\\nself.momentum \\n\\u200b\\n= \\n\\u200b\\nmomentum\\n \\n \\n    \\n\\u200b\\n# Call once before any parameter updates\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\npre_update_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nself.decay:\\n \\n            self.current_learning_rate \\n\\u200b\\n= \\n\\u200b\\nself.learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                (\\n\\u200b\\n1. \\n\\u200b\\n/ \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\n+ \\n\\u200b\\nself.decay \\n\\u200b\\n* \\n\\u200b\\nself.iterations))\\n \\n \\n    \\n\\u200b\\n# Update parameters\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nupdate_params\\n\\u200b\\n(\\n\\u200b\\nself', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 283}),\n",
       " Document(page_content=\"\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlayer\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# If we use momentum\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nself.momentum:\\n \\n \\n            \\n\\u200b\\n# If layer does not contain momentum arrays, create them\\n \\n            # filled with zeros\\n \\n            \\n\\u200b\\nif not \\n\\u200b\\nhasattr\\n\\u200b\\n(layer, \\n\\u200b\\n'weight_momentums'\\n\\u200b\\n):\\n \\n                layer.weight_momentums \\n\\u200b\\n= \\n\\u200b\\nnp.zeros_like(layer.weights)\\n \\n                \\n\\u200b\\n# If there is no momentum array for weights\\n \\n                # The array doesn't exist for biases yet either.\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 283}),\n",
       " Document(page_content='\\u200b\\nlayer.bias_momentums \\n\\u200b\\n= \\n\\u200b\\nnp.zeros_like(layer.biases)\\n \\n \\n            \\n\\u200b\\n# Build weight updates with momentum - take previous\\n \\n            # updates multiplied by retain factor and update with\\n \\n            # current gradients\\n \\n            \\n\\u200b\\nweight_updates \\n\\u200b\\n= \\n\\u200b\\n\\\\\\n \\n                self.momentum \\n\\u200b\\n* \\n\\u200b\\nlayer.weight_momentums \\n\\u200b\\n- \\n\\u200b\\n\\\\\\n \\n                self.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dweights\\n \\n            layer.weight_momentums \\n\\u200b\\n= \\n\\u200b\\nweight_updates', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 283}),\n",
       " Document(page_content='weight_updates\\n \\n \\n            \\n\\u200b\\n# Build bias updates\\n \\n            \\n\\u200b\\nbias_updates \\n\\u200b\\n= \\n\\u200b\\n\\\\\\n \\n                self.momentum \\n\\u200b\\n* \\n\\u200b\\nlayer.bias_momentums \\n\\u200b\\n- \\n\\u200b\\n\\\\\\n \\n                self.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\nlayer.dbiases\\n \\n            layer.bias_momentums \\n\\u200b\\n= \\n\\u200b\\nbias_updates', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 283}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n46\\n \\n        \\n\\u200b\\n# Vanilla SGD updates (as before momentum update)\\n \\n        \\n\\u200b\\nelse\\n\\u200b\\n:\\n \\n            weight_updates \\n\\u200b\\n= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                             layer.dweights\\n \\n            bias_updates \\n\\u200b\\n= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                           layer.dbiases\\n \\n \\n        \\n\\u200b\\n# Update weights and biases using either\\n \\n        # vanilla or momentum updates\\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 284}),\n",
       " Document(page_content='\\u200b\\nlayer.weights \\n\\u200b\\n+= \\n\\u200b\\nweight_updates\\n \\n        layer.biases \\n\\u200b\\n+= \\n\\u200b\\nbias_updates\\n \\n \\n    \\n\\u200b\\n# Call once after any parameter updates\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\npost_update_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        self.iterations \\n\\u200b\\n+= \\n\\u200b\\n1\\n \\n \\nLet’s show an example illustrating how adding momentum changes the learning process. Keeping\\n \\nthe same starting \\n\\u200b\\nlearning rate \\n\\u200b\\n(1) and \\n\\u200b\\ndecay \\n\\u200b\\n(1e-3) from the previous training attempt and using\\n \\na momentum of 0.5:\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 284}),\n",
       " Document(page_content='X, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 64 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 64 input features (as we take output\\n \\n# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 284}),\n",
       " Document(page_content=\"64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\n# Create optimizer\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_SGD(\\n\\u200b\\ndecay\\n\\u200b\\n=\\n\\u200b\\n1e-3\\n\\u200b\\n, \\n\\u200b\\nmomentum\\n\\u200b\\n=\\n\\u200b\\n0.5\\n\\u200b\\n)\\n \\n \\n# Train in loop\\n \\nfor \\n\\u200b\\nepoch \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10001\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of our training data through this layer\\n \\n    \\n\\u200b\\ndense1.forward(X)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 284}),\n",
       " Document(page_content='# takes the output of first dense layer here\\n \\n    \\n\\u200b\\nactivation1.forward(dense1.output)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 284}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n47\\n \\n    \\n\\u200b\\n# Perform a forward pass through second Dense layer\\n \\n    # takes outputs of activation function of first layer as inputs\\n \\n    \\n\\u200b\\ndense2.forward(activation1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through the activation/loss function\\n \\n    # takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)\\n \\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 285}),\n",
       " Document(page_content=\"\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis\\n \\n    \\n\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n        y \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n    \\n\\u200b\\nif not \\n\\u200b\\nepoch \\n\\u200b\\n% \\n\\u200b\\n100\\n\\u200b\\n:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\nf\\n\\u200b\\n'epoch: \\n\\u200b\\n{epoch}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'acc: \\n\\u200b\\n{accuracy\\n\\u200b\\n:.3f\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 285}),\n",
       " Document(page_content=\"\\u200b\\n{accuracy\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'loss: \\n\\u200b\\n{loss\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'lr: \\n\\u200b\\n{optimizer.current_learning_rate}\\n\\u200b\\n'\\n\\u200b\\n)\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\nloss_activation.backward(loss_activation.output, y)\\n \\n    dense2.backward(loss_activation.dinputs)\\n \\n    activation1.backward(dense2.dinputs)\\n \\n    dense1.backward(activation1.dinputs)\\n \\n \\n    \\n\\u200b\\n# Update weights and biases\\n \\n    \\n\\u200b\\noptimizer.pre_update_params()\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 285}),\n",
       " Document(page_content='optimizer.update_params(dense1)\\n \\n    optimizer.update_params(dense2)\\n \\n    optimizer.post_update_params()\\n \\n \\n \\n>>>\\n \\nepoch: \\n\\u200b\\n0\\n\\u200b\\n, acc: \\n\\u200b\\n0.360\\n\\u200b\\n, loss: \\n\\u200b\\n1.099\\n\\u200b\\n, lr: \\n\\u200b\\n1.0\\n \\nepoch: \\n\\u200b\\n100\\n\\u200b\\n, acc: \\n\\u200b\\n0.427\\n\\u200b\\n, loss: \\n\\u200b\\n1.078\\n\\u200b\\n, lr: \\n\\u200b\\n0.9099181073703367\\n \\nepoch: \\n\\u200b\\n200\\n\\u200b\\n, acc: \\n\\u200b\\n0.423\\n\\u200b\\n, loss: \\n\\u200b\\n1.075\\n\\u200b\\n, lr: \\n\\u200b\\n0.8340283569641367\\n \\n...\\n \\nepoch: \\n\\u200b\\n1800\\n\\u200b\\n, acc: \\n\\u200b\\n0.483\\n\\u200b\\n, loss: \\n\\u200b\\n0.978\\n\\u200b\\n, lr: \\n\\u200b\\n0.35727045373347627\\n \\nepoch: \\n\\u200b\\n1900\\n\\u200b\\n, acc: \\n\\u200b\\n0.547\\n\\u200b\\n, loss: \\n\\u200b\\n0.984', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 285}),\n",
       " Document(page_content='\\u200b\\n, loss: \\n\\u200b\\n0.984\\n\\u200b\\n, lr: \\n\\u200b\\n0.3449465332873405\\n \\n...\\n \\nepoch: \\n\\u200b\\n3100\\n\\u200b\\n, acc: \\n\\u200b\\n0.593\\n\\u200b\\n, loss: \\n\\u200b\\n0.883\\n\\u200b\\n, lr: \\n\\u200b\\n0.2439619419370578\\n \\nepoch: \\n\\u200b\\n3200\\n\\u200b\\n, acc: \\n\\u200b\\n0.570\\n\\u200b\\n, loss: \\n\\u200b\\n0.878\\n\\u200b\\n, lr: \\n\\u200b\\n0.23815194093831865\\n \\nepoch: \\n\\u200b\\n3300\\n\\u200b\\n, acc: \\n\\u200b\\n0.563\\n\\u200b\\n, loss: \\n\\u200b\\n0.863\\n\\u200b\\n, lr: \\n\\u200b\\n0.23261223540358225\\n \\nepoch: \\n\\u200b\\n3400\\n\\u200b\\n, acc: \\n\\u200b\\n0.607\\n\\u200b\\n, loss: \\n\\u200b\\n0.860\\n\\u200b\\n, lr: \\n\\u200b\\n0.22732439190725165\\n \\n...\\n \\nepoch: \\n\\u200b\\n4600\\n\\u200b\\n, acc: \\n\\u200b\\n0.670\\n\\u200b\\n, loss: \\n\\u200b\\n0.761\\n\\u200b\\n, lr: \\n\\u200b\\n0.1786033220217896\\n \\nepoch: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 285}),\n",
       " Document(page_content='epoch: \\n\\u200b\\n4700\\n\\u200b\\n, acc: \\n\\u200b\\n0.690\\n\\u200b\\n, loss: \\n\\u200b\\n0.749\\n\\u200b\\n, lr: \\n\\u200b\\n0.1754693805930865', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 285}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n48\\n \\n...\\n \\nepoch: \\n\\u200b\\n6000\\n\\u200b\\n, acc: \\n\\u200b\\n0.743\\n\\u200b\\n, loss: \\n\\u200b\\n0.661\\n\\u200b\\n, lr: \\n\\u200b\\n0.1428775539362766\\n \\n...\\n \\nepoch: \\n\\u200b\\n8000\\n\\u200b\\n, acc: \\n\\u200b\\n0.763\\n\\u200b\\n, loss: \\n\\u200b\\n0.586\\n\\u200b\\n, lr: \\n\\u200b\\n0.11112345816201799\\n \\n...\\n \\nepoch: \\n\\u200b\\n10000\\n\\u200b\\n, acc: \\n\\u200b\\n0.800\\n\\u200b\\n, loss: \\n\\u200b\\n0.539\\n\\u200b\\n, lr: \\n\\u200b\\n0.09091735612328393\\n \\n \\n \\nFig 10.18:\\n\\u200b\\n Model training with SGD optimizer, learning rate decay and Momentum.\\n \\nEpilepsy Warning (quick flashing colors)\\n \\n \\nAnim 10.18\\n\\u200b\\n: \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 286}),\n",
       " Document(page_content='Anim 10.18\\n\\u200b\\n: \\n\\u200b\\nhttps://nnfs.io/ram', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 286}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n49\\n \\nThe model achieved the lowest loss and highest accuracy that we’ve seen so far, but can we do\\n \\neven better? Sure we can! Let’s try to set the momentum to 0.9:\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 64 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 287}),\n",
       " Document(page_content=\"2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 64 input features (as we take output\\n \\n# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\n# Create optimizer\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_SGD(\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 287}),\n",
       " Document(page_content='= \\n\\u200b\\nOptimizer_SGD(\\n\\u200b\\ndecay\\n\\u200b\\n=\\n\\u200b\\n1e-3\\n\\u200b\\n, \\n\\u200b\\nmomentum\\n\\u200b\\n=\\n\\u200b\\n0.9\\n\\u200b\\n)\\n \\n \\n# Train in loop\\n \\nfor \\n\\u200b\\nepoch \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10001\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of our training data through this layer\\n \\n    \\n\\u200b\\ndense1.forward(X)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\\n \\n    # takes the output of first dense layer here\\n \\n    \\n\\u200b\\nactivation1.forward(dense1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through second Dense layer', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 287}),\n",
       " Document(page_content='# takes outputs of activation function of first layer as inputs\\n \\n    \\n\\u200b\\ndense2.forward(activation1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through the activation/loss function\\n \\n    # takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)\\n \\n \\n    \\n\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis\\n \\n    \\n\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 287}),\n",
       " Document(page_content='\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 287}),\n",
       " Document(page_content=\"Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n50\\n \\n    \\n\\u200b\\nif not \\n\\u200b\\nepoch \\n\\u200b\\n% \\n\\u200b\\n100\\n\\u200b\\n:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\nf\\n\\u200b\\n'epoch: \\n\\u200b\\n{epoch}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'acc: \\n\\u200b\\n{accuracy\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'loss: \\n\\u200b\\n{loss\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'lr: \\n\\u200b\\n{optimizer.current_learning_rate}\\n\\u200b\\n'\\n\\u200b\\n)\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\nloss_activation.backward(loss_activation.output, y)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 288}),\n",
       " Document(page_content='dense2.backward(loss_activation.dinputs)\\n \\n    activation1.backward(dense2.dinputs)\\n \\n    dense1.backward(activation1.dinputs)\\n \\n \\n    \\n\\u200b\\n# Update weights and biases\\n \\n    \\n\\u200b\\noptimizer.pre_update_params()\\n \\n    optimizer.update_params(dense1)\\n \\n    optimizer.update_params(dense2)\\n \\n    optimizer.post_update_params()\\n \\n \\n \\n>>>\\n \\nepoch: \\n\\u200b\\n0\\n\\u200b\\n, acc: \\n\\u200b\\n0.360\\n\\u200b\\n, loss: \\n\\u200b\\n1.099\\n\\u200b\\n, lr: \\n\\u200b\\n1.0\\n \\nepoch: \\n\\u200b\\n100\\n\\u200b\\n, acc: \\n\\u200b\\n0.443\\n\\u200b\\n, loss: \\n\\u200b\\n1.053\\n\\u200b\\n, lr: \\n\\u200b\\n0.9099181073703367\\n \\nepoch: \\n\\u200b\\n200', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 288}),\n",
       " Document(page_content='epoch: \\n\\u200b\\n200\\n\\u200b\\n, acc: \\n\\u200b\\n0.497\\n\\u200b\\n, loss: \\n\\u200b\\n0.999\\n\\u200b\\n, lr: \\n\\u200b\\n0.8340283569641367\\n \\nepoch: \\n\\u200b\\n300\\n\\u200b\\n, acc: \\n\\u200b\\n0.603\\n\\u200b\\n, loss: \\n\\u200b\\n0.810\\n\\u200b\\n, lr: \\n\\u200b\\n0.7698229407236336\\n \\nepoch: \\n\\u200b\\n400\\n\\u200b\\n, acc: \\n\\u200b\\n0.700\\n\\u200b\\n, loss: \\n\\u200b\\n0.700\\n\\u200b\\n, lr: \\n\\u200b\\n0.7147962830593281\\n \\nepoch: \\n\\u200b\\n500\\n\\u200b\\n, acc: \\n\\u200b\\n0.750\\n\\u200b\\n, loss: \\n\\u200b\\n0.595\\n\\u200b\\n, lr: \\n\\u200b\\n0.66711140760507\\n \\nepoch: \\n\\u200b\\n600\\n\\u200b\\n, acc: \\n\\u200b\\n0.810\\n\\u200b\\n, loss: \\n\\u200b\\n0.496\\n\\u200b\\n, lr: \\n\\u200b\\n0.6253908692933083\\n \\nepoch: \\n\\u200b\\n700\\n\\u200b\\n, acc: \\n\\u200b\\n0.810\\n\\u200b\\n, loss: \\n\\u200b\\n0.466\\n\\u200b\\n, lr: \\n\\u200b\\n0.5885815185403178', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 288}),\n",
       " Document(page_content='epoch: \\n\\u200b\\n800\\n\\u200b\\n, acc: \\n\\u200b\\n0.847\\n\\u200b\\n, loss: \\n\\u200b\\n0.384\\n\\u200b\\n, lr: \\n\\u200b\\n0.5558643690939411\\n \\nepoch: \\n\\u200b\\n900\\n\\u200b\\n, acc: \\n\\u200b\\n0.850\\n\\u200b\\n, loss: \\n\\u200b\\n0.364\\n\\u200b\\n, lr: \\n\\u200b\\n0.526592943654555\\n \\nepoch: \\n\\u200b\\n1000\\n\\u200b\\n, acc: \\n\\u200b\\n0.877\\n\\u200b\\n, loss: \\n\\u200b\\n0.344\\n\\u200b\\n, lr: \\n\\u200b\\n0.5002501250625312\\n \\n...\\n \\nepoch: \\n\\u200b\\n2200\\n\\u200b\\n, acc: \\n\\u200b\\n0.900\\n\\u200b\\n, loss: \\n\\u200b\\n0.242\\n\\u200b\\n, lr: \\n\\u200b\\n0.31259768677711786\\n \\n...\\n \\nepoch: \\n\\u200b\\n2900\\n\\u200b\\n, acc: \\n\\u200b\\n0.910\\n\\u200b\\n, loss: \\n\\u200b\\n0.216\\n\\u200b\\n, lr: \\n\\u200b\\n0.25647601949217746\\n \\n...\\n \\nepoch: \\n\\u200b\\n3800\\n\\u200b\\n, acc: \\n\\u200b\\n0.920\\n\\u200b\\n, loss: \\n\\u200b\\n0.202\\n\\u200b\\n, lr:', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 288}),\n",
       " Document(page_content='\\u200b\\n0.202\\n\\u200b\\n, lr: \\n\\u200b\\n0.20837674515524068\\n \\n...\\n \\nepoch: \\n\\u200b\\n7100\\n\\u200b\\n, acc: \\n\\u200b\\n0.930\\n\\u200b\\n, loss: \\n\\u200b\\n0.181\\n\\u200b\\n, lr: \\n\\u200b\\n0.12347203358439313\\n \\n...\\n \\nepoch: \\n\\u200b\\n10000\\n\\u200b\\n, acc: \\n\\u200b\\n0.933\\n\\u200b\\n, loss: \\n\\u200b\\n0.173\\n\\u200b\\n, lr: \\n\\u200b\\n0.09091735612328393', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 288}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n51\\n \\n \\nFig 10.19:\\n\\u200b\\n Model training with SGD optimizer, learning rate decay and Momentum (tuned).\\n \\nEpilepsy Warning (quick flashing colors)\\n \\n \\nAnim 10.19\\n\\u200b\\n: \\n\\u200b\\nhttps://nnfs.io/map\\n \\nThis is a decent enough example of how momentum can prove useful. The model achieved an\\n \\naccuracy of almost 88% in the first 1000 epochs and improved further, ending with an accuracy of', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 289}),\n",
       " Document(page_content='93.3% and a loss of 0.173. These results are a great improvement. The SGD optimizer with\\n \\nmomentum is usually one of 2 main choices for an optimizer in practice next to the Adam\\n \\noptimizer, which we’ll talk about shortly. First, we have 2 other optimizers to talk about. The next\\n \\nmodification to Stochastic Gradient Descent is \\n\\u200b\\nAdaGrad\\n\\u200b\\n.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 289}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n52\\n \\n \\nAdaGrad\\n \\nAdaGrad\\n\\u200b\\n, short for \\n\\u200b\\nadaptive gradient\\n\\u200b\\n, institutes a per-parameter learning rate rather than a\\n \\nglobally-shared rate. The idea here is to normalize updates made to the features. During the\\n \\ntraining process, some weights can rise significantly, while others tend to not change by much. It\\n \\nis usually better for weights to not rise too high compared to the other weights, and we’ll talk', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 290}),\n",
       " Document(page_content='about this with regularization techniques. AdaGrad provides a way to normalize parameter\\n \\nupdates by keeping a history of previous updates — the bigger the sum of the updates is, in either\\n \\ndirection (positive or negative), the smaller updates are made further in training. This lets\\n \\nless-frequently updated parameters to keep-up with changes, effectively utilizing more neurons\\n \\nfor training. The concept of AdaGrad can be contained in the following two lines of code:\\n \\ncache \\n\\u200b\\n+= \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 290}),\n",
       " Document(page_content='cache \\n\\u200b\\n+= \\n\\u200b\\nparm_gradient \\n\\u200b\\n** \\n\\u200b\\n2\\n \\nparm_updates \\n\\u200b\\n= \\n\\u200b\\nlearning_rate \\n\\u200b\\n* \\n\\u200b\\nparm_gradient \\n\\u200b\\n/ \\n\\u200b\\n(sqrt(cache) \\n\\u200b\\n+ \\n\\u200b\\neps)\\n \\n \\nThe \\n\\u200b\\ncache\\n\\u200b\\n holds a history of squared gradients, and the \\n\\u200b\\nparm_updates\\n\\u200b\\n is a function of the\\n \\nlearning rate multiplied by the gradient (basic SGD so far) and then is divided by the square root\\n \\nof the cache plus some \\n\\u200b\\nepsilon\\n\\u200b\\n value. The division operation performed with a constantly rising', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 290}),\n",
       " Document(page_content='cache might also cause the learning to stall as updates become smaller with time, due to the\\n \\nmonotonic nature of updates. That’s why this optimizer is not widely used, except for some\\n \\nspecific applications. The \\n\\u200b\\nepsilon\\n\\u200b\\n is a \\n\\u200b\\nhyperparameter\\n\\u200b\\n (pre-training control knob setting)\\n \\npreventing division by 0. The epsilon value is usually a small value, such as \\n\\u200b\\n1e-7\\n\\u200b\\n, which we’ll be\\n \\ndefaulting to. You might also notice that we are summing the squared value, only to calculate', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 290}),\n",
       " Document(page_content='the square root later, which might look counter-intuitive as to why we do this. We are adding\\n \\nsquared values and taking the square root, which is not the same as just adding the value, for\\n \\nexample:\\n \\n \\n \\nThe resulting cache value grows slower, and in a different way, taking care of the negative\\n \\nnumbers (we would not want to divide the update by the negative number and flip its sign).\\n \\nOverall, the impact is the learning rates for parameters with smaller gradients are decreased', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 290}),\n",
       " Document(page_content='slowly, while the parameters with larger gradients have their learning rates decreased faster.', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 290}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n53\\n \\nTo implement AdaGrad, we start by copying and pasting our SGD optimizer class, changing the\\n \\nname, adding a property for \\n\\u200b\\nepsilon\\n\\u200b\\n with a default of 1e-7 to the \\n\\u200b\\n__init__\\n\\u200b\\n method, and\\n \\nremoving the momentum. Next, inside the \\n\\u200b\\nupdate_params\\n\\u200b\\n method, we’ll replace the\\n \\nmomentum code with:\\n \\n    # Update parameters\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nupdate_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlayer\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 291}),\n",
       " Document(page_content=\"\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# If layer does not contain cache arrays,\\n \\n        # create them filled with zeros\\n \\n        \\n\\u200b\\nif not \\n\\u200b\\nhasattr\\n\\u200b\\n(layer, \\n\\u200b\\n'weight_cache'\\n\\u200b\\n):\\n \\n            layer.weight_cache \\n\\u200b\\n= \\n\\u200b\\nnp.zeros_like(layer.weights)\\n \\n            layer.bias_cache \\n\\u200b\\n= \\n\\u200b\\nnp.zeros_like(layer.biases)\\n \\n \\n        \\n\\u200b\\n# Update cache with squared current gradients\\n \\n        \\n\\u200b\\nlayer.weight_cache \\n\\u200b\\n+= \\n\\u200b\\nlayer.dweights\\n\\u200b\\n**\\n\\u200b\\n2\\n \\n        \\n\\u200b\\nlayer.bias_cache \\n\\u200b\\n+= \\n\\u200b\\nlayer.dbiases\\n\\u200b\\n**\\n\\u200b\\n2\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 291}),\n",
       " Document(page_content='\\u200b\\n**\\n\\u200b\\n2\\n \\n \\n        \\n\\u200b\\n# Vanilla SGD parameter update + normalization\\n \\n        # with square rooted cache\\n \\n        \\n\\u200b\\nlayer.weights \\n\\u200b\\n+= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                         layer.dweights \\n\\u200b\\n/ \\n\\u200b\\n\\\\\\n \\n                         (np.sqrt(layer.weight_cache) \\n\\u200b\\n+ \\n\\u200b\\nself.epsilon)\\n \\n        layer.biases \\n\\u200b\\n+= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                        layer.dbiases \\n\\u200b\\n/ \\n\\u200b\\n\\\\\\n \\n                        (np.sqrt(layer.bias_cache) \\n\\u200b\\n+ \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 291}),\n",
       " Document(page_content='\\u200b\\n+ \\n\\u200b\\nself.epsilon)\\n \\n \\nWe added the cache and its updates, then added dividing the updates by the square root of the\\n \\ncache. Full code for the AdaGrad optimizer:\\n \\n# Adagrad optimizer\\n \\nclass \\n\\u200b\\nOptimizer_Adagrad\\n\\u200b\\n:\\n \\n \\n    \\n\\u200b\\n# Initialize optimizer - set settings\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\n__init__\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlearning_rate\\n\\u200b\\n=\\n\\u200b\\n1.\\n\\u200b\\n, \\n\\u200b\\ndecay\\n\\u200b\\n=\\n\\u200b\\n0.\\n\\u200b\\n, \\n\\u200b\\nepsilon\\n\\u200b\\n=\\n\\u200b\\n1e-7\\n\\u200b\\n):\\n \\n        self.learning_rate \\n\\u200b\\n= \\n\\u200b\\nlearning_rate\\n \\n        self.current_learning_rate \\n\\u200b\\n= \\n\\u200b\\nlearning_rate', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 291}),\n",
       " Document(page_content='\\u200b\\nlearning_rate\\n \\n        self.decay \\n\\u200b\\n= \\n\\u200b\\ndecay\\n \\n        self.iterations \\n\\u200b\\n= \\n\\u200b\\n0\\n \\n        \\n\\u200b\\nself.epsilon \\n\\u200b\\n= \\n\\u200b\\nepsilon\\n \\n \\n    \\n\\u200b\\n# Call once before any parameter updates\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\npre_update_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        \\n\\u200b\\nif \\n\\u200b\\nself.decay:\\n \\n            self.current_learning_rate \\n\\u200b\\n= \\n\\u200b\\nself.learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                (\\n\\u200b\\n1. \\n\\u200b\\n/ \\n\\u200b\\n(\\n\\u200b\\n1. \\n\\u200b\\n+ \\n\\u200b\\nself.decay \\n\\u200b\\n* \\n\\u200b\\nself.iterations))', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 291}),\n",
       " Document(page_content=\"Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n54\\n \\n    \\n\\u200b\\n# Update parameters\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\nupdate_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n, \\n\\u200b\\nlayer\\n\\u200b\\n):\\n \\n \\n        \\n\\u200b\\n# If layer does not contain cache arrays,\\n \\n        # create them filled with zeros\\n \\n        \\n\\u200b\\nif not \\n\\u200b\\nhasattr\\n\\u200b\\n(layer, \\n\\u200b\\n'weight_cache'\\n\\u200b\\n):\\n \\n            layer.weight_cache \\n\\u200b\\n= \\n\\u200b\\nnp.zeros_like(layer.weights)\\n \\n            layer.bias_cache \\n\\u200b\\n= \\n\\u200b\\nnp.zeros_like(layer.biases)\\n \\n \\n        \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 292}),\n",
       " Document(page_content='\\u200b\\n# Update cache with squared current gradients\\n \\n        \\n\\u200b\\nlayer.weight_cache \\n\\u200b\\n+= \\n\\u200b\\nlayer.dweights\\n\\u200b\\n**\\n\\u200b\\n2\\n \\n        \\n\\u200b\\nlayer.bias_cache \\n\\u200b\\n+= \\n\\u200b\\nlayer.dbiases\\n\\u200b\\n**\\n\\u200b\\n2\\n \\n \\n        \\n\\u200b\\n# Vanilla SGD parameter update + normalization\\n \\n        # with square rooted cache\\n \\n        \\n\\u200b\\nlayer.weights \\n\\u200b\\n+= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                         layer.dweights \\n\\u200b\\n/ \\n\\u200b\\n\\\\\\n \\n                         (np.sqrt(layer.weight_cache) \\n\\u200b\\n+ \\n\\u200b\\nself.epsilon)', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 292}),\n",
       " Document(page_content='\\u200b\\nself.epsilon)\\n \\n        layer.biases \\n\\u200b\\n+= -\\n\\u200b\\nself.current_learning_rate \\n\\u200b\\n* \\n\\u200b\\n\\\\\\n \\n                        layer.dbiases \\n\\u200b\\n/ \\n\\u200b\\n\\\\\\n \\n                        (np.sqrt(layer.bias_cache) \\n\\u200b\\n+ \\n\\u200b\\nself.epsilon)\\n \\n \\n    \\n\\u200b\\n# Call once after any parameter updates\\n \\n    \\n\\u200b\\ndef \\n\\u200b\\npost_update_params\\n\\u200b\\n(\\n\\u200b\\nself\\n\\u200b\\n):\\n \\n        self.iterations \\n\\u200b\\n+= \\n\\u200b\\n1\\n \\n \\nTesting this optimizer now with decaying set to \\n\\u200b\\n1e-4\\n\\u200b\\n as well as \\n\\u200b\\n1e-5\\n\\u200b\\n works better than \\n\\u200b\\n1e-3\\n\\u200b\\n,', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 292}),\n",
       " Document(page_content='\\u200b\\n1e-3\\n\\u200b\\n,\\n \\nwhich we have used previously. This optimizer with our dataset works better with lesser\\n \\ndecaying:\\n \\n# Create dataset\\n \\nX, y \\n\\u200b\\n= \\n\\u200b\\nspiral_data(\\n\\u200b\\nsamples\\n\\u200b\\n=\\n\\u200b\\n100\\n\\u200b\\n, \\n\\u200b\\nclasses\\n\\u200b\\n=\\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Dense layer with 2 input features and 64 output values\\n \\ndense1 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n2\\n\\u200b\\n, \\n\\u200b\\n64\\n\\u200b\\n)\\n \\n \\n# Create ReLU activation (to be used with Dense layer):\\n \\nactivation1 \\n\\u200b\\n= \\n\\u200b\\nActivation_ReLU()\\n \\n \\n# Create second Dense layer with 64 input features (as we take output', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 292}),\n",
       " Document(page_content=\"# of previous layer here) and 3 output values (output values)\\n \\ndense2 \\n\\u200b\\n= \\n\\u200b\\nLayer_Dense(\\n\\u200b\\n64\\n\\u200b\\n, \\n\\u200b\\n3\\n\\u200b\\n)\\n \\n \\n# Create Softmax classifier's combined loss and activation\\n \\nloss_activation \\n\\u200b\\n= \\n\\u200b\\nActivation_Softmax_Loss_CategoricalCrossentropy()\\n \\n \\n# Create optimizer\\n \\n#optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9)\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 292}),\n",
       " Document(page_content='Chapter 10 - Optimizers - Neural Networks from Scratch in Python\\n \\n55\\n \\noptimizer \\n\\u200b\\n= \\n\\u200b\\nOptimizer_Adagrad(\\n\\u200b\\ndecay\\n\\u200b\\n=\\n\\u200b\\n1e-4\\n\\u200b\\n)\\n \\n# Train in loop\\n \\nfor \\n\\u200b\\nepoch \\n\\u200b\\nin \\n\\u200b\\nrange\\n\\u200b\\n(\\n\\u200b\\n10001\\n\\u200b\\n):\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass of our training data through this layer\\n \\n    \\n\\u200b\\ndense1.forward(X)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through activation function\\n \\n    # takes the output of first dense layer here\\n \\n    \\n\\u200b\\nactivation1.forward(dense1.output)\\n \\n \\n    \\n\\u200b', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 293}),\n",
       " Document(page_content='\\u200b\\n# Perform a forward pass through second Dense layer\\n \\n    # takes outputs of activation function of first layer as inputs\\n \\n    \\n\\u200b\\ndense2.forward(activation1.output)\\n \\n \\n    \\n\\u200b\\n# Perform a forward pass through the activation/loss function\\n \\n    # takes the output of second dense layer here and returns loss\\n \\n    \\n\\u200b\\nloss \\n\\u200b\\n= \\n\\u200b\\nloss_activation.forward(dense2.output, y)\\n \\n \\n    \\n\\u200b\\n# Calculate accuracy from output of activation2 and targets\\n \\n    # calculate values along first axis', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 293}),\n",
       " Document(page_content=\"\\u200b\\npredictions \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(loss_activation.output, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    \\n\\u200b\\nif \\n\\u200b\\nlen\\n\\u200b\\n(y.shape) \\n\\u200b\\n== \\n\\u200b\\n2\\n\\u200b\\n:\\n \\n        y \\n\\u200b\\n= \\n\\u200b\\nnp.argmax(y, \\n\\u200b\\naxis\\n\\u200b\\n=\\n\\u200b\\n1\\n\\u200b\\n)\\n \\n    accuracy \\n\\u200b\\n= \\n\\u200b\\nnp.mean(predictions\\n\\u200b\\n==\\n\\u200b\\ny)\\n \\n \\n    \\n\\u200b\\nif not \\n\\u200b\\nepoch \\n\\u200b\\n% \\n\\u200b\\n100\\n\\u200b\\n:\\n \\n        \\n\\u200b\\nprint\\n\\u200b\\n(\\n\\u200b\\nf\\n\\u200b\\n'epoch: \\n\\u200b\\n{epoch}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'acc: \\n\\u200b\\n{accuracy\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'loss: \\n\\u200b\\n{loss\\n\\u200b\\n:.3f\\n\\u200b\\n}\\n\\u200b\\n, ' \\n\\u200b\\n+\\n \\n              \\n\\u200b\\nf\\n\\u200b\\n'lr: \\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 293}),\n",
       " Document(page_content=\"\\u200b\\nf\\n\\u200b\\n'lr: \\n\\u200b\\n{optimizer.current_learning_rate}\\n\\u200b\\n'\\n\\u200b\\n)\\n \\n \\n    \\n\\u200b\\n# Backward pass\\n \\n    \\n\\u200b\\nloss_activation.backward(loss_activation.output, y)\\n \\n    dense2.backward(loss_activation.dinputs)\\n \\n    activation1.backward(dense2.dinputs)\\n \\n    dense1.backward(activation1.dinputs)\\n \\n \\n    \\n\\u200b\\n# Update weights and biases\\n \\n    \\n\\u200b\\noptimizer.pre_update_params()\\n \\n    optimizer.update_params(dense1)\\n \\n    optimizer.update_params(dense2)\\n \\n    optimizer.post_update_params()\\n \\n \\n \\n>>>\\n \\nepoch: \\n\\u200b\\n0\\n\\u200b\", metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 293}),\n",
       " Document(page_content='>>>\\n \\nepoch: \\n\\u200b\\n0\\n\\u200b\\n, acc: \\n\\u200b\\n0.360\\n\\u200b\\n, loss: \\n\\u200b\\n1.099\\n\\u200b\\n, lr: \\n\\u200b\\n1.0\\n \\nepoch: \\n\\u200b\\n100\\n\\u200b\\n, acc: \\n\\u200b\\n0.457\\n\\u200b\\n, loss: \\n\\u200b\\n1.012\\n\\u200b\\n, lr: \\n\\u200b\\n0.9901970492127933\\n \\nepoch: \\n\\u200b\\n200\\n\\u200b\\n, acc: \\n\\u200b\\n0.527\\n\\u200b\\n, loss: \\n\\u200b\\n0.936\\n\\u200b\\n, lr: \\n\\u200b\\n0.9804882831650161', metadata={'source': 'pdfs\\\\neural network.pdf', 'page': 293}),\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d163e20-726c-47cb-a8d6-dd43c9e998c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b140ef99-150b-4277-9351-91041ef48b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preface - Neural Networks from Scratch in Python\n",
      " \n",
      "3\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Acknowledgements\n",
      " \n",
      "Harrison Kinsley:\n",
      " \n",
      "My wife, Stephanie, for her unfailing support and faith in me throughout the years. You’ve never\n",
      " \n",
      "doubted me.\n",
      " \n",
      "Each and every viewer and person who supported this book and project. Without my audience,\n",
      " \n",
      "none of this would have been possible.\n",
      " \n",
      "The Python programming community in general for being awesome!\n",
      " \n",
      "Daniel Kukie\n",
      "​\n",
      "ł\n",
      "​\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58a2f026-1499-4388-aa26-f685717f8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d4bb29-06af-4e7e-8e8e-660fac6becd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52da4ce3-4f0c-49d1-a989-0fdfcbc97a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d25b0c1-a302-4a95-93f6-177fe07308d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding.embed_query(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65862afb-f57a-4127-ac9c-b337fb50d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4aa74b1-cc20-44d4-a6e2-20578a36559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get(\"PINECONE_API_KEY\",\"169358c7-dd49-4537-b308-9e969fb0818a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ed06082-c073-4150-b16a-cda24694a608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (3.2.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pinecone-client) (2023.7.22)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pinecone-client) (4.8.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from pinecone-client) (2.0.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493cfc8-bc73-4fc0-a965-f22df1506c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
